{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EmotionsRecognition.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "CjWvnaQUrZmD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Emotion classification using the RAVDESS dataset"
      ]
    },
    {
      "metadata": {
        "id": "ldtHMhuLrewK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) is licensed under CC BY-NA-SC 4.0. and can be downloaded free of charge at https://zenodo.org/record/1188976.\n",
        "\n",
        "***Construction and Validation***\n",
        "\n",
        "Construction and validation of the RAVDESS is described in our paper: Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): e0196391. https://doi.org/10.1371/journal.pone.0196391.\n",
        "\n",
        "The RAVDESS contains 7356 files. Each file was rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained adult research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity, interrater reliability, and test-retest intrarater reliability were reported. Validation data is open-access, and can be downloaded along with our paper from PLOS ONE.\n",
        "\n",
        "***Description***\n",
        "\n",
        "The dataset contains the complete set of 7356 RAVDESS files (total size: 24.8 GB). Each of the 24 actors consists of three modality formats: Audio-only (16bit, 48kHz .wav), Audio-Video (720p H.264, AAC 48kHz, .mp4), and Video-only (no sound).  Note, there are no song files for Actor_18.\n",
        "\n",
        "***Data***\n",
        "\n",
        "For this task, I have used 4948 samples from the RAVDESS dataset.\n",
        "\n",
        "The samples comes from:\n",
        "\n",
        "- Audio-only files;\n",
        "- Video + audio files: I have extracted the audio from each file using the script Mp4ToWav.py that you can find in the main directory of the project.\n",
        "\n",
        "***License information***\n",
        "\n",
        "The RAVDESS is released under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License, CC BY-NA-SC 4.0\n",
        "\n",
        "***File naming convention***\n",
        "\n",
        "Each of the 7356 RAVDESS files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 02-01-06-01-02-01-12.mp4). These identifiers define the stimulus characteristics:\n",
        "\n",
        "***Filename identifiers***\n",
        "\n",
        "- Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
        "- Vocal channel (01 = speech, 02 = song).\n",
        "- Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
        "- Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the ‘neutral’ emotion.\n",
        "- Statement (01 = “Kids are talking by the door”, 02 = “Dogs are sitting by the door”).\n",
        "- Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
        "- Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
        "\n",
        "Filename example: 02-01-06-01-02-01-12.mp4 \n",
        "\n",
        "- Video-only (02)\n",
        "- Speech (01)\n",
        "- Fearful (06)\n",
        "- Normal intensity (01)\n",
        "- Statement “dogs” (02)\n",
        "- 1st Repetition (01)\n",
        "- 12th Actor (12)\n",
        "- Female, as the actor ID number is even."
      ]
    },
    {
      "metadata": {
        "id": "JDNbxj45rkvB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Analysis\n",
        "\n",
        "We are using Colab, a Google Cloud environment for jupyter, so we need to import our files from Google Drive and then install LibROSA, a python package for music and audio analysis.\n",
        "\n",
        "After the import, we will plot the signal of the first file."
      ]
    },
    {
      "metadata": {
        "id": "N-o2JI49WBAe",
        "colab_type": "code",
        "outputId": "6f5ea689-4391-440c-8fb8-8db0d8c1d782",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EgFwaDhMbJVm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "395475aa-0286-47c7-f615-f743a13f1c38"
      },
      "cell_type": "code",
      "source": [
        "!pip install librosa"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (2.1.6)\n",
            "Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.14.6)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.20.2)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (4.3.0)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.11.0)\n",
            "Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.2.1)\n",
            "Requirement already satisfied: numba>=0.38.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.40.1)\n",
            "Requirement already satisfied: llvmlite>=0.25.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa) (0.27.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rxI4xzngdS-e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "from librosa import display\n",
        "\n",
        "data, sampling_rate = librosa.load('/content/drive/My Drive/Ravdess/03-01-01-01-01-01-01.wav')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WgaSHtCIdtX2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "453dc779-4f51-47b3-dcdb-786a4dd35110"
      },
      "cell_type": "code",
      "source": [
        "% pylab inline\n",
        "import os\n",
        "import pandas as pd\n",
        "import glob \n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "librosa.display.waveplot(data, sr=sampling_rate)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['display']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PolyCollection at 0x7f21cb3de940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAswAAAEGCAYAAABxSsNVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 864x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "vCtNuVWlr5jL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load all files\n",
        "\n",
        "We will create our numpy array extracting Mel-frequency cepstral coefficients (MFCCs), while the classes to predict will be extracted from the name of the file (see the introductory section of this notebook to see the naming convention of the files of this dataset)."
      ]
    },
    {
      "metadata": {
        "id": "AKvuF--gd6F-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path = '/content/drive/My Drive/Ravdess'\n",
        "lst = []\n",
        "\n",
        "for subdir, dirs, files in os.walk(path):\n",
        "  for file in files:\n",
        "      try:\n",
        "        #Load librosa array, obtain mfcss, store the file and the mcss information in a new array\n",
        "        X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')\n",
        "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
        "        file = file[6:8]\n",
        "        arr = mfccs, file\n",
        "        lst.append(arr)\n",
        "      # If the file is not valid, skip it\n",
        "      except ValueError:\n",
        "        continue       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kLSggnF7kKY1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Creating X and y: zip makes a list of all the first elements, and a list of all the second elements.\n",
        "X, y = zip(*lst)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VzvBRTJIlIE9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7bdede45-02de-4e07-c071-167f923f8623"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X = np.asarray(X)\n",
        "y = np.asarray(y)\n",
        "\n",
        "\n",
        "X.shape, y.shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4948, 40), (4948,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "Agw-3KN1sDhh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Decision Tree Classifier\n",
        "\n",
        "To make a first attempt in accomplishing this classification task I chose a decision tree:"
      ]
    },
    {
      "metadata": {
        "id": "Q-Xgb5NslTBO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UshLOC1ClWL3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_BnCR52nlXw0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dtree = DecisionTreeClassifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qWyTownblZM0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "316674d3-db0b-498c-c13b-3ca04b1e919f"
      },
      "cell_type": "code",
      "source": [
        "dtree.fit(X_train, y_train)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
              "            max_features=None, max_leaf_nodes=None,\n",
              "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "            min_samples_leaf=1, min_samples_split=2,\n",
              "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
              "            splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "HEuw6TUQlr7C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = dtree.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_1v0i0V7sMw7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's go with our classification report.\n",
        "\n",
        "Before we start, a quick reminder of the classes we are trying to predict:\n",
        "\n",
        "emotions = {\n",
        "    \"neutral\": \"01\",\n",
        "    \"calm\": \"02\",\n",
        "    \"happy\": \"03\",\n",
        "    \"sad\": \"04\",\n",
        "    \"angry\": \"05\", \n",
        "    \"fearful\": \"06\", \n",
        "    \"disgust\": \"07\", \n",
        "    \"surprised\": \"08\"\n",
        "}"
      ]
    },
    {
      "metadata": {
        "id": "c4kNSYkAleIv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "4283184e-8420-4bf5-b7f6-bf789e7cd94a"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "print(classification_report(y_test,predictions))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          01       0.78      0.82      0.80       134\n",
            "          02       0.84      0.84      0.84       251\n",
            "          03       0.79      0.71      0.75       242\n",
            "          04       0.78      0.73      0.76       271\n",
            "          05       0.81      0.85      0.83       253\n",
            "          06       0.76      0.80      0.78       239\n",
            "          07       0.77      0.73      0.75       127\n",
            "          08       0.70      0.76      0.73       116\n",
            "\n",
            "   micro avg       0.79      0.79      0.79      1633\n",
            "   macro avg       0.78      0.78      0.78      1633\n",
            "weighted avg       0.79      0.79      0.79      1633\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lCVgjLj-gwE2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ]
    },
    {
      "metadata": {
        "id": "jfaTxzZ1w__y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this second approach, I switched to a random forest classifier and I made a gridsearch to make some hyperparameters tuning.\n",
        "\n",
        "The gridsearch is not shown in the code below otherwise the notebook will require too much time to run."
      ]
    },
    {
      "metadata": {
        "id": "wcov_DCXgs7v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3eo0ljqzg-KM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rforest = RandomForestClassifier(criterion=\"gini\", max_depth=10, max_features=\"log2\", \n",
        "                                 max_leaf_nodes = 100, min_samples_leaf = 3, min_samples_split = 20, \n",
        "                                 n_estimators= 22000, random_state= 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tg45qSOfg-26",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "e3cae74b-a078-4629-b4f8-b18cb73efee8"
      },
      "cell_type": "code",
      "source": [
        "rforest.fit(X_train, y_train)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
              "            max_depth=10, max_features='log2', max_leaf_nodes=100,\n",
              "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "            min_samples_leaf=3, min_samples_split=20,\n",
              "            min_weight_fraction_leaf=0.0, n_estimators=22000, n_jobs=None,\n",
              "            oob_score=False, random_state=5, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "metadata": {
        "id": "aM8KU3qxhGBM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = rforest.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "296FW5sBdanI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "52121a16-0bf8-4971-df71-5c7848296ff4"
      },
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test,predictions))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          01       1.00      0.54      0.70       134\n",
            "          02       0.66      0.96      0.78       251\n",
            "          03       0.86      0.71      0.78       242\n",
            "          04       0.81      0.64      0.71       271\n",
            "          05       0.89      0.88      0.88       253\n",
            "          06       0.70      0.80      0.75       239\n",
            "          07       0.73      0.61      0.66       127\n",
            "          08       0.60      0.78      0.68       116\n",
            "\n",
            "   micro avg       0.76      0.76      0.76      1633\n",
            "   macro avg       0.78      0.74      0.74      1633\n",
            "weighted avg       0.79      0.76      0.76      1633\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "t9eqMHV3S8i6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Neural network"
      ]
    },
    {
      "metadata": {
        "id": "G-QscoyMxQtn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's build our neural network!\n",
        "\n",
        "To do so, we need to expand the dimensions of our array, adding a third one using the numpy \"expand_dims\" feature."
      ]
    },
    {
      "metadata": {
        "id": "W4i187-Pe-w5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_traincnn = np.expand_dims(X_train, axis=2)\n",
        "x_testcnn = np.expand_dims(X_test, axis=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vnvoCRX1gQCh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "67145dd7-87d9-4df5-8f91-eeb0b27b20cf"
      },
      "cell_type": "code",
      "source": [
        "x_traincnn.shape, x_testcnn.shape"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3315, 40, 1), (1633, 40, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "HZOGIpuefCd3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e19c3030-3153-4ab7-d229-38d69d50c6f1"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from matplotlib.pyplot import specgram\n",
        "import keras\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Input, Flatten, Dropout, Activation\n",
        "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(128, 5,padding='same',\n",
        "                 input_shape=(40,1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(MaxPooling1D(pool_size=(8)))\n",
        "model.add(Conv1D(128, 5,padding='same',))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "opt = keras.optimizers.rmsprop(lr=0.00005, rho=0.9, epsilon=None, decay=0.0)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "LphftMIZzUvz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "With *model.summary* we can see a recap of what we have build:"
      ]
    },
    {
      "metadata": {
        "id": "pIWPB4Zgfic7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "161306f9-99e2-4562-b304-e7fda4a7a01a"
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_1 (Conv1D)            (None, 40, 128)           768       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 5, 128)            82048     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 640)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                6410      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 89,226\n",
            "Trainable params: 89,226\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5qQSBeBhzcLu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can compile and fit our model:"
      ]
    },
    {
      "metadata": {
        "id": "iNI1znbsfpTx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ktdF-nJKfq6F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34034
        },
        "outputId": "216727c4-96a3-4a83-f2ee-f7c72ba7642e"
      },
      "cell_type": "code",
      "source": [
        "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=1000, validation_data=(x_testcnn, y_test))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3315 samples, validate on 1633 samples\n",
            "Epoch 1/1000\n",
            "3315/3315 [==============================] - 2s 638us/step - loss: 8.4918 - acc: 0.1403 - val_loss: 3.7311 - val_acc: 0.1672\n",
            "Epoch 2/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 6.2504 - acc: 0.1523 - val_loss: 3.1803 - val_acc: 0.2174\n",
            "Epoch 3/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 4.6751 - acc: 0.1602 - val_loss: 1.9775 - val_acc: 0.2701\n",
            "Epoch 4/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 3.5738 - acc: 0.1867 - val_loss: 2.0096 - val_acc: 0.2149\n",
            "Epoch 5/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 2.9645 - acc: 0.2006 - val_loss: 1.9828 - val_acc: 0.2174\n",
            "Epoch 6/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 2.4672 - acc: 0.2124 - val_loss: 1.7313 - val_acc: 0.2909\n",
            "Epoch 7/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 2.1490 - acc: 0.2265 - val_loss: 1.8278 - val_acc: 0.2823\n",
            "Epoch 8/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 1.9589 - acc: 0.2564 - val_loss: 1.7916 - val_acc: 0.3374\n",
            "Epoch 9/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 1.9026 - acc: 0.2727 - val_loss: 1.7453 - val_acc: 0.3576\n",
            "Epoch 10/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 1.8503 - acc: 0.2962 - val_loss: 1.7304 - val_acc: 0.3748\n",
            "Epoch 11/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 1.8109 - acc: 0.2974 - val_loss: 1.7229 - val_acc: 0.3301\n",
            "Epoch 12/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 1.7800 - acc: 0.3083 - val_loss: 1.6879 - val_acc: 0.3889\n",
            "Epoch 13/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 1.7413 - acc: 0.3315 - val_loss: 1.6540 - val_acc: 0.4170\n",
            "Epoch 14/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 1.7195 - acc: 0.3409 - val_loss: 1.6318 - val_acc: 0.4256\n",
            "Epoch 15/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 1.6804 - acc: 0.3560 - val_loss: 1.6029 - val_acc: 0.4648\n",
            "Epoch 16/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 1.6570 - acc: 0.3689 - val_loss: 1.5800 - val_acc: 0.4574\n",
            "Epoch 17/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 1.6349 - acc: 0.3674 - val_loss: 1.5630 - val_acc: 0.4489\n",
            "Epoch 18/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 1.6012 - acc: 0.3937 - val_loss: 1.5339 - val_acc: 0.4636\n",
            "Epoch 19/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 1.5907 - acc: 0.4024 - val_loss: 1.5195 - val_acc: 0.4697\n",
            "Epoch 20/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 1.5687 - acc: 0.4075 - val_loss: 1.4994 - val_acc: 0.4691\n",
            "Epoch 21/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 1.5593 - acc: 0.4154 - val_loss: 1.4927 - val_acc: 0.4679\n",
            "Epoch 22/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 1.5381 - acc: 0.4284 - val_loss: 1.4710 - val_acc: 0.4715\n",
            "Epoch 23/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 1.5190 - acc: 0.4335 - val_loss: 1.4504 - val_acc: 0.4942\n",
            "Epoch 24/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 1.4956 - acc: 0.4398 - val_loss: 1.4513 - val_acc: 0.4813\n",
            "Epoch 25/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 1.4905 - acc: 0.4446 - val_loss: 1.4369 - val_acc: 0.4850\n",
            "Epoch 26/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 1.4729 - acc: 0.4492 - val_loss: 1.4127 - val_acc: 0.4930\n",
            "Epoch 27/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 1.4622 - acc: 0.4564 - val_loss: 1.4093 - val_acc: 0.4997\n",
            "Epoch 28/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 1.4242 - acc: 0.4739 - val_loss: 1.3834 - val_acc: 0.5028\n",
            "Epoch 29/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 1.4417 - acc: 0.4576 - val_loss: 1.4012 - val_acc: 0.4911\n",
            "Epoch 30/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 1.4199 - acc: 0.4742 - val_loss: 1.3847 - val_acc: 0.5089\n",
            "Epoch 31/1000\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 1.4055 - acc: 0.4715 - val_loss: 1.3630 - val_acc: 0.4930\n",
            "Epoch 32/1000\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 1.3905 - acc: 0.4851 - val_loss: 1.3470 - val_acc: 0.5113\n",
            "Epoch 33/1000\n",
            "3315/3315 [==============================] - 2s 453us/step - loss: 1.3767 - acc: 0.4908 - val_loss: 1.3517 - val_acc: 0.5150\n",
            "Epoch 34/1000\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 1.3759 - acc: 0.4917 - val_loss: 1.3468 - val_acc: 0.5248\n",
            "Epoch 35/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 1.3572 - acc: 0.4950 - val_loss: 1.3427 - val_acc: 0.5132\n",
            "Epoch 36/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 1.3527 - acc: 0.5068 - val_loss: 1.3209 - val_acc: 0.5248\n",
            "Epoch 37/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 1.3463 - acc: 0.4932 - val_loss: 1.3222 - val_acc: 0.5181\n",
            "Epoch 38/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 1.3300 - acc: 0.5083 - val_loss: 1.3156 - val_acc: 0.5224\n",
            "Epoch 39/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 1.3295 - acc: 0.5059 - val_loss: 1.2900 - val_acc: 0.5383\n",
            "Epoch 40/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 1.3229 - acc: 0.5083 - val_loss: 1.2995 - val_acc: 0.5413\n",
            "Epoch 41/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 1.3061 - acc: 0.5192 - val_loss: 1.2597 - val_acc: 0.5573\n",
            "Epoch 42/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 1.2938 - acc: 0.5237 - val_loss: 1.2766 - val_acc: 0.5315\n",
            "Epoch 43/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 1.2892 - acc: 0.5231 - val_loss: 1.2757 - val_acc: 0.5291\n",
            "Epoch 44/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 1.2782 - acc: 0.5336 - val_loss: 1.2508 - val_acc: 0.5585\n",
            "Epoch 45/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 1.2787 - acc: 0.5261 - val_loss: 1.2619 - val_acc: 0.5444\n",
            "Epoch 46/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 1.2754 - acc: 0.5327 - val_loss: 1.2374 - val_acc: 0.5628\n",
            "Epoch 47/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 1.2686 - acc: 0.5363 - val_loss: 1.2354 - val_acc: 0.5640\n",
            "Epoch 48/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 1.2430 - acc: 0.5412 - val_loss: 1.2213 - val_acc: 0.5603\n",
            "Epoch 49/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 1.2449 - acc: 0.5373 - val_loss: 1.2108 - val_acc: 0.5793\n",
            "Epoch 50/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 1.2295 - acc: 0.5478 - val_loss: 1.2091 - val_acc: 0.5744\n",
            "Epoch 51/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 1.2232 - acc: 0.5566 - val_loss: 1.2222 - val_acc: 0.5603\n",
            "Epoch 52/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 1.2180 - acc: 0.5484 - val_loss: 1.1873 - val_acc: 0.5769\n",
            "Epoch 53/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 1.2196 - acc: 0.5548 - val_loss: 1.1964 - val_acc: 0.5732\n",
            "Epoch 54/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 1.2148 - acc: 0.5532 - val_loss: 1.1733 - val_acc: 0.5879\n",
            "Epoch 55/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 1.2058 - acc: 0.5623 - val_loss: 1.1965 - val_acc: 0.5732\n",
            "Epoch 56/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 1.2065 - acc: 0.5551 - val_loss: 1.1751 - val_acc: 0.5885\n",
            "Epoch 57/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 1.1941 - acc: 0.5683 - val_loss: 1.1885 - val_acc: 0.5628\n",
            "Epoch 58/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 1.1954 - acc: 0.5656 - val_loss: 1.1617 - val_acc: 0.5793\n",
            "Epoch 59/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 1.1705 - acc: 0.5828 - val_loss: 1.1792 - val_acc: 0.5744\n",
            "Epoch 60/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 1.1696 - acc: 0.5771 - val_loss: 1.1474 - val_acc: 0.5848\n",
            "Epoch 61/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 1.1653 - acc: 0.5662 - val_loss: 1.1433 - val_acc: 0.5885\n",
            "Epoch 62/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 1.1789 - acc: 0.5662 - val_loss: 1.1632 - val_acc: 0.5634\n",
            "Epoch 63/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 1.1578 - acc: 0.5722 - val_loss: 1.1558 - val_acc: 0.5775\n",
            "Epoch 64/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 1.1605 - acc: 0.5780 - val_loss: 1.1280 - val_acc: 0.6038\n",
            "Epoch 65/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 1.1389 - acc: 0.5855 - val_loss: 1.1224 - val_acc: 0.6007\n",
            "Epoch 66/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 1.1426 - acc: 0.5822 - val_loss: 1.1445 - val_acc: 0.5824\n",
            "Epoch 67/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 1.1332 - acc: 0.5795 - val_loss: 1.1003 - val_acc: 0.6087\n",
            "Epoch 68/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 1.1355 - acc: 0.5837 - val_loss: 1.1137 - val_acc: 0.6087\n",
            "Epoch 69/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 1.1209 - acc: 0.5879 - val_loss: 1.1012 - val_acc: 0.6050\n",
            "Epoch 70/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 1.1121 - acc: 0.5922 - val_loss: 1.0953 - val_acc: 0.6130\n",
            "Epoch 71/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 1.1230 - acc: 0.5894 - val_loss: 1.0894 - val_acc: 0.6265\n",
            "Epoch 72/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 1.1132 - acc: 0.5910 - val_loss: 1.0911 - val_acc: 0.6160\n",
            "Epoch 73/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 1.1006 - acc: 0.5940 - val_loss: 1.0842 - val_acc: 0.6191\n",
            "Epoch 74/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 1.0893 - acc: 0.6051 - val_loss: 1.0792 - val_acc: 0.6222\n",
            "Epoch 75/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 1.0933 - acc: 0.6057 - val_loss: 1.0720 - val_acc: 0.6179\n",
            "Epoch 76/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 1.0819 - acc: 0.6075 - val_loss: 1.0774 - val_acc: 0.6197\n",
            "Epoch 77/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 1.0685 - acc: 0.6130 - val_loss: 1.0741 - val_acc: 0.6087\n",
            "Epoch 78/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 1.0652 - acc: 0.6214 - val_loss: 1.0737 - val_acc: 0.5995\n",
            "Epoch 79/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 1.0594 - acc: 0.6112 - val_loss: 1.0702 - val_acc: 0.6130\n",
            "Epoch 80/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 1.0700 - acc: 0.6124 - val_loss: 1.0908 - val_acc: 0.6062\n",
            "Epoch 81/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 1.0604 - acc: 0.6097 - val_loss: 1.0788 - val_acc: 0.6173\n",
            "Epoch 82/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 1.0575 - acc: 0.6103 - val_loss: 1.0434 - val_acc: 0.6320\n",
            "Epoch 83/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 1.0577 - acc: 0.6133 - val_loss: 1.0437 - val_acc: 0.6234\n",
            "Epoch 84/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 1.0418 - acc: 0.6190 - val_loss: 1.0506 - val_acc: 0.6246\n",
            "Epoch 85/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 1.0442 - acc: 0.6181 - val_loss: 1.0233 - val_acc: 0.6454\n",
            "Epoch 86/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 1.0379 - acc: 0.6265 - val_loss: 1.0689 - val_acc: 0.6307\n",
            "Epoch 87/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 1.0416 - acc: 0.6241 - val_loss: 1.0444 - val_acc: 0.6350\n",
            "Epoch 88/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 1.0276 - acc: 0.6368 - val_loss: 1.0113 - val_acc: 0.6485\n",
            "Epoch 89/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 1.0160 - acc: 0.6205 - val_loss: 1.0328 - val_acc: 0.6399\n",
            "Epoch 90/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 1.0149 - acc: 0.6250 - val_loss: 1.0111 - val_acc: 0.6528\n",
            "Epoch 91/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 1.0207 - acc: 0.6341 - val_loss: 1.0002 - val_acc: 0.6393\n",
            "Epoch 92/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.9973 - acc: 0.6410 - val_loss: 1.0085 - val_acc: 0.6424\n",
            "Epoch 93/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 1.0036 - acc: 0.6386 - val_loss: 0.9953 - val_acc: 0.6418\n",
            "Epoch 94/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 1.0007 - acc: 0.6398 - val_loss: 1.0250 - val_acc: 0.6387\n",
            "Epoch 95/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 1.0077 - acc: 0.6281 - val_loss: 0.9999 - val_acc: 0.6534\n",
            "Epoch 96/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.9908 - acc: 0.6437 - val_loss: 1.0051 - val_acc: 0.6461\n",
            "Epoch 97/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.9866 - acc: 0.6422 - val_loss: 1.0027 - val_acc: 0.6558\n",
            "Epoch 98/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.9883 - acc: 0.6425 - val_loss: 0.9907 - val_acc: 0.6534\n",
            "Epoch 99/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.9856 - acc: 0.6431 - val_loss: 0.9863 - val_acc: 0.6540\n",
            "Epoch 100/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.9777 - acc: 0.6452 - val_loss: 0.9847 - val_acc: 0.6430\n",
            "Epoch 101/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.9789 - acc: 0.6428 - val_loss: 0.9687 - val_acc: 0.6644\n",
            "Epoch 102/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.9713 - acc: 0.6498 - val_loss: 0.9778 - val_acc: 0.6540\n",
            "Epoch 103/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.9685 - acc: 0.6465 - val_loss: 0.9635 - val_acc: 0.6718\n",
            "Epoch 104/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.9663 - acc: 0.6510 - val_loss: 0.9768 - val_acc: 0.6497\n",
            "Epoch 105/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.9622 - acc: 0.6465 - val_loss: 0.9866 - val_acc: 0.6503\n",
            "Epoch 106/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.9632 - acc: 0.6465 - val_loss: 0.9468 - val_acc: 0.6583\n",
            "Epoch 107/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.9362 - acc: 0.6606 - val_loss: 0.9508 - val_acc: 0.6675\n",
            "Epoch 108/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.9384 - acc: 0.6643 - val_loss: 0.9621 - val_acc: 0.6516\n",
            "Epoch 109/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.9423 - acc: 0.6504 - val_loss: 0.9507 - val_acc: 0.6699\n",
            "Epoch 110/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.9312 - acc: 0.6706 - val_loss: 0.9459 - val_acc: 0.6607\n",
            "Epoch 111/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.9414 - acc: 0.6621 - val_loss: 0.9262 - val_acc: 0.6638\n",
            "Epoch 112/1000\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 0.9301 - acc: 0.6646 - val_loss: 0.9387 - val_acc: 0.6620\n",
            "Epoch 113/1000\n",
            "3315/3315 [==============================] - 1s 449us/step - loss: 0.9247 - acc: 0.6640 - val_loss: 0.9285 - val_acc: 0.6754\n",
            "Epoch 114/1000\n",
            "3315/3315 [==============================] - 2s 457us/step - loss: 0.9324 - acc: 0.6627 - val_loss: 0.9372 - val_acc: 0.6614\n",
            "Epoch 115/1000\n",
            "3315/3315 [==============================] - 2s 455us/step - loss: 0.9092 - acc: 0.6730 - val_loss: 0.9264 - val_acc: 0.6840\n",
            "Epoch 116/1000\n",
            "3315/3315 [==============================] - 2s 453us/step - loss: 0.9213 - acc: 0.6627 - val_loss: 0.9260 - val_acc: 0.6632\n",
            "Epoch 117/1000\n",
            "3315/3315 [==============================] - 1s 452us/step - loss: 0.9081 - acc: 0.6643 - val_loss: 0.9348 - val_acc: 0.6748\n",
            "Epoch 118/1000\n",
            "3315/3315 [==============================] - 2s 454us/step - loss: 0.9064 - acc: 0.6700 - val_loss: 0.9231 - val_acc: 0.6644\n",
            "Epoch 119/1000\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.9146 - acc: 0.6664 - val_loss: 0.9582 - val_acc: 0.6595\n",
            "Epoch 120/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.8997 - acc: 0.6736 - val_loss: 0.9380 - val_acc: 0.6693\n",
            "Epoch 121/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.8848 - acc: 0.6805 - val_loss: 0.9147 - val_acc: 0.6748\n",
            "Epoch 122/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.8972 - acc: 0.6796 - val_loss: 0.9026 - val_acc: 0.6877\n",
            "Epoch 123/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.8790 - acc: 0.6760 - val_loss: 0.9002 - val_acc: 0.6828\n",
            "Epoch 124/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.8856 - acc: 0.6787 - val_loss: 0.9201 - val_acc: 0.6705\n",
            "Epoch 125/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.8840 - acc: 0.6781 - val_loss: 0.9024 - val_acc: 0.6767\n",
            "Epoch 126/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.8689 - acc: 0.6821 - val_loss: 0.9081 - val_acc: 0.6742\n",
            "Epoch 127/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 0.8767 - acc: 0.6839 - val_loss: 0.8931 - val_acc: 0.6877\n",
            "Epoch 128/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.8843 - acc: 0.6799 - val_loss: 0.8967 - val_acc: 0.6822\n",
            "Epoch 129/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.8694 - acc: 0.6808 - val_loss: 0.8945 - val_acc: 0.6816\n",
            "Epoch 130/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.8638 - acc: 0.6938 - val_loss: 0.9107 - val_acc: 0.6785\n",
            "Epoch 131/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.8577 - acc: 0.6863 - val_loss: 0.9208 - val_acc: 0.6503\n",
            "Epoch 132/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.8430 - acc: 0.6983 - val_loss: 0.9029 - val_acc: 0.6754\n",
            "Epoch 133/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.8464 - acc: 0.6974 - val_loss: 0.8875 - val_acc: 0.6730\n",
            "Epoch 134/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.8458 - acc: 0.6869 - val_loss: 0.8671 - val_acc: 0.6908\n",
            "Epoch 135/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.8450 - acc: 0.6872 - val_loss: 0.8707 - val_acc: 0.6920\n",
            "Epoch 136/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.8305 - acc: 0.7053 - val_loss: 0.8673 - val_acc: 0.6895\n",
            "Epoch 137/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.8392 - acc: 0.6929 - val_loss: 0.8662 - val_acc: 0.6877\n",
            "Epoch 138/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.8336 - acc: 0.6956 - val_loss: 0.8506 - val_acc: 0.6944\n",
            "Epoch 139/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.8414 - acc: 0.6923 - val_loss: 0.8607 - val_acc: 0.6944\n",
            "Epoch 140/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.8260 - acc: 0.7026 - val_loss: 0.8392 - val_acc: 0.6975\n",
            "Epoch 141/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.8282 - acc: 0.7002 - val_loss: 0.8469 - val_acc: 0.6859\n",
            "Epoch 142/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.8203 - acc: 0.6989 - val_loss: 0.8646 - val_acc: 0.6810\n",
            "Epoch 143/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.8158 - acc: 0.7023 - val_loss: 0.8583 - val_acc: 0.6981\n",
            "Epoch 144/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.8166 - acc: 0.6950 - val_loss: 0.8593 - val_acc: 0.6944\n",
            "Epoch 145/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.8123 - acc: 0.7062 - val_loss: 0.8507 - val_acc: 0.7012\n",
            "Epoch 146/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.8155 - acc: 0.7062 - val_loss: 0.8485 - val_acc: 0.6963\n",
            "Epoch 147/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.8044 - acc: 0.7101 - val_loss: 0.8367 - val_acc: 0.7030\n",
            "Epoch 148/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.7944 - acc: 0.7173 - val_loss: 0.8441 - val_acc: 0.7018\n",
            "Epoch 149/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.7957 - acc: 0.7261 - val_loss: 0.8433 - val_acc: 0.6944\n",
            "Epoch 150/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.7943 - acc: 0.7186 - val_loss: 0.8601 - val_acc: 0.6987\n",
            "Epoch 151/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.7858 - acc: 0.7167 - val_loss: 0.8404 - val_acc: 0.6889\n",
            "Epoch 152/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.7814 - acc: 0.7210 - val_loss: 0.8217 - val_acc: 0.7110\n",
            "Epoch 153/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.7955 - acc: 0.7143 - val_loss: 0.8418 - val_acc: 0.7061\n",
            "Epoch 154/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.7948 - acc: 0.7062 - val_loss: 0.8844 - val_acc: 0.6840\n",
            "Epoch 155/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.7827 - acc: 0.7207 - val_loss: 0.8180 - val_acc: 0.7165\n",
            "Epoch 156/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.7680 - acc: 0.7285 - val_loss: 0.8179 - val_acc: 0.7024\n",
            "Epoch 157/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.7853 - acc: 0.7125 - val_loss: 0.8257 - val_acc: 0.6926\n",
            "Epoch 158/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.7703 - acc: 0.7195 - val_loss: 0.8144 - val_acc: 0.6993\n",
            "Epoch 159/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.7654 - acc: 0.7216 - val_loss: 0.8203 - val_acc: 0.7183\n",
            "Epoch 160/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.7692 - acc: 0.7210 - val_loss: 0.8268 - val_acc: 0.6950\n",
            "Epoch 161/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.7648 - acc: 0.7243 - val_loss: 0.8034 - val_acc: 0.7146\n",
            "Epoch 162/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.7650 - acc: 0.7131 - val_loss: 0.8219 - val_acc: 0.7048\n",
            "Epoch 163/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.7590 - acc: 0.7339 - val_loss: 0.7991 - val_acc: 0.7110\n",
            "Epoch 164/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.7501 - acc: 0.7258 - val_loss: 0.8206 - val_acc: 0.7085\n",
            "Epoch 165/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.7554 - acc: 0.7237 - val_loss: 0.8077 - val_acc: 0.7091\n",
            "Epoch 166/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.7474 - acc: 0.7327 - val_loss: 0.8105 - val_acc: 0.7134\n",
            "Epoch 167/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.7510 - acc: 0.7270 - val_loss: 0.8148 - val_acc: 0.7091\n",
            "Epoch 168/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.7440 - acc: 0.7370 - val_loss: 0.7933 - val_acc: 0.7152\n",
            "Epoch 169/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.7354 - acc: 0.7379 - val_loss: 0.7991 - val_acc: 0.7159\n",
            "Epoch 170/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.7429 - acc: 0.7336 - val_loss: 0.7818 - val_acc: 0.7159\n",
            "Epoch 171/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.7368 - acc: 0.7342 - val_loss: 0.8188 - val_acc: 0.6987\n",
            "Epoch 172/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.7320 - acc: 0.7376 - val_loss: 0.7739 - val_acc: 0.7238\n",
            "Epoch 173/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.7171 - acc: 0.7481 - val_loss: 0.8096 - val_acc: 0.7085\n",
            "Epoch 174/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.7224 - acc: 0.7406 - val_loss: 0.7808 - val_acc: 0.7208\n",
            "Epoch 175/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.7208 - acc: 0.7448 - val_loss: 0.7948 - val_acc: 0.7165\n",
            "Epoch 176/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.7176 - acc: 0.7469 - val_loss: 0.8648 - val_acc: 0.6852\n",
            "Epoch 177/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.7299 - acc: 0.7427 - val_loss: 0.7947 - val_acc: 0.7103\n",
            "Epoch 178/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.7073 - acc: 0.7520 - val_loss: 0.7581 - val_acc: 0.7367\n",
            "Epoch 179/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.7060 - acc: 0.7436 - val_loss: 0.7701 - val_acc: 0.7275\n",
            "Epoch 180/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.7044 - acc: 0.7424 - val_loss: 0.7641 - val_acc: 0.7287\n",
            "Epoch 181/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.6986 - acc: 0.7517 - val_loss: 0.7651 - val_acc: 0.7250\n",
            "Epoch 182/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.7057 - acc: 0.7478 - val_loss: 0.7880 - val_acc: 0.7140\n",
            "Epoch 183/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.6939 - acc: 0.7511 - val_loss: 0.7640 - val_acc: 0.7177\n",
            "Epoch 184/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.7052 - acc: 0.7487 - val_loss: 0.7662 - val_acc: 0.7287\n",
            "Epoch 185/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.6967 - acc: 0.7430 - val_loss: 0.7471 - val_acc: 0.7269\n",
            "Epoch 186/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.6946 - acc: 0.7496 - val_loss: 0.7605 - val_acc: 0.7342\n",
            "Epoch 187/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.6867 - acc: 0.7463 - val_loss: 0.7604 - val_acc: 0.7244\n",
            "Epoch 188/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.6966 - acc: 0.7517 - val_loss: 0.7641 - val_acc: 0.7299\n",
            "Epoch 189/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.6841 - acc: 0.7523 - val_loss: 0.7664 - val_acc: 0.7355\n",
            "Epoch 190/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.6945 - acc: 0.7499 - val_loss: 0.7386 - val_acc: 0.7453\n",
            "Epoch 191/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.6860 - acc: 0.7569 - val_loss: 0.7558 - val_acc: 0.7244\n",
            "Epoch 192/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.6803 - acc: 0.7548 - val_loss: 0.7609 - val_acc: 0.7214\n",
            "Epoch 193/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.6809 - acc: 0.7532 - val_loss: 0.7502 - val_acc: 0.7220\n",
            "Epoch 194/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.6663 - acc: 0.7617 - val_loss: 0.7274 - val_acc: 0.7355\n",
            "Epoch 195/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.6756 - acc: 0.7529 - val_loss: 0.7298 - val_acc: 0.7342\n",
            "Epoch 196/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.6663 - acc: 0.7623 - val_loss: 0.7480 - val_acc: 0.7257\n",
            "Epoch 197/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.6731 - acc: 0.7623 - val_loss: 0.7359 - val_acc: 0.7465\n",
            "Epoch 198/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.6581 - acc: 0.7653 - val_loss: 0.7431 - val_acc: 0.7287\n",
            "Epoch 199/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.6575 - acc: 0.7629 - val_loss: 0.7196 - val_acc: 0.7520\n",
            "Epoch 200/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.6703 - acc: 0.7605 - val_loss: 0.7352 - val_acc: 0.7422\n",
            "Epoch 201/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.6541 - acc: 0.7671 - val_loss: 0.7227 - val_acc: 0.7379\n",
            "Epoch 202/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.6674 - acc: 0.7554 - val_loss: 0.7496 - val_acc: 0.7459\n",
            "Epoch 203/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.6448 - acc: 0.7695 - val_loss: 0.7261 - val_acc: 0.7391\n",
            "Epoch 204/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.6534 - acc: 0.7590 - val_loss: 0.7321 - val_acc: 0.7379\n",
            "Epoch 205/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.6449 - acc: 0.7656 - val_loss: 0.7207 - val_acc: 0.7410\n",
            "Epoch 206/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.6398 - acc: 0.7707 - val_loss: 0.7313 - val_acc: 0.7391\n",
            "Epoch 207/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.6311 - acc: 0.7686 - val_loss: 0.7040 - val_acc: 0.7544\n",
            "Epoch 208/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.6384 - acc: 0.7659 - val_loss: 0.7234 - val_acc: 0.7416\n",
            "Epoch 209/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.6440 - acc: 0.7659 - val_loss: 0.7026 - val_acc: 0.7563\n",
            "Epoch 210/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.6418 - acc: 0.7722 - val_loss: 0.7159 - val_acc: 0.7397\n",
            "Epoch 211/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.6383 - acc: 0.7653 - val_loss: 0.6960 - val_acc: 0.7434\n",
            "Epoch 212/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.6317 - acc: 0.7704 - val_loss: 0.7112 - val_acc: 0.7422\n",
            "Epoch 213/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.6374 - acc: 0.7719 - val_loss: 0.7046 - val_acc: 0.7410\n",
            "Epoch 214/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.6206 - acc: 0.7831 - val_loss: 0.7220 - val_acc: 0.7385\n",
            "Epoch 215/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.6348 - acc: 0.7725 - val_loss: 0.7022 - val_acc: 0.7428\n",
            "Epoch 216/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.6157 - acc: 0.7762 - val_loss: 0.6881 - val_acc: 0.7600\n",
            "Epoch 217/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.6242 - acc: 0.7692 - val_loss: 0.6969 - val_acc: 0.7606\n",
            "Epoch 218/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.6214 - acc: 0.7801 - val_loss: 0.6924 - val_acc: 0.7520\n",
            "Epoch 219/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.6228 - acc: 0.7765 - val_loss: 0.7130 - val_acc: 0.7495\n",
            "Epoch 220/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.6147 - acc: 0.7804 - val_loss: 0.7033 - val_acc: 0.7514\n",
            "Epoch 221/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.6070 - acc: 0.7813 - val_loss: 0.6934 - val_acc: 0.7532\n",
            "Epoch 222/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.6103 - acc: 0.7843 - val_loss: 0.7041 - val_acc: 0.7373\n",
            "Epoch 223/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.6031 - acc: 0.7837 - val_loss: 0.6821 - val_acc: 0.7575\n",
            "Epoch 224/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.6015 - acc: 0.7834 - val_loss: 0.6831 - val_acc: 0.7593\n",
            "Epoch 225/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.6023 - acc: 0.7864 - val_loss: 0.6838 - val_acc: 0.7508\n",
            "Epoch 226/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.6056 - acc: 0.7750 - val_loss: 0.6876 - val_acc: 0.7532\n",
            "Epoch 227/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.5998 - acc: 0.7910 - val_loss: 0.6805 - val_acc: 0.7697\n",
            "Epoch 228/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.6024 - acc: 0.7903 - val_loss: 0.6875 - val_acc: 0.7606\n",
            "Epoch 229/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.5956 - acc: 0.7931 - val_loss: 0.6686 - val_acc: 0.7642\n",
            "Epoch 230/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.5867 - acc: 0.7870 - val_loss: 0.6707 - val_acc: 0.7600\n",
            "Epoch 231/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.5812 - acc: 0.8006 - val_loss: 0.6818 - val_acc: 0.7642\n",
            "Epoch 232/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.5828 - acc: 0.7931 - val_loss: 0.7044 - val_acc: 0.7342\n",
            "Epoch 233/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.5921 - acc: 0.7864 - val_loss: 0.6715 - val_acc: 0.7746\n",
            "Epoch 234/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.5813 - acc: 0.7916 - val_loss: 0.6719 - val_acc: 0.7600\n",
            "Epoch 235/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.5826 - acc: 0.7888 - val_loss: 0.6567 - val_acc: 0.7734\n",
            "Epoch 236/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.5847 - acc: 0.7903 - val_loss: 0.6719 - val_acc: 0.7630\n",
            "Epoch 237/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.5816 - acc: 0.7879 - val_loss: 0.6728 - val_acc: 0.7581\n",
            "Epoch 238/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.5791 - acc: 0.7979 - val_loss: 0.6755 - val_acc: 0.7630\n",
            "Epoch 239/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.5766 - acc: 0.7994 - val_loss: 0.6654 - val_acc: 0.7728\n",
            "Epoch 240/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.5698 - acc: 0.8036 - val_loss: 0.6404 - val_acc: 0.7814\n",
            "Epoch 241/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.5836 - acc: 0.7958 - val_loss: 0.6708 - val_acc: 0.7648\n",
            "Epoch 242/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.5594 - acc: 0.8048 - val_loss: 0.6669 - val_acc: 0.7740\n",
            "Epoch 243/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.5676 - acc: 0.7922 - val_loss: 0.6547 - val_acc: 0.7624\n",
            "Epoch 244/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.5735 - acc: 0.7958 - val_loss: 0.6420 - val_acc: 0.7844\n",
            "Epoch 245/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.5606 - acc: 0.8048 - val_loss: 0.6623 - val_acc: 0.7642\n",
            "Epoch 246/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.5697 - acc: 0.7985 - val_loss: 0.6542 - val_acc: 0.7685\n",
            "Epoch 247/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.5545 - acc: 0.8063 - val_loss: 0.6673 - val_acc: 0.7648\n",
            "Epoch 248/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.5520 - acc: 0.7955 - val_loss: 0.6312 - val_acc: 0.7795\n",
            "Epoch 249/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.5519 - acc: 0.8018 - val_loss: 0.6406 - val_acc: 0.7710\n",
            "Epoch 250/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.5549 - acc: 0.7958 - val_loss: 0.6516 - val_acc: 0.7661\n",
            "Epoch 251/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.5425 - acc: 0.8127 - val_loss: 0.6458 - val_acc: 0.7710\n",
            "Epoch 252/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.5422 - acc: 0.8066 - val_loss: 0.6313 - val_acc: 0.7820\n",
            "Epoch 253/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.5555 - acc: 0.8003 - val_loss: 0.6560 - val_acc: 0.7661\n",
            "Epoch 254/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.5532 - acc: 0.8003 - val_loss: 0.6392 - val_acc: 0.7844\n",
            "Epoch 255/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.5412 - acc: 0.8048 - val_loss: 0.6355 - val_acc: 0.7710\n",
            "Epoch 256/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.5383 - acc: 0.8118 - val_loss: 0.6261 - val_acc: 0.7857\n",
            "Epoch 257/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.5414 - acc: 0.8127 - val_loss: 0.6587 - val_acc: 0.7765\n",
            "Epoch 258/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.5446 - acc: 0.8081 - val_loss: 0.6221 - val_acc: 0.7875\n",
            "Epoch 259/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.5144 - acc: 0.8193 - val_loss: 0.6315 - val_acc: 0.7765\n",
            "Epoch 260/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.5315 - acc: 0.8142 - val_loss: 0.6305 - val_acc: 0.7679\n",
            "Epoch 261/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.5261 - acc: 0.8133 - val_loss: 0.6225 - val_acc: 0.7753\n",
            "Epoch 262/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.5287 - acc: 0.8124 - val_loss: 0.6335 - val_acc: 0.7820\n",
            "Epoch 263/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.5331 - acc: 0.8066 - val_loss: 0.6492 - val_acc: 0.7661\n",
            "Epoch 264/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.5275 - acc: 0.8057 - val_loss: 0.6147 - val_acc: 0.7869\n",
            "Epoch 265/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.5260 - acc: 0.8130 - val_loss: 0.6349 - val_acc: 0.7771\n",
            "Epoch 266/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.5260 - acc: 0.8106 - val_loss: 0.6118 - val_acc: 0.7924\n",
            "Epoch 267/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.5181 - acc: 0.8112 - val_loss: 0.6109 - val_acc: 0.7802\n",
            "Epoch 268/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.5154 - acc: 0.8154 - val_loss: 0.6241 - val_acc: 0.7746\n",
            "Epoch 269/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.5120 - acc: 0.8205 - val_loss: 0.6154 - val_acc: 0.7912\n",
            "Epoch 270/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.5192 - acc: 0.8157 - val_loss: 0.6520 - val_acc: 0.7618\n",
            "Epoch 271/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.5070 - acc: 0.8220 - val_loss: 0.6338 - val_acc: 0.7789\n",
            "Epoch 272/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.5166 - acc: 0.8181 - val_loss: 0.6171 - val_acc: 0.7906\n",
            "Epoch 273/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.5004 - acc: 0.8202 - val_loss: 0.6253 - val_acc: 0.7851\n",
            "Epoch 274/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.5196 - acc: 0.8106 - val_loss: 0.6335 - val_acc: 0.7765\n",
            "Epoch 275/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.5183 - acc: 0.8103 - val_loss: 0.6321 - val_acc: 0.7777\n",
            "Epoch 276/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.5200 - acc: 0.8087 - val_loss: 0.6580 - val_acc: 0.7655\n",
            "Epoch 277/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.4985 - acc: 0.8229 - val_loss: 0.5902 - val_acc: 0.8040\n",
            "Epoch 278/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.5019 - acc: 0.8184 - val_loss: 0.6010 - val_acc: 0.7875\n",
            "Epoch 279/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.4934 - acc: 0.8247 - val_loss: 0.5924 - val_acc: 0.7900\n",
            "Epoch 280/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.4893 - acc: 0.8326 - val_loss: 0.6082 - val_acc: 0.7930\n",
            "Epoch 281/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.4971 - acc: 0.8268 - val_loss: 0.6015 - val_acc: 0.7906\n",
            "Epoch 282/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.4986 - acc: 0.8169 - val_loss: 0.5799 - val_acc: 0.8089\n",
            "Epoch 283/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.4898 - acc: 0.8311 - val_loss: 0.5888 - val_acc: 0.7961\n",
            "Epoch 284/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.4734 - acc: 0.8404 - val_loss: 0.5860 - val_acc: 0.7924\n",
            "Epoch 285/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.4831 - acc: 0.8296 - val_loss: 0.5971 - val_acc: 0.7930\n",
            "Epoch 286/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.4929 - acc: 0.8275 - val_loss: 0.6039 - val_acc: 0.7887\n",
            "Epoch 287/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.4911 - acc: 0.8256 - val_loss: 0.5864 - val_acc: 0.7991\n",
            "Epoch 288/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.4706 - acc: 0.8374 - val_loss: 0.5924 - val_acc: 0.7991\n",
            "Epoch 289/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.4775 - acc: 0.8305 - val_loss: 0.5930 - val_acc: 0.7998\n",
            "Epoch 290/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.4802 - acc: 0.8323 - val_loss: 0.5854 - val_acc: 0.7979\n",
            "Epoch 291/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.4895 - acc: 0.8268 - val_loss: 0.5950 - val_acc: 0.7906\n",
            "Epoch 292/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.4663 - acc: 0.8383 - val_loss: 0.5844 - val_acc: 0.7955\n",
            "Epoch 293/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.4761 - acc: 0.8374 - val_loss: 0.5936 - val_acc: 0.7967\n",
            "Epoch 294/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.4868 - acc: 0.8262 - val_loss: 0.5804 - val_acc: 0.7979\n",
            "Epoch 295/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.4866 - acc: 0.8284 - val_loss: 0.6033 - val_acc: 0.7924\n",
            "Epoch 296/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.4788 - acc: 0.8311 - val_loss: 0.5742 - val_acc: 0.7930\n",
            "Epoch 297/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.4746 - acc: 0.8332 - val_loss: 0.5878 - val_acc: 0.7991\n",
            "Epoch 298/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.4806 - acc: 0.8296 - val_loss: 0.5956 - val_acc: 0.7942\n",
            "Epoch 299/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.4570 - acc: 0.8465 - val_loss: 0.5704 - val_acc: 0.8010\n",
            "Epoch 300/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.4795 - acc: 0.8311 - val_loss: 0.5595 - val_acc: 0.8163\n",
            "Epoch 301/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.4491 - acc: 0.8468 - val_loss: 0.6167 - val_acc: 0.7802\n",
            "Epoch 302/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.4669 - acc: 0.8353 - val_loss: 0.5647 - val_acc: 0.8138\n",
            "Epoch 303/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.4653 - acc: 0.8407 - val_loss: 0.5742 - val_acc: 0.8089\n",
            "Epoch 304/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.4692 - acc: 0.8323 - val_loss: 0.5708 - val_acc: 0.8004\n",
            "Epoch 305/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.4573 - acc: 0.8416 - val_loss: 0.5755 - val_acc: 0.8053\n",
            "Epoch 306/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.4584 - acc: 0.8350 - val_loss: 0.5789 - val_acc: 0.8004\n",
            "Epoch 307/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.4459 - acc: 0.8462 - val_loss: 0.5618 - val_acc: 0.8181\n",
            "Epoch 308/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.4603 - acc: 0.8344 - val_loss: 0.5801 - val_acc: 0.7936\n",
            "Epoch 309/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.4578 - acc: 0.8443 - val_loss: 0.5654 - val_acc: 0.8138\n",
            "Epoch 310/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.4513 - acc: 0.8383 - val_loss: 0.5734 - val_acc: 0.8004\n",
            "Epoch 311/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.4421 - acc: 0.8498 - val_loss: 0.5756 - val_acc: 0.8126\n",
            "Epoch 312/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.4411 - acc: 0.8459 - val_loss: 0.5542 - val_acc: 0.8169\n",
            "Epoch 313/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.4522 - acc: 0.8374 - val_loss: 0.5777 - val_acc: 0.7998\n",
            "Epoch 314/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.4464 - acc: 0.8437 - val_loss: 0.5850 - val_acc: 0.8034\n",
            "Epoch 315/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.4476 - acc: 0.8480 - val_loss: 0.5975 - val_acc: 0.7979\n",
            "Epoch 316/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.4457 - acc: 0.8501 - val_loss: 0.5423 - val_acc: 0.8157\n",
            "Epoch 317/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.4415 - acc: 0.8456 - val_loss: 0.5728 - val_acc: 0.8022\n",
            "Epoch 318/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.4387 - acc: 0.8480 - val_loss: 0.5626 - val_acc: 0.7985\n",
            "Epoch 319/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.4298 - acc: 0.8516 - val_loss: 0.5722 - val_acc: 0.8016\n",
            "Epoch 320/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.4415 - acc: 0.8416 - val_loss: 0.5726 - val_acc: 0.7985\n",
            "Epoch 321/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.4315 - acc: 0.8555 - val_loss: 0.5532 - val_acc: 0.8053\n",
            "Epoch 322/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.4358 - acc: 0.8465 - val_loss: 0.5889 - val_acc: 0.7942\n",
            "Epoch 323/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.4283 - acc: 0.8480 - val_loss: 0.5427 - val_acc: 0.8151\n",
            "Epoch 324/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.4393 - acc: 0.8456 - val_loss: 0.5662 - val_acc: 0.8077\n",
            "Epoch 325/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.4339 - acc: 0.8492 - val_loss: 0.5674 - val_acc: 0.8077\n",
            "Epoch 326/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.4354 - acc: 0.8434 - val_loss: 0.5369 - val_acc: 0.8120\n",
            "Epoch 327/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.4208 - acc: 0.8537 - val_loss: 0.5517 - val_acc: 0.8071\n",
            "Epoch 328/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.4250 - acc: 0.8537 - val_loss: 0.5593 - val_acc: 0.8108\n",
            "Epoch 329/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.4272 - acc: 0.8540 - val_loss: 0.5445 - val_acc: 0.8151\n",
            "Epoch 330/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.4170 - acc: 0.8603 - val_loss: 0.5471 - val_acc: 0.8071\n",
            "Epoch 331/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.4077 - acc: 0.8528 - val_loss: 0.5481 - val_acc: 0.8120\n",
            "Epoch 332/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.4135 - acc: 0.8540 - val_loss: 0.5264 - val_acc: 0.8267\n",
            "Epoch 333/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.4239 - acc: 0.8486 - val_loss: 0.5732 - val_acc: 0.8004\n",
            "Epoch 334/1000\n",
            "3315/3315 [==============================] - 1s 446us/step - loss: 0.4223 - acc: 0.8504 - val_loss: 0.5639 - val_acc: 0.8126\n",
            "Epoch 335/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.4264 - acc: 0.8570 - val_loss: 0.5528 - val_acc: 0.8108\n",
            "Epoch 336/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.4228 - acc: 0.8546 - val_loss: 0.5440 - val_acc: 0.8132\n",
            "Epoch 337/1000\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.4112 - acc: 0.8576 - val_loss: 0.5205 - val_acc: 0.8322\n",
            "Epoch 338/1000\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.4105 - acc: 0.8609 - val_loss: 0.5279 - val_acc: 0.8138\n",
            "Epoch 339/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.4098 - acc: 0.8549 - val_loss: 0.5283 - val_acc: 0.8175\n",
            "Epoch 340/1000\n",
            "3315/3315 [==============================] - 1s 450us/step - loss: 0.4033 - acc: 0.8664 - val_loss: 0.5289 - val_acc: 0.8218\n",
            "Epoch 341/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.4107 - acc: 0.8585 - val_loss: 0.5094 - val_acc: 0.8365\n",
            "Epoch 342/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.3980 - acc: 0.8700 - val_loss: 0.5135 - val_acc: 0.8298\n",
            "Epoch 343/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.4015 - acc: 0.8637 - val_loss: 0.5157 - val_acc: 0.8298\n",
            "Epoch 344/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.3967 - acc: 0.8652 - val_loss: 0.5379 - val_acc: 0.8163\n",
            "Epoch 345/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.4005 - acc: 0.8585 - val_loss: 0.5393 - val_acc: 0.8279\n",
            "Epoch 346/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.4003 - acc: 0.8561 - val_loss: 0.5298 - val_acc: 0.8291\n",
            "Epoch 347/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.4211 - acc: 0.8525 - val_loss: 0.5538 - val_acc: 0.8016\n",
            "Epoch 348/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.3971 - acc: 0.8633 - val_loss: 0.5110 - val_acc: 0.8279\n",
            "Epoch 349/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.3941 - acc: 0.8658 - val_loss: 0.5219 - val_acc: 0.8255\n",
            "Epoch 350/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.3869 - acc: 0.8703 - val_loss: 0.5222 - val_acc: 0.8285\n",
            "Epoch 351/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.3934 - acc: 0.8564 - val_loss: 0.5244 - val_acc: 0.8249\n",
            "Epoch 352/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.3931 - acc: 0.8736 - val_loss: 0.5429 - val_acc: 0.8157\n",
            "Epoch 353/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.3989 - acc: 0.8646 - val_loss: 0.5306 - val_acc: 0.8200\n",
            "Epoch 354/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.3898 - acc: 0.8591 - val_loss: 0.5205 - val_acc: 0.8310\n",
            "Epoch 355/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.3899 - acc: 0.8685 - val_loss: 0.5028 - val_acc: 0.8340\n",
            "Epoch 356/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.3898 - acc: 0.8612 - val_loss: 0.4917 - val_acc: 0.8457\n",
            "Epoch 357/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.3988 - acc: 0.8618 - val_loss: 0.5176 - val_acc: 0.8255\n",
            "Epoch 358/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.3907 - acc: 0.8661 - val_loss: 0.5204 - val_acc: 0.8230\n",
            "Epoch 359/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.3847 - acc: 0.8646 - val_loss: 0.5290 - val_acc: 0.8175\n",
            "Epoch 360/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.3884 - acc: 0.8609 - val_loss: 0.5428 - val_acc: 0.8151\n",
            "Epoch 361/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.3842 - acc: 0.8688 - val_loss: 0.4984 - val_acc: 0.8445\n",
            "Epoch 362/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.3806 - acc: 0.8682 - val_loss: 0.4985 - val_acc: 0.8402\n",
            "Epoch 363/1000\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.3959 - acc: 0.8594 - val_loss: 0.5239 - val_acc: 0.8249\n",
            "Epoch 364/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.3814 - acc: 0.8727 - val_loss: 0.5243 - val_acc: 0.8279\n",
            "Epoch 365/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.3749 - acc: 0.8706 - val_loss: 0.5027 - val_acc: 0.8347\n",
            "Epoch 366/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.3902 - acc: 0.8633 - val_loss: 0.5276 - val_acc: 0.8255\n",
            "Epoch 367/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.3759 - acc: 0.8694 - val_loss: 0.5117 - val_acc: 0.8279\n",
            "Epoch 368/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.3743 - acc: 0.8757 - val_loss: 0.5027 - val_acc: 0.8340\n",
            "Epoch 369/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.3837 - acc: 0.8751 - val_loss: 0.5373 - val_acc: 0.8096\n",
            "Epoch 370/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.3778 - acc: 0.8733 - val_loss: 0.5055 - val_acc: 0.8371\n",
            "Epoch 371/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.3730 - acc: 0.8757 - val_loss: 0.4970 - val_acc: 0.8347\n",
            "Epoch 372/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.3707 - acc: 0.8697 - val_loss: 0.4908 - val_acc: 0.8377\n",
            "Epoch 373/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.3720 - acc: 0.8763 - val_loss: 0.5071 - val_acc: 0.8298\n",
            "Epoch 374/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.3657 - acc: 0.8763 - val_loss: 0.4960 - val_acc: 0.8334\n",
            "Epoch 375/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.3679 - acc: 0.8673 - val_loss: 0.4960 - val_acc: 0.8285\n",
            "Epoch 376/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.3733 - acc: 0.8751 - val_loss: 0.4961 - val_acc: 0.8340\n",
            "Epoch 377/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.3664 - acc: 0.8748 - val_loss: 0.4944 - val_acc: 0.8353\n",
            "Epoch 378/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.3485 - acc: 0.8802 - val_loss: 0.4883 - val_acc: 0.8377\n",
            "Epoch 379/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.3581 - acc: 0.8854 - val_loss: 0.5186 - val_acc: 0.8273\n",
            "Epoch 380/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.3570 - acc: 0.8766 - val_loss: 0.5015 - val_acc: 0.8273\n",
            "Epoch 381/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.3602 - acc: 0.8730 - val_loss: 0.4758 - val_acc: 0.8389\n",
            "Epoch 382/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.3594 - acc: 0.8784 - val_loss: 0.4842 - val_acc: 0.8432\n",
            "Epoch 383/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.3575 - acc: 0.8790 - val_loss: 0.5090 - val_acc: 0.8316\n",
            "Epoch 384/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.3542 - acc: 0.8736 - val_loss: 0.5070 - val_acc: 0.8298\n",
            "Epoch 385/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.3735 - acc: 0.8724 - val_loss: 0.5072 - val_acc: 0.8261\n",
            "Epoch 386/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.3655 - acc: 0.8790 - val_loss: 0.4862 - val_acc: 0.8383\n",
            "Epoch 387/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.3610 - acc: 0.8790 - val_loss: 0.5027 - val_acc: 0.8316\n",
            "Epoch 388/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.3382 - acc: 0.8836 - val_loss: 0.5050 - val_acc: 0.8230\n",
            "Epoch 389/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.3497 - acc: 0.8839 - val_loss: 0.4919 - val_acc: 0.8402\n",
            "Epoch 390/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 0.3480 - acc: 0.8811 - val_loss: 0.4899 - val_acc: 0.8396\n",
            "Epoch 391/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.3363 - acc: 0.8854 - val_loss: 0.5062 - val_acc: 0.8377\n",
            "Epoch 392/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.3430 - acc: 0.8817 - val_loss: 0.5179 - val_acc: 0.8132\n",
            "Epoch 393/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.3508 - acc: 0.8781 - val_loss: 0.4941 - val_acc: 0.8316\n",
            "Epoch 394/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.3545 - acc: 0.8781 - val_loss: 0.4965 - val_acc: 0.8383\n",
            "Epoch 395/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 0.3560 - acc: 0.8811 - val_loss: 0.4908 - val_acc: 0.8377\n",
            "Epoch 396/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.3499 - acc: 0.8851 - val_loss: 0.4732 - val_acc: 0.8494\n",
            "Epoch 397/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.3281 - acc: 0.8887 - val_loss: 0.4786 - val_acc: 0.8408\n",
            "Epoch 398/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.3448 - acc: 0.8811 - val_loss: 0.4763 - val_acc: 0.8469\n",
            "Epoch 399/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.3449 - acc: 0.8778 - val_loss: 0.4709 - val_acc: 0.8426\n",
            "Epoch 400/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.3415 - acc: 0.8845 - val_loss: 0.4833 - val_acc: 0.8359\n",
            "Epoch 401/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.3326 - acc: 0.8884 - val_loss: 0.4727 - val_acc: 0.8506\n",
            "Epoch 402/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.3291 - acc: 0.8866 - val_loss: 0.4744 - val_acc: 0.8414\n",
            "Epoch 403/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.3536 - acc: 0.8772 - val_loss: 0.4771 - val_acc: 0.8408\n",
            "Epoch 404/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.3377 - acc: 0.8887 - val_loss: 0.4919 - val_acc: 0.8463\n",
            "Epoch 405/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.3315 - acc: 0.8869 - val_loss: 0.4791 - val_acc: 0.8469\n",
            "Epoch 406/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.3233 - acc: 0.8890 - val_loss: 0.5358 - val_acc: 0.8059\n",
            "Epoch 407/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.3378 - acc: 0.8766 - val_loss: 0.4840 - val_acc: 0.8402\n",
            "Epoch 408/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.3347 - acc: 0.8836 - val_loss: 0.4975 - val_acc: 0.8371\n",
            "Epoch 409/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.3416 - acc: 0.8805 - val_loss: 0.4836 - val_acc: 0.8469\n",
            "Epoch 410/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.3248 - acc: 0.8863 - val_loss: 0.4768 - val_acc: 0.8543\n",
            "Epoch 411/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.3479 - acc: 0.8824 - val_loss: 0.4890 - val_acc: 0.8383\n",
            "Epoch 412/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.3322 - acc: 0.8893 - val_loss: 0.4591 - val_acc: 0.8518\n",
            "Epoch 413/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.3231 - acc: 0.8926 - val_loss: 0.4906 - val_acc: 0.8359\n",
            "Epoch 414/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.3286 - acc: 0.8914 - val_loss: 0.4826 - val_acc: 0.8494\n",
            "Epoch 415/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.3239 - acc: 0.8935 - val_loss: 0.4943 - val_acc: 0.8291\n",
            "Epoch 416/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.3275 - acc: 0.8878 - val_loss: 0.4633 - val_acc: 0.8432\n",
            "Epoch 417/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.3159 - acc: 0.8989 - val_loss: 0.4539 - val_acc: 0.8579\n",
            "Epoch 418/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.3289 - acc: 0.8938 - val_loss: 0.4619 - val_acc: 0.8585\n",
            "Epoch 419/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.3133 - acc: 0.8926 - val_loss: 0.5060 - val_acc: 0.8218\n",
            "Epoch 420/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.3107 - acc: 0.8923 - val_loss: 0.4748 - val_acc: 0.8365\n",
            "Epoch 421/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.3253 - acc: 0.8878 - val_loss: 0.4433 - val_acc: 0.8579\n",
            "Epoch 422/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.3118 - acc: 0.8962 - val_loss: 0.4587 - val_acc: 0.8494\n",
            "Epoch 423/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.3258 - acc: 0.8845 - val_loss: 0.4493 - val_acc: 0.8561\n",
            "Epoch 424/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.3177 - acc: 0.8941 - val_loss: 0.4588 - val_acc: 0.8579\n",
            "Epoch 425/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.3174 - acc: 0.8836 - val_loss: 0.4595 - val_acc: 0.8445\n",
            "Epoch 426/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.3145 - acc: 0.8896 - val_loss: 0.4640 - val_acc: 0.8451\n",
            "Epoch 427/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.3256 - acc: 0.8872 - val_loss: 0.4674 - val_acc: 0.8463\n",
            "Epoch 428/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.3114 - acc: 0.8971 - val_loss: 0.4675 - val_acc: 0.8518\n",
            "Epoch 429/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.3156 - acc: 0.8983 - val_loss: 0.4644 - val_acc: 0.8610\n",
            "Epoch 430/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.3193 - acc: 0.8842 - val_loss: 0.4531 - val_acc: 0.8604\n",
            "Epoch 431/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.3168 - acc: 0.8959 - val_loss: 0.4656 - val_acc: 0.8457\n",
            "Epoch 432/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.2973 - acc: 0.9008 - val_loss: 0.4442 - val_acc: 0.8628\n",
            "Epoch 433/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.3001 - acc: 0.8998 - val_loss: 0.4398 - val_acc: 0.8567\n",
            "Epoch 434/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.3084 - acc: 0.8914 - val_loss: 0.4440 - val_acc: 0.8647\n",
            "Epoch 435/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.2993 - acc: 0.9017 - val_loss: 0.4667 - val_acc: 0.8451\n",
            "Epoch 436/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.3020 - acc: 0.8890 - val_loss: 0.4372 - val_acc: 0.8604\n",
            "Epoch 437/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.2999 - acc: 0.9005 - val_loss: 0.4449 - val_acc: 0.8549\n",
            "Epoch 438/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.2954 - acc: 0.9053 - val_loss: 0.4771 - val_acc: 0.8371\n",
            "Epoch 439/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.3150 - acc: 0.8947 - val_loss: 0.4560 - val_acc: 0.8549\n",
            "Epoch 440/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.2972 - acc: 0.9002 - val_loss: 0.4640 - val_acc: 0.8487\n",
            "Epoch 441/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.3165 - acc: 0.8968 - val_loss: 0.4526 - val_acc: 0.8567\n",
            "Epoch 442/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.3008 - acc: 0.8977 - val_loss: 0.4497 - val_acc: 0.8543\n",
            "Epoch 443/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.2962 - acc: 0.9026 - val_loss: 0.4745 - val_acc: 0.8463\n",
            "Epoch 444/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.2887 - acc: 0.9017 - val_loss: 0.4520 - val_acc: 0.8512\n",
            "Epoch 445/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.3028 - acc: 0.8968 - val_loss: 0.4696 - val_acc: 0.8396\n",
            "Epoch 446/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.2929 - acc: 0.8992 - val_loss: 0.4409 - val_acc: 0.8634\n",
            "Epoch 447/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.2920 - acc: 0.9023 - val_loss: 0.4344 - val_acc: 0.8616\n",
            "Epoch 448/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.2894 - acc: 0.9086 - val_loss: 0.4483 - val_acc: 0.8592\n",
            "Epoch 449/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.2862 - acc: 0.9038 - val_loss: 0.4247 - val_acc: 0.8696\n",
            "Epoch 450/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.2896 - acc: 0.9038 - val_loss: 0.4560 - val_acc: 0.8518\n",
            "Epoch 451/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.2845 - acc: 0.8983 - val_loss: 0.4316 - val_acc: 0.8665\n",
            "Epoch 452/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.3070 - acc: 0.8968 - val_loss: 0.4565 - val_acc: 0.8598\n",
            "Epoch 453/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.2869 - acc: 0.9008 - val_loss: 0.4502 - val_acc: 0.8622\n",
            "Epoch 454/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.2876 - acc: 0.9035 - val_loss: 0.4218 - val_acc: 0.8677\n",
            "Epoch 455/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.2836 - acc: 0.9029 - val_loss: 0.4318 - val_acc: 0.8628\n",
            "Epoch 456/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.2898 - acc: 0.9050 - val_loss: 0.4221 - val_acc: 0.8659\n",
            "Epoch 457/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.2831 - acc: 0.9065 - val_loss: 0.4595 - val_acc: 0.8592\n",
            "Epoch 458/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.2805 - acc: 0.9125 - val_loss: 0.4865 - val_acc: 0.8438\n",
            "Epoch 459/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.3035 - acc: 0.8881 - val_loss: 0.4385 - val_acc: 0.8524\n",
            "Epoch 460/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.2840 - acc: 0.9017 - val_loss: 0.4713 - val_acc: 0.8451\n",
            "Epoch 461/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.2890 - acc: 0.9032 - val_loss: 0.4329 - val_acc: 0.8622\n",
            "Epoch 462/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.2777 - acc: 0.9122 - val_loss: 0.4372 - val_acc: 0.8604\n",
            "Epoch 463/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.2755 - acc: 0.9011 - val_loss: 0.4633 - val_acc: 0.8426\n",
            "Epoch 464/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.2884 - acc: 0.9017 - val_loss: 0.4369 - val_acc: 0.8634\n",
            "Epoch 465/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.2859 - acc: 0.9050 - val_loss: 0.4381 - val_acc: 0.8628\n",
            "Epoch 466/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 0.2807 - acc: 0.9029 - val_loss: 0.4520 - val_acc: 0.8573\n",
            "Epoch 467/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.2883 - acc: 0.9083 - val_loss: 0.4435 - val_acc: 0.8610\n",
            "Epoch 468/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.2722 - acc: 0.9059 - val_loss: 0.4198 - val_acc: 0.8714\n",
            "Epoch 469/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.2790 - acc: 0.9044 - val_loss: 0.4307 - val_acc: 0.8659\n",
            "Epoch 470/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.2756 - acc: 0.9017 - val_loss: 0.4137 - val_acc: 0.8751\n",
            "Epoch 471/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.2853 - acc: 0.9071 - val_loss: 0.4169 - val_acc: 0.8696\n",
            "Epoch 472/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.2816 - acc: 0.9080 - val_loss: 0.4260 - val_acc: 0.8671\n",
            "Epoch 473/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.2763 - acc: 0.9107 - val_loss: 0.4046 - val_acc: 0.8812\n",
            "Epoch 474/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.2712 - acc: 0.9095 - val_loss: 0.4397 - val_acc: 0.8598\n",
            "Epoch 475/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.2760 - acc: 0.9071 - val_loss: 0.4281 - val_acc: 0.8555\n",
            "Epoch 476/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.2681 - acc: 0.9098 - val_loss: 0.4231 - val_acc: 0.8677\n",
            "Epoch 477/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.2730 - acc: 0.9053 - val_loss: 0.4247 - val_acc: 0.8690\n",
            "Epoch 478/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.2728 - acc: 0.8998 - val_loss: 0.4250 - val_acc: 0.8634\n",
            "Epoch 479/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.2720 - acc: 0.9050 - val_loss: 0.4260 - val_acc: 0.8641\n",
            "Epoch 480/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.2687 - acc: 0.9056 - val_loss: 0.4454 - val_acc: 0.8555\n",
            "Epoch 481/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.2767 - acc: 0.9047 - val_loss: 0.4326 - val_acc: 0.8616\n",
            "Epoch 482/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.2618 - acc: 0.9107 - val_loss: 0.4278 - val_acc: 0.8671\n",
            "Epoch 483/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.2747 - acc: 0.9089 - val_loss: 0.4143 - val_acc: 0.8751\n",
            "Epoch 484/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.2646 - acc: 0.9161 - val_loss: 0.4130 - val_acc: 0.8781\n",
            "Epoch 485/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.2643 - acc: 0.9116 - val_loss: 0.4233 - val_acc: 0.8677\n",
            "Epoch 486/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.2580 - acc: 0.9179 - val_loss: 0.4147 - val_acc: 0.8696\n",
            "Epoch 487/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.2465 - acc: 0.9201 - val_loss: 0.4085 - val_acc: 0.8726\n",
            "Epoch 488/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.2620 - acc: 0.9107 - val_loss: 0.4220 - val_acc: 0.8683\n",
            "Epoch 489/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.2634 - acc: 0.9119 - val_loss: 0.4133 - val_acc: 0.8683\n",
            "Epoch 490/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.2612 - acc: 0.9152 - val_loss: 0.4103 - val_acc: 0.8677\n",
            "Epoch 491/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.2685 - acc: 0.9074 - val_loss: 0.4111 - val_acc: 0.8763\n",
            "Epoch 492/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.2586 - acc: 0.9122 - val_loss: 0.4495 - val_acc: 0.8696\n",
            "Epoch 493/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.2672 - acc: 0.9074 - val_loss: 0.4133 - val_acc: 0.8739\n",
            "Epoch 494/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.2680 - acc: 0.9131 - val_loss: 0.4260 - val_acc: 0.8647\n",
            "Epoch 495/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.2547 - acc: 0.9158 - val_loss: 0.4136 - val_acc: 0.8745\n",
            "Epoch 496/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.2540 - acc: 0.9161 - val_loss: 0.4084 - val_acc: 0.8683\n",
            "Epoch 497/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.2518 - acc: 0.9155 - val_loss: 0.4223 - val_acc: 0.8720\n",
            "Epoch 498/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.2512 - acc: 0.9213 - val_loss: 0.4265 - val_acc: 0.8677\n",
            "Epoch 499/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.2564 - acc: 0.9137 - val_loss: 0.4300 - val_acc: 0.8641\n",
            "Epoch 500/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.2560 - acc: 0.9198 - val_loss: 0.4088 - val_acc: 0.8769\n",
            "Epoch 501/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.2667 - acc: 0.9092 - val_loss: 0.3959 - val_acc: 0.8708\n",
            "Epoch 502/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 0.2386 - acc: 0.9161 - val_loss: 0.4292 - val_acc: 0.8573\n",
            "Epoch 503/1000\n",
            "3315/3315 [==============================] - 2s 455us/step - loss: 0.2619 - acc: 0.9161 - val_loss: 0.4250 - val_acc: 0.8665\n",
            "Epoch 504/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2556 - acc: 0.9170 - val_loss: 0.4218 - val_acc: 0.8665\n",
            "Epoch 505/1000\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.2504 - acc: 0.9173 - val_loss: 0.4187 - val_acc: 0.8696\n",
            "Epoch 506/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 0.2646 - acc: 0.9137 - val_loss: 0.4041 - val_acc: 0.8855\n",
            "Epoch 507/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.2612 - acc: 0.9098 - val_loss: 0.4187 - val_acc: 0.8714\n",
            "Epoch 508/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.2421 - acc: 0.9164 - val_loss: 0.4133 - val_acc: 0.8775\n",
            "Epoch 509/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.2446 - acc: 0.9176 - val_loss: 0.4299 - val_acc: 0.8683\n",
            "Epoch 510/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.2555 - acc: 0.9077 - val_loss: 0.4117 - val_acc: 0.8647\n",
            "Epoch 511/1000\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 0.2574 - acc: 0.9131 - val_loss: 0.4246 - val_acc: 0.8751\n",
            "Epoch 512/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.2425 - acc: 0.9183 - val_loss: 0.4492 - val_acc: 0.8512\n",
            "Epoch 513/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.2423 - acc: 0.9176 - val_loss: 0.4040 - val_acc: 0.8732\n",
            "Epoch 514/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.2475 - acc: 0.9195 - val_loss: 0.4381 - val_acc: 0.8616\n",
            "Epoch 515/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.2406 - acc: 0.9195 - val_loss: 0.4356 - val_acc: 0.8616\n",
            "Epoch 516/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.2422 - acc: 0.9183 - val_loss: 0.3918 - val_acc: 0.8794\n",
            "Epoch 517/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.2461 - acc: 0.9231 - val_loss: 0.4128 - val_acc: 0.8720\n",
            "Epoch 518/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.2439 - acc: 0.9104 - val_loss: 0.4241 - val_acc: 0.8690\n",
            "Epoch 519/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.2501 - acc: 0.9143 - val_loss: 0.4119 - val_acc: 0.8763\n",
            "Epoch 520/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.2434 - acc: 0.9234 - val_loss: 0.3946 - val_acc: 0.8769\n",
            "Epoch 521/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.2464 - acc: 0.9167 - val_loss: 0.3941 - val_acc: 0.8763\n",
            "Epoch 522/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.2365 - acc: 0.9176 - val_loss: 0.3990 - val_acc: 0.8775\n",
            "Epoch 523/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.2405 - acc: 0.9219 - val_loss: 0.3925 - val_acc: 0.8726\n",
            "Epoch 524/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.2473 - acc: 0.9195 - val_loss: 0.3956 - val_acc: 0.8830\n",
            "Epoch 525/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.2447 - acc: 0.9155 - val_loss: 0.3971 - val_acc: 0.8788\n",
            "Epoch 526/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.2314 - acc: 0.9234 - val_loss: 0.3972 - val_acc: 0.8879\n",
            "Epoch 527/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.2526 - acc: 0.9149 - val_loss: 0.4137 - val_acc: 0.8830\n",
            "Epoch 528/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.2364 - acc: 0.9207 - val_loss: 0.3913 - val_acc: 0.8818\n",
            "Epoch 529/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.2175 - acc: 0.9252 - val_loss: 0.4027 - val_acc: 0.8794\n",
            "Epoch 530/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.2366 - acc: 0.9213 - val_loss: 0.3868 - val_acc: 0.8873\n",
            "Epoch 531/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.2454 - acc: 0.9201 - val_loss: 0.3878 - val_acc: 0.8849\n",
            "Epoch 532/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.2441 - acc: 0.9143 - val_loss: 0.3992 - val_acc: 0.8720\n",
            "Epoch 533/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.2477 - acc: 0.9167 - val_loss: 0.4148 - val_acc: 0.8708\n",
            "Epoch 534/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.2372 - acc: 0.9201 - val_loss: 0.4210 - val_acc: 0.8818\n",
            "Epoch 535/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.2394 - acc: 0.9213 - val_loss: 0.4083 - val_acc: 0.8751\n",
            "Epoch 536/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.2367 - acc: 0.9198 - val_loss: 0.4302 - val_acc: 0.8653\n",
            "Epoch 537/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.2293 - acc: 0.9246 - val_loss: 0.3828 - val_acc: 0.8800\n",
            "Epoch 538/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.2288 - acc: 0.9179 - val_loss: 0.4162 - val_acc: 0.8659\n",
            "Epoch 539/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.2326 - acc: 0.9216 - val_loss: 0.4083 - val_acc: 0.8812\n",
            "Epoch 540/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.2400 - acc: 0.9234 - val_loss: 0.4041 - val_acc: 0.8812\n",
            "Epoch 541/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.2257 - acc: 0.9213 - val_loss: 0.3966 - val_acc: 0.8818\n",
            "Epoch 542/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.2433 - acc: 0.9173 - val_loss: 0.4102 - val_acc: 0.8732\n",
            "Epoch 543/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.2258 - acc: 0.9264 - val_loss: 0.4058 - val_acc: 0.8781\n",
            "Epoch 544/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.2150 - acc: 0.9249 - val_loss: 0.4114 - val_acc: 0.8647\n",
            "Epoch 545/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.2296 - acc: 0.9240 - val_loss: 0.3913 - val_acc: 0.8873\n",
            "Epoch 546/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.2199 - acc: 0.9285 - val_loss: 0.3874 - val_acc: 0.8855\n",
            "Epoch 547/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.2269 - acc: 0.9249 - val_loss: 0.4234 - val_acc: 0.8763\n",
            "Epoch 548/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.2346 - acc: 0.9246 - val_loss: 0.3952 - val_acc: 0.8843\n",
            "Epoch 549/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.2172 - acc: 0.9270 - val_loss: 0.3774 - val_acc: 0.8867\n",
            "Epoch 550/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.2406 - acc: 0.9198 - val_loss: 0.3914 - val_acc: 0.8794\n",
            "Epoch 551/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.2224 - acc: 0.9252 - val_loss: 0.3953 - val_acc: 0.8836\n",
            "Epoch 552/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.2229 - acc: 0.9234 - val_loss: 0.3871 - val_acc: 0.8818\n",
            "Epoch 553/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.2271 - acc: 0.9276 - val_loss: 0.4011 - val_acc: 0.8757\n",
            "Epoch 554/1000\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.2223 - acc: 0.9315 - val_loss: 0.3797 - val_acc: 0.8916\n",
            "Epoch 555/1000\n",
            "3315/3315 [==============================] - 1s 452us/step - loss: 0.2121 - acc: 0.9312 - val_loss: 0.3921 - val_acc: 0.8788\n",
            "Epoch 556/1000\n",
            "3315/3315 [==============================] - 2s 457us/step - loss: 0.2169 - acc: 0.9279 - val_loss: 0.3882 - val_acc: 0.8910\n",
            "Epoch 557/1000\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.2095 - acc: 0.9342 - val_loss: 0.4333 - val_acc: 0.8677\n",
            "Epoch 558/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.2293 - acc: 0.9246 - val_loss: 0.3888 - val_acc: 0.8830\n",
            "Epoch 559/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.2184 - acc: 0.9291 - val_loss: 0.4155 - val_acc: 0.8751\n",
            "Epoch 560/1000\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.2133 - acc: 0.9285 - val_loss: 0.3823 - val_acc: 0.8836\n",
            "Epoch 561/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.2151 - acc: 0.9270 - val_loss: 0.3824 - val_acc: 0.8867\n",
            "Epoch 562/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.2152 - acc: 0.9270 - val_loss: 0.3890 - val_acc: 0.8849\n",
            "Epoch 563/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.2279 - acc: 0.9267 - val_loss: 0.3921 - val_acc: 0.8855\n",
            "Epoch 564/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.2327 - acc: 0.9204 - val_loss: 0.4161 - val_acc: 0.8843\n",
            "Epoch 565/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.2235 - acc: 0.9240 - val_loss: 0.4115 - val_acc: 0.8690\n",
            "Epoch 566/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.2143 - acc: 0.9273 - val_loss: 0.3844 - val_acc: 0.8824\n",
            "Epoch 567/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.2147 - acc: 0.9276 - val_loss: 0.3787 - val_acc: 0.8824\n",
            "Epoch 568/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.2154 - acc: 0.9258 - val_loss: 0.3655 - val_acc: 0.8953\n",
            "Epoch 569/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.2086 - acc: 0.9327 - val_loss: 0.3855 - val_acc: 0.8781\n",
            "Epoch 570/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.2128 - acc: 0.9312 - val_loss: 0.3916 - val_acc: 0.8849\n",
            "Epoch 571/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.2130 - acc: 0.9294 - val_loss: 0.4108 - val_acc: 0.8622\n",
            "Epoch 572/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.2148 - acc: 0.9270 - val_loss: 0.3899 - val_acc: 0.8824\n",
            "Epoch 573/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.2111 - acc: 0.9252 - val_loss: 0.4015 - val_acc: 0.8794\n",
            "Epoch 574/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.2069 - acc: 0.9315 - val_loss: 0.3682 - val_acc: 0.8916\n",
            "Epoch 575/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.2031 - acc: 0.9370 - val_loss: 0.3680 - val_acc: 0.8953\n",
            "Epoch 576/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.2098 - acc: 0.9255 - val_loss: 0.3704 - val_acc: 0.8947\n",
            "Epoch 577/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.2173 - acc: 0.9243 - val_loss: 0.3695 - val_acc: 0.8941\n",
            "Epoch 578/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.2152 - acc: 0.9264 - val_loss: 0.3716 - val_acc: 0.8849\n",
            "Epoch 579/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.2074 - acc: 0.9312 - val_loss: 0.3832 - val_acc: 0.8879\n",
            "Epoch 580/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.2070 - acc: 0.9309 - val_loss: 0.3952 - val_acc: 0.8892\n",
            "Epoch 581/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.2248 - acc: 0.9267 - val_loss: 0.3927 - val_acc: 0.8928\n",
            "Epoch 582/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.2114 - acc: 0.9255 - val_loss: 0.3708 - val_acc: 0.8892\n",
            "Epoch 583/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.2058 - acc: 0.9324 - val_loss: 0.3751 - val_acc: 0.8855\n",
            "Epoch 584/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1991 - acc: 0.9297 - val_loss: 0.3757 - val_acc: 0.8843\n",
            "Epoch 585/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.2029 - acc: 0.9339 - val_loss: 0.3798 - val_acc: 0.8892\n",
            "Epoch 586/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1921 - acc: 0.9342 - val_loss: 0.3853 - val_acc: 0.8855\n",
            "Epoch 587/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.2052 - acc: 0.9279 - val_loss: 0.3715 - val_acc: 0.8855\n",
            "Epoch 588/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1995 - acc: 0.9306 - val_loss: 0.3595 - val_acc: 0.8990\n",
            "Epoch 589/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1982 - acc: 0.9345 - val_loss: 0.3830 - val_acc: 0.8904\n",
            "Epoch 590/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.2003 - acc: 0.9315 - val_loss: 0.3593 - val_acc: 0.8947\n",
            "Epoch 591/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.2091 - acc: 0.9288 - val_loss: 0.3787 - val_acc: 0.8855\n",
            "Epoch 592/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.2148 - acc: 0.9333 - val_loss: 0.3865 - val_acc: 0.8812\n",
            "Epoch 593/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1924 - acc: 0.9379 - val_loss: 0.3739 - val_acc: 0.8861\n",
            "Epoch 594/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.2002 - acc: 0.9345 - val_loss: 0.3659 - val_acc: 0.8922\n",
            "Epoch 595/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.2011 - acc: 0.9330 - val_loss: 0.3760 - val_acc: 0.8916\n",
            "Epoch 596/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1881 - acc: 0.9406 - val_loss: 0.3745 - val_acc: 0.8910\n",
            "Epoch 597/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.2084 - acc: 0.9303 - val_loss: 0.3965 - val_acc: 0.8806\n",
            "Epoch 598/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1980 - acc: 0.9367 - val_loss: 0.3640 - val_acc: 0.8898\n",
            "Epoch 599/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.2103 - acc: 0.9276 - val_loss: 0.3798 - val_acc: 0.8849\n",
            "Epoch 600/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1934 - acc: 0.9379 - val_loss: 0.3636 - val_acc: 0.8879\n",
            "Epoch 601/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1998 - acc: 0.9348 - val_loss: 0.3608 - val_acc: 0.8996\n",
            "Epoch 602/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.2060 - acc: 0.9249 - val_loss: 0.3681 - val_acc: 0.8910\n",
            "Epoch 603/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.2021 - acc: 0.9348 - val_loss: 0.3758 - val_acc: 0.8843\n",
            "Epoch 604/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.1977 - acc: 0.9354 - val_loss: 0.3560 - val_acc: 0.8928\n",
            "Epoch 605/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.2025 - acc: 0.9315 - val_loss: 0.3585 - val_acc: 0.8892\n",
            "Epoch 606/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1935 - acc: 0.9321 - val_loss: 0.3576 - val_acc: 0.8947\n",
            "Epoch 607/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1947 - acc: 0.9306 - val_loss: 0.3687 - val_acc: 0.8873\n",
            "Epoch 608/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.1977 - acc: 0.9351 - val_loss: 0.3630 - val_acc: 0.8873\n",
            "Epoch 609/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 0.1878 - acc: 0.9394 - val_loss: 0.3696 - val_acc: 0.8996\n",
            "Epoch 610/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1905 - acc: 0.9336 - val_loss: 0.3943 - val_acc: 0.8757\n",
            "Epoch 611/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.2080 - acc: 0.9279 - val_loss: 0.3692 - val_acc: 0.8947\n",
            "Epoch 612/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.2037 - acc: 0.9327 - val_loss: 0.3758 - val_acc: 0.8885\n",
            "Epoch 613/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.2053 - acc: 0.9327 - val_loss: 0.3749 - val_acc: 0.8879\n",
            "Epoch 614/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1963 - acc: 0.9327 - val_loss: 0.3652 - val_acc: 0.8922\n",
            "Epoch 615/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1850 - acc: 0.9376 - val_loss: 0.3674 - val_acc: 0.8873\n",
            "Epoch 616/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.1918 - acc: 0.9348 - val_loss: 0.3626 - val_acc: 0.9008\n",
            "Epoch 617/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.1993 - acc: 0.9357 - val_loss: 0.3733 - val_acc: 0.8885\n",
            "Epoch 618/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1887 - acc: 0.9370 - val_loss: 0.3802 - val_acc: 0.8922\n",
            "Epoch 619/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1957 - acc: 0.9351 - val_loss: 0.3549 - val_acc: 0.8928\n",
            "Epoch 620/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1926 - acc: 0.9360 - val_loss: 0.3880 - val_acc: 0.8824\n",
            "Epoch 621/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1936 - acc: 0.9370 - val_loss: 0.3654 - val_acc: 0.8953\n",
            "Epoch 622/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1899 - acc: 0.9427 - val_loss: 0.3646 - val_acc: 0.8977\n",
            "Epoch 623/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1895 - acc: 0.9330 - val_loss: 0.3776 - val_acc: 0.8849\n",
            "Epoch 624/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1846 - acc: 0.9430 - val_loss: 0.3558 - val_acc: 0.8953\n",
            "Epoch 625/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1709 - acc: 0.9412 - val_loss: 0.3520 - val_acc: 0.8990\n",
            "Epoch 626/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1807 - acc: 0.9433 - val_loss: 0.3692 - val_acc: 0.8879\n",
            "Epoch 627/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1746 - acc: 0.9391 - val_loss: 0.3698 - val_acc: 0.8861\n",
            "Epoch 628/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1988 - acc: 0.9318 - val_loss: 0.3656 - val_acc: 0.8898\n",
            "Epoch 629/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1865 - acc: 0.9367 - val_loss: 0.3473 - val_acc: 0.8904\n",
            "Epoch 630/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1805 - acc: 0.9391 - val_loss: 0.3577 - val_acc: 0.8996\n",
            "Epoch 631/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1900 - acc: 0.9345 - val_loss: 0.3591 - val_acc: 0.8990\n",
            "Epoch 632/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1759 - acc: 0.9409 - val_loss: 0.3609 - val_acc: 0.8922\n",
            "Epoch 633/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1784 - acc: 0.9406 - val_loss: 0.3907 - val_acc: 0.8867\n",
            "Epoch 634/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.1802 - acc: 0.9433 - val_loss: 0.3657 - val_acc: 0.8996\n",
            "Epoch 635/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1862 - acc: 0.9367 - val_loss: 0.3809 - val_acc: 0.8934\n",
            "Epoch 636/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1860 - acc: 0.9348 - val_loss: 0.3890 - val_acc: 0.8867\n",
            "Epoch 637/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.1873 - acc: 0.9363 - val_loss: 0.3845 - val_acc: 0.8861\n",
            "Epoch 638/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1897 - acc: 0.9394 - val_loss: 0.3951 - val_acc: 0.8873\n",
            "Epoch 639/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1971 - acc: 0.9351 - val_loss: 0.3721 - val_acc: 0.8928\n",
            "Epoch 640/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1777 - acc: 0.9403 - val_loss: 0.3563 - val_acc: 0.9020\n",
            "Epoch 641/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1874 - acc: 0.9312 - val_loss: 0.3578 - val_acc: 0.8947\n",
            "Epoch 642/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1852 - acc: 0.9418 - val_loss: 0.3628 - val_acc: 0.8941\n",
            "Epoch 643/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1689 - acc: 0.9439 - val_loss: 0.3617 - val_acc: 0.8928\n",
            "Epoch 644/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1830 - acc: 0.9363 - val_loss: 0.3511 - val_acc: 0.9008\n",
            "Epoch 645/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1832 - acc: 0.9367 - val_loss: 0.3630 - val_acc: 0.8922\n",
            "Epoch 646/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1931 - acc: 0.9339 - val_loss: 0.3702 - val_acc: 0.8928\n",
            "Epoch 647/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1813 - acc: 0.9394 - val_loss: 0.3907 - val_acc: 0.8830\n",
            "Epoch 648/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1874 - acc: 0.9427 - val_loss: 0.3699 - val_acc: 0.8928\n",
            "Epoch 649/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1795 - acc: 0.9360 - val_loss: 0.3664 - val_acc: 0.8916\n",
            "Epoch 650/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1910 - acc: 0.9360 - val_loss: 0.3653 - val_acc: 0.8977\n",
            "Epoch 651/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.1774 - acc: 0.9412 - val_loss: 0.3658 - val_acc: 0.8990\n",
            "Epoch 652/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1817 - acc: 0.9357 - val_loss: 0.3594 - val_acc: 0.8922\n",
            "Epoch 653/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.1890 - acc: 0.9360 - val_loss: 0.3759 - val_acc: 0.8879\n",
            "Epoch 654/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1822 - acc: 0.9382 - val_loss: 0.3573 - val_acc: 0.8953\n",
            "Epoch 655/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1797 - acc: 0.9409 - val_loss: 0.3510 - val_acc: 0.9045\n",
            "Epoch 656/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1745 - acc: 0.9397 - val_loss: 0.3552 - val_acc: 0.9032\n",
            "Epoch 657/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1755 - acc: 0.9436 - val_loss: 0.3517 - val_acc: 0.8983\n",
            "Epoch 658/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1825 - acc: 0.9379 - val_loss: 0.3746 - val_acc: 0.8959\n",
            "Epoch 659/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1708 - acc: 0.9436 - val_loss: 0.3804 - val_acc: 0.8928\n",
            "Epoch 660/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1766 - acc: 0.9403 - val_loss: 0.3375 - val_acc: 0.9002\n",
            "Epoch 661/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1783 - acc: 0.9418 - val_loss: 0.3589 - val_acc: 0.8953\n",
            "Epoch 662/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1667 - acc: 0.9472 - val_loss: 0.3608 - val_acc: 0.8855\n",
            "Epoch 663/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1777 - acc: 0.9421 - val_loss: 0.3781 - val_acc: 0.8855\n",
            "Epoch 664/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1660 - acc: 0.9457 - val_loss: 0.3542 - val_acc: 0.8934\n",
            "Epoch 665/1000\n",
            "3315/3315 [==============================] - 1s 411us/step - loss: 0.1831 - acc: 0.9391 - val_loss: 0.3924 - val_acc: 0.8775\n",
            "Epoch 666/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1691 - acc: 0.9412 - val_loss: 0.3604 - val_acc: 0.8971\n",
            "Epoch 667/1000\n",
            "3315/3315 [==============================] - 1s 407us/step - loss: 0.1587 - acc: 0.9463 - val_loss: 0.3484 - val_acc: 0.8990\n",
            "Epoch 668/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1663 - acc: 0.9445 - val_loss: 0.3615 - val_acc: 0.8922\n",
            "Epoch 669/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1610 - acc: 0.9502 - val_loss: 0.3645 - val_acc: 0.8977\n",
            "Epoch 670/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1720 - acc: 0.9424 - val_loss: 0.3630 - val_acc: 0.8965\n",
            "Epoch 671/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1664 - acc: 0.9460 - val_loss: 0.3840 - val_acc: 0.8928\n",
            "Epoch 672/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.1736 - acc: 0.9436 - val_loss: 0.3600 - val_acc: 0.8916\n",
            "Epoch 673/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1723 - acc: 0.9472 - val_loss: 0.3509 - val_acc: 0.8971\n",
            "Epoch 674/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.1696 - acc: 0.9442 - val_loss: 0.3734 - val_acc: 0.8953\n",
            "Epoch 675/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1716 - acc: 0.9406 - val_loss: 0.3560 - val_acc: 0.8996\n",
            "Epoch 676/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1667 - acc: 0.9445 - val_loss: 0.3547 - val_acc: 0.8941\n",
            "Epoch 677/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.1816 - acc: 0.9409 - val_loss: 0.3739 - val_acc: 0.8904\n",
            "Epoch 678/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1652 - acc: 0.9451 - val_loss: 0.3531 - val_acc: 0.8959\n",
            "Epoch 679/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1733 - acc: 0.9436 - val_loss: 0.3525 - val_acc: 0.9008\n",
            "Epoch 680/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1667 - acc: 0.9460 - val_loss: 0.3462 - val_acc: 0.8990\n",
            "Epoch 681/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1597 - acc: 0.9436 - val_loss: 0.3677 - val_acc: 0.8965\n",
            "Epoch 682/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.1766 - acc: 0.9388 - val_loss: 0.3698 - val_acc: 0.8941\n",
            "Epoch 683/1000\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.1643 - acc: 0.9469 - val_loss: 0.3600 - val_acc: 0.9026\n",
            "Epoch 684/1000\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.1822 - acc: 0.9418 - val_loss: 0.3494 - val_acc: 0.9057\n",
            "Epoch 685/1000\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.1584 - acc: 0.9448 - val_loss: 0.3615 - val_acc: 0.9020\n",
            "Epoch 686/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1757 - acc: 0.9409 - val_loss: 0.3598 - val_acc: 0.9002\n",
            "Epoch 687/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1716 - acc: 0.9421 - val_loss: 0.3592 - val_acc: 0.8879\n",
            "Epoch 688/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1622 - acc: 0.9481 - val_loss: 0.3443 - val_acc: 0.9032\n",
            "Epoch 689/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1651 - acc: 0.9439 - val_loss: 0.3404 - val_acc: 0.9020\n",
            "Epoch 690/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1697 - acc: 0.9469 - val_loss: 0.3589 - val_acc: 0.8971\n",
            "Epoch 691/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1689 - acc: 0.9451 - val_loss: 0.3532 - val_acc: 0.9020\n",
            "Epoch 692/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1860 - acc: 0.9385 - val_loss: 0.3494 - val_acc: 0.9008\n",
            "Epoch 693/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1701 - acc: 0.9448 - val_loss: 0.3547 - val_acc: 0.8959\n",
            "Epoch 694/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1638 - acc: 0.9478 - val_loss: 0.3717 - val_acc: 0.8843\n",
            "Epoch 695/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1691 - acc: 0.9436 - val_loss: 0.3398 - val_acc: 0.8959\n",
            "Epoch 696/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1657 - acc: 0.9448 - val_loss: 0.3449 - val_acc: 0.8990\n",
            "Epoch 697/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1570 - acc: 0.9439 - val_loss: 0.3730 - val_acc: 0.8977\n",
            "Epoch 698/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1602 - acc: 0.9481 - val_loss: 0.3414 - val_acc: 0.9008\n",
            "Epoch 699/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1642 - acc: 0.9442 - val_loss: 0.3574 - val_acc: 0.8922\n",
            "Epoch 700/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1704 - acc: 0.9445 - val_loss: 0.3334 - val_acc: 0.9039\n",
            "Epoch 701/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1560 - acc: 0.9472 - val_loss: 0.3486 - val_acc: 0.8965\n",
            "Epoch 702/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1612 - acc: 0.9460 - val_loss: 0.3411 - val_acc: 0.9063\n",
            "Epoch 703/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1500 - acc: 0.9548 - val_loss: 0.3332 - val_acc: 0.9026\n",
            "Epoch 704/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1762 - acc: 0.9433 - val_loss: 0.3536 - val_acc: 0.8959\n",
            "Epoch 705/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1562 - acc: 0.9514 - val_loss: 0.3603 - val_acc: 0.8971\n",
            "Epoch 706/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1564 - acc: 0.9481 - val_loss: 0.3736 - val_acc: 0.8947\n",
            "Epoch 707/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1585 - acc: 0.9487 - val_loss: 0.3359 - val_acc: 0.9057\n",
            "Epoch 708/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1676 - acc: 0.9496 - val_loss: 0.3566 - val_acc: 0.9026\n",
            "Epoch 709/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1673 - acc: 0.9457 - val_loss: 0.3565 - val_acc: 0.8953\n",
            "Epoch 710/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1416 - acc: 0.9548 - val_loss: 0.3702 - val_acc: 0.8941\n",
            "Epoch 711/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1601 - acc: 0.9475 - val_loss: 0.3336 - val_acc: 0.9020\n",
            "Epoch 712/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1646 - acc: 0.9454 - val_loss: 0.3344 - val_acc: 0.9106\n",
            "Epoch 713/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1570 - acc: 0.9526 - val_loss: 0.3355 - val_acc: 0.9002\n",
            "Epoch 714/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1608 - acc: 0.9424 - val_loss: 0.3556 - val_acc: 0.9032\n",
            "Epoch 715/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1510 - acc: 0.9460 - val_loss: 0.3465 - val_acc: 0.9045\n",
            "Epoch 716/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1700 - acc: 0.9457 - val_loss: 0.3608 - val_acc: 0.8965\n",
            "Epoch 717/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1590 - acc: 0.9490 - val_loss: 0.3461 - val_acc: 0.9020\n",
            "Epoch 718/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1423 - acc: 0.9538 - val_loss: 0.3551 - val_acc: 0.8965\n",
            "Epoch 719/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1592 - acc: 0.9490 - val_loss: 0.3463 - val_acc: 0.9057\n",
            "Epoch 720/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1642 - acc: 0.9439 - val_loss: 0.3310 - val_acc: 0.9051\n",
            "Epoch 721/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1566 - acc: 0.9439 - val_loss: 0.3589 - val_acc: 0.8971\n",
            "Epoch 722/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.1532 - acc: 0.9469 - val_loss: 0.3407 - val_acc: 0.9063\n",
            "Epoch 723/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1542 - acc: 0.9496 - val_loss: 0.3434 - val_acc: 0.9008\n",
            "Epoch 724/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1580 - acc: 0.9490 - val_loss: 0.3358 - val_acc: 0.9069\n",
            "Epoch 725/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1464 - acc: 0.9523 - val_loss: 0.3537 - val_acc: 0.8965\n",
            "Epoch 726/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1733 - acc: 0.9433 - val_loss: 0.3318 - val_acc: 0.8959\n",
            "Epoch 727/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1613 - acc: 0.9463 - val_loss: 0.3304 - val_acc: 0.9088\n",
            "Epoch 728/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1597 - acc: 0.9460 - val_loss: 0.3456 - val_acc: 0.9020\n",
            "Epoch 729/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1611 - acc: 0.9469 - val_loss: 0.3373 - val_acc: 0.9014\n",
            "Epoch 730/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1519 - acc: 0.9523 - val_loss: 0.3432 - val_acc: 0.9051\n",
            "Epoch 731/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1666 - acc: 0.9466 - val_loss: 0.3506 - val_acc: 0.9002\n",
            "Epoch 732/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1459 - acc: 0.9560 - val_loss: 0.3483 - val_acc: 0.9002\n",
            "Epoch 733/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1390 - acc: 0.9526 - val_loss: 0.3475 - val_acc: 0.9002\n",
            "Epoch 734/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1533 - acc: 0.9451 - val_loss: 0.3579 - val_acc: 0.8983\n",
            "Epoch 735/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1602 - acc: 0.9472 - val_loss: 0.3431 - val_acc: 0.9002\n",
            "Epoch 736/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1496 - acc: 0.9487 - val_loss: 0.3528 - val_acc: 0.8959\n",
            "Epoch 737/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1560 - acc: 0.9463 - val_loss: 0.3375 - val_acc: 0.9045\n",
            "Epoch 738/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1495 - acc: 0.9487 - val_loss: 0.3871 - val_acc: 0.8879\n",
            "Epoch 739/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1538 - acc: 0.9484 - val_loss: 0.3475 - val_acc: 0.9039\n",
            "Epoch 740/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1519 - acc: 0.9511 - val_loss: 0.3395 - val_acc: 0.9063\n",
            "Epoch 741/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1474 - acc: 0.9520 - val_loss: 0.3334 - val_acc: 0.9026\n",
            "Epoch 742/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1443 - acc: 0.9520 - val_loss: 0.3805 - val_acc: 0.8934\n",
            "Epoch 743/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1542 - acc: 0.9493 - val_loss: 0.3498 - val_acc: 0.9045\n",
            "Epoch 744/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1532 - acc: 0.9466 - val_loss: 0.3261 - val_acc: 0.9088\n",
            "Epoch 745/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1527 - acc: 0.9529 - val_loss: 0.3486 - val_acc: 0.9051\n",
            "Epoch 746/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1518 - acc: 0.9478 - val_loss: 0.3483 - val_acc: 0.9045\n",
            "Epoch 747/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1437 - acc: 0.9508 - val_loss: 0.3547 - val_acc: 0.8977\n",
            "Epoch 748/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1464 - acc: 0.9487 - val_loss: 0.3507 - val_acc: 0.9057\n",
            "Epoch 749/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1538 - acc: 0.9496 - val_loss: 0.3571 - val_acc: 0.9057\n",
            "Epoch 750/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1488 - acc: 0.9520 - val_loss: 0.3574 - val_acc: 0.8922\n",
            "Epoch 751/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1530 - acc: 0.9481 - val_loss: 0.3423 - val_acc: 0.9051\n",
            "Epoch 752/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1511 - acc: 0.9505 - val_loss: 0.3462 - val_acc: 0.9100\n",
            "Epoch 753/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1488 - acc: 0.9490 - val_loss: 0.3403 - val_acc: 0.9032\n",
            "Epoch 754/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1472 - acc: 0.9523 - val_loss: 0.3435 - val_acc: 0.9026\n",
            "Epoch 755/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1528 - acc: 0.9508 - val_loss: 0.3484 - val_acc: 0.9008\n",
            "Epoch 756/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1523 - acc: 0.9505 - val_loss: 0.3384 - val_acc: 0.9020\n",
            "Epoch 757/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1415 - acc: 0.9508 - val_loss: 0.3560 - val_acc: 0.9020\n",
            "Epoch 758/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1510 - acc: 0.9463 - val_loss: 0.3297 - val_acc: 0.9014\n",
            "Epoch 759/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1515 - acc: 0.9499 - val_loss: 0.3975 - val_acc: 0.8775\n",
            "Epoch 760/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1423 - acc: 0.9529 - val_loss: 0.3345 - val_acc: 0.9032\n",
            "Epoch 761/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1510 - acc: 0.9454 - val_loss: 0.3450 - val_acc: 0.9014\n",
            "Epoch 762/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1428 - acc: 0.9502 - val_loss: 0.3311 - val_acc: 0.9081\n",
            "Epoch 763/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1498 - acc: 0.9472 - val_loss: 0.3408 - val_acc: 0.9014\n",
            "Epoch 764/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1409 - acc: 0.9569 - val_loss: 0.3586 - val_acc: 0.8965\n",
            "Epoch 765/1000\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.1328 - acc: 0.9569 - val_loss: 0.3316 - val_acc: 0.9106\n",
            "Epoch 766/1000\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.1426 - acc: 0.9554 - val_loss: 0.3313 - val_acc: 0.9094\n",
            "Epoch 767/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1345 - acc: 0.9538 - val_loss: 0.3266 - val_acc: 0.9075\n",
            "Epoch 768/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1397 - acc: 0.9554 - val_loss: 0.3479 - val_acc: 0.9057\n",
            "Epoch 769/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1434 - acc: 0.9532 - val_loss: 0.3504 - val_acc: 0.8934\n",
            "Epoch 770/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1500 - acc: 0.9532 - val_loss: 0.3586 - val_acc: 0.8996\n",
            "Epoch 771/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1271 - acc: 0.9566 - val_loss: 0.3553 - val_acc: 0.9002\n",
            "Epoch 772/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1356 - acc: 0.9523 - val_loss: 0.3461 - val_acc: 0.8977\n",
            "Epoch 773/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1435 - acc: 0.9499 - val_loss: 0.3462 - val_acc: 0.9045\n",
            "Epoch 774/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1475 - acc: 0.9502 - val_loss: 0.3292 - val_acc: 0.9069\n",
            "Epoch 775/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1519 - acc: 0.9493 - val_loss: 0.3308 - val_acc: 0.9026\n",
            "Epoch 776/1000\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.1534 - acc: 0.9508 - val_loss: 0.3461 - val_acc: 0.9051\n",
            "Epoch 777/1000\n",
            "3315/3315 [==============================] - 1s 452us/step - loss: 0.1427 - acc: 0.9502 - val_loss: 0.3456 - val_acc: 0.9014\n",
            "Epoch 778/1000\n",
            "3315/3315 [==============================] - 1s 450us/step - loss: 0.1471 - acc: 0.9460 - val_loss: 0.3386 - val_acc: 0.9088\n",
            "Epoch 779/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.1420 - acc: 0.9508 - val_loss: 0.3388 - val_acc: 0.9143\n",
            "Epoch 780/1000\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.1381 - acc: 0.9505 - val_loss: 0.3519 - val_acc: 0.8983\n",
            "Epoch 781/1000\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.1462 - acc: 0.9505 - val_loss: 0.3390 - val_acc: 0.9032\n",
            "Epoch 782/1000\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.1461 - acc: 0.9532 - val_loss: 0.3381 - val_acc: 0.9002\n",
            "Epoch 783/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1396 - acc: 0.9544 - val_loss: 0.3297 - val_acc: 0.9051\n",
            "Epoch 784/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1426 - acc: 0.9529 - val_loss: 0.3377 - val_acc: 0.9032\n",
            "Epoch 785/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1468 - acc: 0.9535 - val_loss: 0.3546 - val_acc: 0.9014\n",
            "Epoch 786/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1365 - acc: 0.9551 - val_loss: 0.3267 - val_acc: 0.9149\n",
            "Epoch 787/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1456 - acc: 0.9514 - val_loss: 0.3308 - val_acc: 0.9137\n",
            "Epoch 788/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1283 - acc: 0.9566 - val_loss: 0.3167 - val_acc: 0.9143\n",
            "Epoch 789/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1439 - acc: 0.9541 - val_loss: 0.3559 - val_acc: 0.8953\n",
            "Epoch 790/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1402 - acc: 0.9517 - val_loss: 0.3493 - val_acc: 0.9032\n",
            "Epoch 791/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1432 - acc: 0.9508 - val_loss: 0.3220 - val_acc: 0.9081\n",
            "Epoch 792/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1358 - acc: 0.9511 - val_loss: 0.3090 - val_acc: 0.9057\n",
            "Epoch 793/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1273 - acc: 0.9599 - val_loss: 0.3359 - val_acc: 0.9069\n",
            "Epoch 794/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1373 - acc: 0.9511 - val_loss: 0.3367 - val_acc: 0.9069\n",
            "Epoch 795/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1370 - acc: 0.9584 - val_loss: 0.3351 - val_acc: 0.9032\n",
            "Epoch 796/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1338 - acc: 0.9566 - val_loss: 0.3288 - val_acc: 0.9069\n",
            "Epoch 797/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1347 - acc: 0.9538 - val_loss: 0.3361 - val_acc: 0.9020\n",
            "Epoch 798/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1397 - acc: 0.9526 - val_loss: 0.3482 - val_acc: 0.9045\n",
            "Epoch 799/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1409 - acc: 0.9529 - val_loss: 0.3399 - val_acc: 0.9081\n",
            "Epoch 800/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1366 - acc: 0.9563 - val_loss: 0.3233 - val_acc: 0.9051\n",
            "Epoch 801/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.1520 - acc: 0.9487 - val_loss: 0.3402 - val_acc: 0.9057\n",
            "Epoch 802/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1210 - acc: 0.9629 - val_loss: 0.3314 - val_acc: 0.9094\n",
            "Epoch 803/1000\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 0.1366 - acc: 0.9499 - val_loss: 0.3422 - val_acc: 0.9008\n",
            "Epoch 804/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1364 - acc: 0.9563 - val_loss: 0.3391 - val_acc: 0.9063\n",
            "Epoch 805/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1365 - acc: 0.9584 - val_loss: 0.3665 - val_acc: 0.8990\n",
            "Epoch 806/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.1333 - acc: 0.9569 - val_loss: 0.3476 - val_acc: 0.9069\n",
            "Epoch 807/1000\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 0.1182 - acc: 0.9623 - val_loss: 0.3353 - val_acc: 0.9051\n",
            "Epoch 808/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1409 - acc: 0.9517 - val_loss: 0.3236 - val_acc: 0.9124\n",
            "Epoch 809/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1394 - acc: 0.9551 - val_loss: 0.3529 - val_acc: 0.9057\n",
            "Epoch 810/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1314 - acc: 0.9535 - val_loss: 0.3591 - val_acc: 0.9014\n",
            "Epoch 811/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1389 - acc: 0.9499 - val_loss: 0.3257 - val_acc: 0.9118\n",
            "Epoch 812/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1321 - acc: 0.9578 - val_loss: 0.3295 - val_acc: 0.9112\n",
            "Epoch 813/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.1270 - acc: 0.9596 - val_loss: 0.3396 - val_acc: 0.9081\n",
            "Epoch 814/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1236 - acc: 0.9602 - val_loss: 0.3292 - val_acc: 0.9112\n",
            "Epoch 815/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1344 - acc: 0.9548 - val_loss: 0.3454 - val_acc: 0.9106\n",
            "Epoch 816/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1314 - acc: 0.9587 - val_loss: 0.3422 - val_acc: 0.9069\n",
            "Epoch 817/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1314 - acc: 0.9560 - val_loss: 0.3360 - val_acc: 0.9051\n",
            "Epoch 818/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1322 - acc: 0.9520 - val_loss: 0.3293 - val_acc: 0.9094\n",
            "Epoch 819/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1381 - acc: 0.9526 - val_loss: 0.3464 - val_acc: 0.9081\n",
            "Epoch 820/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.1302 - acc: 0.9566 - val_loss: 0.3446 - val_acc: 0.9032\n",
            "Epoch 821/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1217 - acc: 0.9584 - val_loss: 0.3319 - val_acc: 0.9069\n",
            "Epoch 822/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1325 - acc: 0.9566 - val_loss: 0.3439 - val_acc: 0.9057\n",
            "Epoch 823/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1304 - acc: 0.9578 - val_loss: 0.3335 - val_acc: 0.9112\n",
            "Epoch 824/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1382 - acc: 0.9517 - val_loss: 0.3464 - val_acc: 0.9014\n",
            "Epoch 825/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1300 - acc: 0.9575 - val_loss: 0.3413 - val_acc: 0.9124\n",
            "Epoch 826/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1284 - acc: 0.9569 - val_loss: 0.3384 - val_acc: 0.9124\n",
            "Epoch 827/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1229 - acc: 0.9587 - val_loss: 0.3480 - val_acc: 0.9094\n",
            "Epoch 828/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1214 - acc: 0.9593 - val_loss: 0.3326 - val_acc: 0.9069\n",
            "Epoch 829/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1163 - acc: 0.9587 - val_loss: 0.3310 - val_acc: 0.9088\n",
            "Epoch 830/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1279 - acc: 0.9526 - val_loss: 0.3336 - val_acc: 0.9100\n",
            "Epoch 831/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1238 - acc: 0.9593 - val_loss: 0.3262 - val_acc: 0.9088\n",
            "Epoch 832/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1208 - acc: 0.9608 - val_loss: 0.3401 - val_acc: 0.9081\n",
            "Epoch 833/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1212 - acc: 0.9626 - val_loss: 0.3437 - val_acc: 0.9057\n",
            "Epoch 834/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1272 - acc: 0.9569 - val_loss: 0.3361 - val_acc: 0.9143\n",
            "Epoch 835/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.1321 - acc: 0.9560 - val_loss: 0.3228 - val_acc: 0.9124\n",
            "Epoch 836/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1252 - acc: 0.9593 - val_loss: 0.3390 - val_acc: 0.9075\n",
            "Epoch 837/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1252 - acc: 0.9590 - val_loss: 0.3319 - val_acc: 0.9143\n",
            "Epoch 838/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1442 - acc: 0.9526 - val_loss: 0.3241 - val_acc: 0.9075\n",
            "Epoch 839/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1260 - acc: 0.9596 - val_loss: 0.3410 - val_acc: 0.9063\n",
            "Epoch 840/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1293 - acc: 0.9590 - val_loss: 0.3168 - val_acc: 0.9143\n",
            "Epoch 841/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1183 - acc: 0.9596 - val_loss: 0.3191 - val_acc: 0.9161\n",
            "Epoch 842/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.1284 - acc: 0.9548 - val_loss: 0.3205 - val_acc: 0.9161\n",
            "Epoch 843/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1261 - acc: 0.9590 - val_loss: 0.3291 - val_acc: 0.9149\n",
            "Epoch 844/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.1395 - acc: 0.9578 - val_loss: 0.3413 - val_acc: 0.9057\n",
            "Epoch 845/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1344 - acc: 0.9535 - val_loss: 0.3158 - val_acc: 0.9143\n",
            "Epoch 846/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1270 - acc: 0.9605 - val_loss: 0.3312 - val_acc: 0.9112\n",
            "Epoch 847/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1166 - acc: 0.9620 - val_loss: 0.3661 - val_acc: 0.9020\n",
            "Epoch 848/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1331 - acc: 0.9529 - val_loss: 0.3256 - val_acc: 0.9143\n",
            "Epoch 849/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1361 - acc: 0.9551 - val_loss: 0.3310 - val_acc: 0.9106\n",
            "Epoch 850/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1239 - acc: 0.9572 - val_loss: 0.3253 - val_acc: 0.9100\n",
            "Epoch 851/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1302 - acc: 0.9541 - val_loss: 0.3373 - val_acc: 0.9008\n",
            "Epoch 852/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1254 - acc: 0.9623 - val_loss: 0.3224 - val_acc: 0.9137\n",
            "Epoch 853/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1244 - acc: 0.9578 - val_loss: 0.3250 - val_acc: 0.9210\n",
            "Epoch 854/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1297 - acc: 0.9584 - val_loss: 0.3543 - val_acc: 0.9051\n",
            "Epoch 855/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1201 - acc: 0.9596 - val_loss: 0.3259 - val_acc: 0.9094\n",
            "Epoch 856/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1203 - acc: 0.9587 - val_loss: 0.3473 - val_acc: 0.9118\n",
            "Epoch 857/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1264 - acc: 0.9590 - val_loss: 0.3429 - val_acc: 0.9057\n",
            "Epoch 858/1000\n",
            "3315/3315 [==============================] - 1s 410us/step - loss: 0.1144 - acc: 0.9635 - val_loss: 0.3401 - val_acc: 0.9057\n",
            "Epoch 859/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1178 - acc: 0.9575 - val_loss: 0.3286 - val_acc: 0.9100\n",
            "Epoch 860/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.1427 - acc: 0.9517 - val_loss: 0.3204 - val_acc: 0.9149\n",
            "Epoch 861/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1175 - acc: 0.9635 - val_loss: 0.3345 - val_acc: 0.9124\n",
            "Epoch 862/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1235 - acc: 0.9557 - val_loss: 0.3386 - val_acc: 0.9063\n",
            "Epoch 863/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1263 - acc: 0.9581 - val_loss: 0.3334 - val_acc: 0.9081\n",
            "Epoch 864/1000\n",
            "3315/3315 [==============================] - 1s 407us/step - loss: 0.1284 - acc: 0.9560 - val_loss: 0.3343 - val_acc: 0.9112\n",
            "Epoch 865/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1316 - acc: 0.9554 - val_loss: 0.3142 - val_acc: 0.9118\n",
            "Epoch 866/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1254 - acc: 0.9593 - val_loss: 0.3412 - val_acc: 0.9094\n",
            "Epoch 867/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1288 - acc: 0.9590 - val_loss: 0.3141 - val_acc: 0.9155\n",
            "Epoch 868/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1184 - acc: 0.9605 - val_loss: 0.3317 - val_acc: 0.9161\n",
            "Epoch 869/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1225 - acc: 0.9578 - val_loss: 0.3409 - val_acc: 0.9112\n",
            "Epoch 870/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1295 - acc: 0.9557 - val_loss: 0.3318 - val_acc: 0.9081\n",
            "Epoch 871/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1283 - acc: 0.9611 - val_loss: 0.3144 - val_acc: 0.9112\n",
            "Epoch 872/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1207 - acc: 0.9605 - val_loss: 0.3356 - val_acc: 0.9149\n",
            "Epoch 873/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1066 - acc: 0.9626 - val_loss: 0.3369 - val_acc: 0.9118\n",
            "Epoch 874/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1184 - acc: 0.9605 - val_loss: 0.3206 - val_acc: 0.9106\n",
            "Epoch 875/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1088 - acc: 0.9635 - val_loss: 0.3483 - val_acc: 0.8977\n",
            "Epoch 876/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.1298 - acc: 0.9566 - val_loss: 0.3361 - val_acc: 0.9155\n",
            "Epoch 877/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1133 - acc: 0.9614 - val_loss: 0.3372 - val_acc: 0.9026\n",
            "Epoch 878/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1041 - acc: 0.9662 - val_loss: 0.3278 - val_acc: 0.9161\n",
            "Epoch 879/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1241 - acc: 0.9572 - val_loss: 0.3597 - val_acc: 0.9020\n",
            "Epoch 880/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.1047 - acc: 0.9671 - val_loss: 0.3620 - val_acc: 0.9057\n",
            "Epoch 881/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1217 - acc: 0.9563 - val_loss: 0.3338 - val_acc: 0.9124\n",
            "Epoch 882/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1253 - acc: 0.9596 - val_loss: 0.3377 - val_acc: 0.9081\n",
            "Epoch 883/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1222 - acc: 0.9629 - val_loss: 0.3375 - val_acc: 0.9100\n",
            "Epoch 884/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1164 - acc: 0.9605 - val_loss: 0.3554 - val_acc: 0.9063\n",
            "Epoch 885/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1084 - acc: 0.9623 - val_loss: 0.3577 - val_acc: 0.9032\n",
            "Epoch 886/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1199 - acc: 0.9617 - val_loss: 0.3581 - val_acc: 0.9002\n",
            "Epoch 887/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1246 - acc: 0.9602 - val_loss: 0.3270 - val_acc: 0.9137\n",
            "Epoch 888/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1096 - acc: 0.9644 - val_loss: 0.3375 - val_acc: 0.9057\n",
            "Epoch 889/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1205 - acc: 0.9590 - val_loss: 0.3697 - val_acc: 0.8947\n",
            "Epoch 890/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1266 - acc: 0.9614 - val_loss: 0.3333 - val_acc: 0.9106\n",
            "Epoch 891/1000\n",
            "3315/3315 [==============================] - 1s 427us/step - loss: 0.1267 - acc: 0.9560 - val_loss: 0.3312 - val_acc: 0.9112\n",
            "Epoch 892/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1102 - acc: 0.9608 - val_loss: 0.3311 - val_acc: 0.9106\n",
            "Epoch 893/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1150 - acc: 0.9593 - val_loss: 0.3169 - val_acc: 0.9173\n",
            "Epoch 894/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1145 - acc: 0.9653 - val_loss: 0.3201 - val_acc: 0.9124\n",
            "Epoch 895/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1045 - acc: 0.9650 - val_loss: 0.3362 - val_acc: 0.9094\n",
            "Epoch 896/1000\n",
            "3315/3315 [==============================] - 1s 411us/step - loss: 0.1381 - acc: 0.9532 - val_loss: 0.3459 - val_acc: 0.9026\n",
            "Epoch 897/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1311 - acc: 0.9590 - val_loss: 0.3403 - val_acc: 0.9088\n",
            "Epoch 898/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1105 - acc: 0.9617 - val_loss: 0.3268 - val_acc: 0.9069\n",
            "Epoch 899/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1193 - acc: 0.9581 - val_loss: 0.3402 - val_acc: 0.9069\n",
            "Epoch 900/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1122 - acc: 0.9614 - val_loss: 0.3464 - val_acc: 0.9094\n",
            "Epoch 901/1000\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 0.1052 - acc: 0.9644 - val_loss: 0.3362 - val_acc: 0.9112\n",
            "Epoch 902/1000\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.1112 - acc: 0.9623 - val_loss: 0.3298 - val_acc: 0.9063\n",
            "Epoch 903/1000\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 0.1188 - acc: 0.9608 - val_loss: 0.3307 - val_acc: 0.9155\n",
            "Epoch 904/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1101 - acc: 0.9638 - val_loss: 0.3260 - val_acc: 0.9161\n",
            "Epoch 905/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1168 - acc: 0.9638 - val_loss: 0.3407 - val_acc: 0.9075\n",
            "Epoch 906/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1139 - acc: 0.9623 - val_loss: 0.3383 - val_acc: 0.9063\n",
            "Epoch 907/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1048 - acc: 0.9659 - val_loss: 0.3403 - val_acc: 0.9112\n",
            "Epoch 908/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1173 - acc: 0.9602 - val_loss: 0.3287 - val_acc: 0.9198\n",
            "Epoch 909/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.1145 - acc: 0.9599 - val_loss: 0.3235 - val_acc: 0.9069\n",
            "Epoch 910/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1176 - acc: 0.9620 - val_loss: 0.3284 - val_acc: 0.9112\n",
            "Epoch 911/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1148 - acc: 0.9632 - val_loss: 0.3216 - val_acc: 0.9130\n",
            "Epoch 912/1000\n",
            "3315/3315 [==============================] - 1s 411us/step - loss: 0.1078 - acc: 0.9593 - val_loss: 0.3364 - val_acc: 0.9124\n",
            "Epoch 913/1000\n",
            "3315/3315 [==============================] - 1s 411us/step - loss: 0.1167 - acc: 0.9623 - val_loss: 0.3533 - val_acc: 0.9100\n",
            "Epoch 914/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.1170 - acc: 0.9641 - val_loss: 0.3429 - val_acc: 0.9039\n",
            "Epoch 915/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1181 - acc: 0.9632 - val_loss: 0.3513 - val_acc: 0.9075\n",
            "Epoch 916/1000\n",
            "3315/3315 [==============================] - 1s 409us/step - loss: 0.1114 - acc: 0.9617 - val_loss: 0.3343 - val_acc: 0.9112\n",
            "Epoch 917/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1131 - acc: 0.9578 - val_loss: 0.3425 - val_acc: 0.9075\n",
            "Epoch 918/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1176 - acc: 0.9554 - val_loss: 0.3284 - val_acc: 0.9155\n",
            "Epoch 919/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.1081 - acc: 0.9623 - val_loss: 0.3362 - val_acc: 0.9112\n",
            "Epoch 920/1000\n",
            "3315/3315 [==============================] - 1s 411us/step - loss: 0.1133 - acc: 0.9620 - val_loss: 0.3246 - val_acc: 0.9094\n",
            "Epoch 921/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.0990 - acc: 0.9656 - val_loss: 0.3289 - val_acc: 0.9118\n",
            "Epoch 922/1000\n",
            "3315/3315 [==============================] - 1s 411us/step - loss: 0.1160 - acc: 0.9620 - val_loss: 0.3470 - val_acc: 0.9014\n",
            "Epoch 923/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.0981 - acc: 0.9656 - val_loss: 0.3309 - val_acc: 0.9137\n",
            "Epoch 924/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1148 - acc: 0.9605 - val_loss: 0.3340 - val_acc: 0.9106\n",
            "Epoch 925/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1107 - acc: 0.9668 - val_loss: 0.3417 - val_acc: 0.9149\n",
            "Epoch 926/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1068 - acc: 0.9659 - val_loss: 0.3568 - val_acc: 0.9106\n",
            "Epoch 927/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1192 - acc: 0.9644 - val_loss: 0.3261 - val_acc: 0.9149\n",
            "Epoch 928/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1124 - acc: 0.9653 - val_loss: 0.3318 - val_acc: 0.9112\n",
            "Epoch 929/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.1141 - acc: 0.9614 - val_loss: 0.3256 - val_acc: 0.9155\n",
            "Epoch 930/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1229 - acc: 0.9608 - val_loss: 0.3099 - val_acc: 0.9137\n",
            "Epoch 931/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.1057 - acc: 0.9653 - val_loss: 0.3151 - val_acc: 0.9179\n",
            "Epoch 932/1000\n",
            "3315/3315 [==============================] - 1s 410us/step - loss: 0.1173 - acc: 0.9635 - val_loss: 0.3267 - val_acc: 0.9094\n",
            "Epoch 933/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.1126 - acc: 0.9680 - val_loss: 0.3324 - val_acc: 0.9179\n",
            "Epoch 934/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1207 - acc: 0.9617 - val_loss: 0.3279 - val_acc: 0.9143\n",
            "Epoch 935/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1149 - acc: 0.9611 - val_loss: 0.3222 - val_acc: 0.9124\n",
            "Epoch 936/1000\n",
            "3315/3315 [==============================] - 1s 410us/step - loss: 0.1058 - acc: 0.9683 - val_loss: 0.3291 - val_acc: 0.9118\n",
            "Epoch 937/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1229 - acc: 0.9587 - val_loss: 0.3409 - val_acc: 0.9094\n",
            "Epoch 938/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1177 - acc: 0.9614 - val_loss: 0.3468 - val_acc: 0.9075\n",
            "Epoch 939/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1111 - acc: 0.9629 - val_loss: 0.3401 - val_acc: 0.9094\n",
            "Epoch 940/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1018 - acc: 0.9656 - val_loss: 0.3330 - val_acc: 0.9106\n",
            "Epoch 941/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1105 - acc: 0.9641 - val_loss: 0.3382 - val_acc: 0.9075\n",
            "Epoch 942/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.1019 - acc: 0.9659 - val_loss: 0.3170 - val_acc: 0.9149\n",
            "Epoch 943/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1086 - acc: 0.9635 - val_loss: 0.3313 - val_acc: 0.9155\n",
            "Epoch 944/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1093 - acc: 0.9587 - val_loss: 0.3346 - val_acc: 0.9118\n",
            "Epoch 945/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1199 - acc: 0.9617 - val_loss: 0.3391 - val_acc: 0.9075\n",
            "Epoch 946/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.1246 - acc: 0.9569 - val_loss: 0.3197 - val_acc: 0.9173\n",
            "Epoch 947/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1171 - acc: 0.9623 - val_loss: 0.3228 - val_acc: 0.9137\n",
            "Epoch 948/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1122 - acc: 0.9635 - val_loss: 0.3531 - val_acc: 0.9081\n",
            "Epoch 949/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1272 - acc: 0.9569 - val_loss: 0.3371 - val_acc: 0.9075\n",
            "Epoch 950/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.1122 - acc: 0.9623 - val_loss: 0.3493 - val_acc: 0.9045\n",
            "Epoch 951/1000\n",
            "3315/3315 [==============================] - 1s 413us/step - loss: 0.1075 - acc: 0.9617 - val_loss: 0.3359 - val_acc: 0.9100\n",
            "Epoch 952/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1093 - acc: 0.9671 - val_loss: 0.3362 - val_acc: 0.9100\n",
            "Epoch 953/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1120 - acc: 0.9620 - val_loss: 0.3234 - val_acc: 0.9155\n",
            "Epoch 954/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.1099 - acc: 0.9614 - val_loss: 0.3358 - val_acc: 0.9167\n",
            "Epoch 955/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1033 - acc: 0.9674 - val_loss: 0.3415 - val_acc: 0.9155\n",
            "Epoch 956/1000\n",
            "3315/3315 [==============================] - 1s 410us/step - loss: 0.1058 - acc: 0.9632 - val_loss: 0.3293 - val_acc: 0.9228\n",
            "Epoch 957/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.1078 - acc: 0.9614 - val_loss: 0.3325 - val_acc: 0.9081\n",
            "Epoch 958/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.1024 - acc: 0.9665 - val_loss: 0.3375 - val_acc: 0.9149\n",
            "Epoch 959/1000\n",
            "3315/3315 [==============================] - 1s 412us/step - loss: 0.1132 - acc: 0.9623 - val_loss: 0.3222 - val_acc: 0.9198\n",
            "Epoch 960/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.1042 - acc: 0.9647 - val_loss: 0.3182 - val_acc: 0.9143\n",
            "Epoch 961/1000\n",
            "3315/3315 [==============================] - 1s 415us/step - loss: 0.1201 - acc: 0.9575 - val_loss: 0.3228 - val_acc: 0.9143\n",
            "Epoch 962/1000\n",
            "3315/3315 [==============================] - 1s 423us/step - loss: 0.1151 - acc: 0.9620 - val_loss: 0.3338 - val_acc: 0.9204\n",
            "Epoch 963/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.1027 - acc: 0.9653 - val_loss: 0.3180 - val_acc: 0.9112\n",
            "Epoch 964/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.0968 - acc: 0.9677 - val_loss: 0.3400 - val_acc: 0.9118\n",
            "Epoch 965/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1153 - acc: 0.9614 - val_loss: 0.3345 - val_acc: 0.9094\n",
            "Epoch 966/1000\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 0.1077 - acc: 0.9623 - val_loss: 0.3266 - val_acc: 0.9094\n",
            "Epoch 967/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1126 - acc: 0.9620 - val_loss: 0.3250 - val_acc: 0.9143\n",
            "Epoch 968/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1234 - acc: 0.9554 - val_loss: 0.3314 - val_acc: 0.9106\n",
            "Epoch 969/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1028 - acc: 0.9668 - val_loss: 0.3265 - val_acc: 0.9179\n",
            "Epoch 970/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.1229 - acc: 0.9629 - val_loss: 0.3391 - val_acc: 0.9130\n",
            "Epoch 971/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1050 - acc: 0.9641 - val_loss: 0.3379 - val_acc: 0.9094\n",
            "Epoch 972/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1087 - acc: 0.9623 - val_loss: 0.3218 - val_acc: 0.9179\n",
            "Epoch 973/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1070 - acc: 0.9641 - val_loss: 0.3325 - val_acc: 0.9075\n",
            "Epoch 974/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1034 - acc: 0.9671 - val_loss: 0.3389 - val_acc: 0.9130\n",
            "Epoch 975/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.0968 - acc: 0.9662 - val_loss: 0.3260 - val_acc: 0.9094\n",
            "Epoch 976/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.0979 - acc: 0.9668 - val_loss: 0.3290 - val_acc: 0.9186\n",
            "Epoch 977/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1205 - acc: 0.9599 - val_loss: 0.3410 - val_acc: 0.9143\n",
            "Epoch 978/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1058 - acc: 0.9686 - val_loss: 0.3388 - val_acc: 0.9100\n",
            "Epoch 979/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.1164 - acc: 0.9608 - val_loss: 0.3245 - val_acc: 0.9192\n",
            "Epoch 980/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1213 - acc: 0.9602 - val_loss: 0.3323 - val_acc: 0.9155\n",
            "Epoch 981/1000\n",
            "3315/3315 [==============================] - 1s 424us/step - loss: 0.1060 - acc: 0.9647 - val_loss: 0.3324 - val_acc: 0.9149\n",
            "Epoch 982/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.0893 - acc: 0.9701 - val_loss: 0.3229 - val_acc: 0.9124\n",
            "Epoch 983/1000\n",
            "3315/3315 [==============================] - 1s 425us/step - loss: 0.0976 - acc: 0.9665 - val_loss: 0.3399 - val_acc: 0.9124\n",
            "Epoch 984/1000\n",
            "3315/3315 [==============================] - 1s 410us/step - loss: 0.1167 - acc: 0.9602 - val_loss: 0.3478 - val_acc: 0.9069\n",
            "Epoch 985/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1105 - acc: 0.9635 - val_loss: 0.3175 - val_acc: 0.9100\n",
            "Epoch 986/1000\n",
            "3315/3315 [==============================] - 1s 418us/step - loss: 0.1055 - acc: 0.9617 - val_loss: 0.3235 - val_acc: 0.9106\n",
            "Epoch 987/1000\n",
            "3315/3315 [==============================] - 1s 416us/step - loss: 0.1091 - acc: 0.9656 - val_loss: 0.3299 - val_acc: 0.9088\n",
            "Epoch 988/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.1001 - acc: 0.9662 - val_loss: 0.3381 - val_acc: 0.9124\n",
            "Epoch 989/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1043 - acc: 0.9662 - val_loss: 0.3365 - val_acc: 0.9106\n",
            "Epoch 990/1000\n",
            "3315/3315 [==============================] - 1s 421us/step - loss: 0.1122 - acc: 0.9611 - val_loss: 0.3286 - val_acc: 0.9106\n",
            "Epoch 991/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.0980 - acc: 0.9689 - val_loss: 0.3305 - val_acc: 0.9124\n",
            "Epoch 992/1000\n",
            "3315/3315 [==============================] - 1s 419us/step - loss: 0.1019 - acc: 0.9632 - val_loss: 0.3318 - val_acc: 0.9143\n",
            "Epoch 993/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1046 - acc: 0.9650 - val_loss: 0.3365 - val_acc: 0.9094\n",
            "Epoch 994/1000\n",
            "3315/3315 [==============================] - 1s 411us/step - loss: 0.1055 - acc: 0.9632 - val_loss: 0.3210 - val_acc: 0.9173\n",
            "Epoch 995/1000\n",
            "3315/3315 [==============================] - 1s 414us/step - loss: 0.1047 - acc: 0.9686 - val_loss: 0.3295 - val_acc: 0.9124\n",
            "Epoch 996/1000\n",
            "3315/3315 [==============================] - 1s 417us/step - loss: 0.1057 - acc: 0.9662 - val_loss: 0.3146 - val_acc: 0.9143\n",
            "Epoch 997/1000\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 0.0993 - acc: 0.9668 - val_loss: 0.3364 - val_acc: 0.9137\n",
            "Epoch 998/1000\n",
            "3315/3315 [==============================] - 1s 420us/step - loss: 0.1054 - acc: 0.9665 - val_loss: 0.3503 - val_acc: 0.9173\n",
            "Epoch 999/1000\n",
            "3315/3315 [==============================] - 1s 448us/step - loss: 0.1144 - acc: 0.9635 - val_loss: 0.3263 - val_acc: 0.9149\n",
            "Epoch 1000/1000\n",
            "3315/3315 [==============================] - 1s 449us/step - loss: 0.1060 - acc: 0.9644 - val_loss: 0.3461 - val_acc: 0.9088\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mFytY6LDzgJ0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's plot the loss:"
      ]
    },
    {
      "metadata": {
        "id": "TFz4ClZov9gZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "d913f09f-6c6a-41a6-e8ca-69657edc0d04"
      },
      "cell_type": "code",
      "source": [
        "plt.plot(cnnhistory.history['loss'])\n",
        "plt.plot(cnnhistory.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEVCAYAAAAb/KWvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEVK4i4gUIYW7iEgRUriLiBSh/w+XJb+02RAfkgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Vf1W7LgP2DA5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "And now let's plot the accuracy:"
      ]
    },
    {
      "metadata": {
        "id": "8yyFBt7ASPUe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "62e03861-3ac5-495c-cf41-5ee9281d3c3b"
      },
      "cell_type": "code",
      "source": [
        "plt.plot(cnnhistory.history['acc'])\n",
        "plt.plot(cnnhistory.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "gaZONl1mD8XD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now create a classification report to review the f1-score of the model per class.\n",
        "To do so, we have to:\n",
        "- Create a variable predictions that will contain the model.predict_classes outcome\n",
        "- Convert our y_test (array of strings with our classes) to an array of int called new_Ytest, otherwise it will not be comparable to the predictions by the classification report."
      ]
    },
    {
      "metadata": {
        "id": "EO25uIL-9vqx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = model.predict_classes(x_testcnn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1i06grlBBSrn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3eb54e9c-7321-4571-9cdb-25cf001b8090"
      },
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 5, 4, ..., 2, 5, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "metadata": {
        "id": "HUHshx93CM_6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "49cc7750-69a7-4379-ba64-57b4424901db"
      },
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['02', '05', '04', ..., '02', '05', '01'], dtype='<U2')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "metadata": {
        "id": "tMxojpvWCxOs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_Ytest = y_test.astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W07EQaC8DE6i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f89f325e-988a-404b-c342-fc0ee306b3b2"
      },
      "cell_type": "code",
      "source": [
        "new_Ytest"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 5, 4, ..., 2, 5, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "metadata": {
        "id": "FW2XHdTtEedk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Okay, now we can display the classification report:"
      ]
    },
    {
      "metadata": {
        "id": "IfVSRmMu96rC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "5b426820-e752-4fb7-82d4-7fe5b1630aa3"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "report = classification_report(new_Ytest, predictions)\n",
        "\n",
        "print(report)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.95      0.94      0.94       134\n",
            "           2       0.98      0.90      0.94       251\n",
            "           3       0.90      0.90      0.90       242\n",
            "           4       0.88      0.85      0.86       271\n",
            "           5       0.93      0.96      0.95       253\n",
            "           6       0.86      0.95      0.90       239\n",
            "           7       0.94      0.94      0.94       127\n",
            "           8       0.82      0.84      0.83       116\n",
            "\n",
            "   micro avg       0.91      0.91      0.91      1633\n",
            "   macro avg       0.91      0.91      0.91      1633\n",
            "weighted avg       0.91      0.91      0.91      1633\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x_ySPOyHxkZ3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Save the model"
      ]
    },
    {
      "metadata": {
        "id": "f5kRmoD-sdHj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a90a551-760a-435c-c3ef-2cb402545019"
      },
      "cell_type": "code",
      "source": [
        "model_name = 'Emotion_Voice_Detection_Model.h5'\n",
        "save_dir = '/content/drive/My Drive/Ravdess_model'\n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved trained model at /content/drive/My Drive/Ravdess_model/Emotion_Voice_Detection_Model.h5 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MNUiznKNwUtJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Reloading the model to test it"
      ]
    },
    {
      "metadata": {
        "id": "T4oAv6Kx8RBE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "2721be47-ff65-43d6-a418-22b53d88a6da"
      },
      "cell_type": "code",
      "source": [
        "loaded_model = keras.models.load_model('/content/drive/My Drive/Ravdess_model/Emotion_Voice_Detection_Model.h5')\n",
        "loaded_model.summary()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_1 (Conv1D)            (None, 40, 128)           768       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 5, 128)            82048     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 640)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                6410      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 89,226\n",
            "Trainable params: 89,226\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FHtPzc0Y8hfZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Checking the accuracy of the loaded model"
      ]
    },
    {
      "metadata": {
        "id": "qUi-Zjuf8hDB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b68d2e5b-6df1-4372-8dbe-41528cc958b4"
      },
      "cell_type": "code",
      "source": [
        "loss, acc = loaded_model.evaluate(x_testcnn, y_test)\n",
        "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1633/1633 [==============================] - 0s 112us/step\n",
            "Restored model, accuracy: 90.88%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8pXH3y7S9A1N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Thank you for your attention! To be continued.."
      ]
    }
  ]
}