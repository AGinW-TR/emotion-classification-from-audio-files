{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EmotionsRecognition.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "CjWvnaQUrZmD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Emotion classification using the RAVDESS dataset"
      ]
    },
    {
      "metadata": {
        "id": "ldtHMhuLrewK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) is licensed under CC BY-NA-SC 4.0. and can be downloaded free of charge at https://zenodo.org/record/1188976.\n",
        "\n",
        "***Construction and Validation***\n",
        "\n",
        "Construction and validation of the RAVDESS is described in our paper: Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): e0196391. https://doi.org/10.1371/journal.pone.0196391.\n",
        "\n",
        "The RAVDESS contains 7356 files. Each file was rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained adult research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity, interrater reliability, and test-retest intrarater reliability were reported. Validation data is open-access, and can be downloaded along with our paper from PLOS ONE.\n",
        "\n",
        "***Description***\n",
        "\n",
        "The dataset contains the complete set of 7356 RAVDESS files (total size: 24.8 GB). Each of the 24 actors consists of three modality formats: Audio-only (16bit, 48kHz .wav), Audio-Video (720p H.264, AAC 48kHz, .mp4), and Video-only (no sound).  Note, there are no song files for Actor_18.\n",
        "\n",
        "***Data***\n",
        "\n",
        "For this notebook, Audio-Only files have been used (2452 files).\n",
        "\n",
        "***License information***\n",
        "\n",
        "The RAVDESS is released under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License, CC BY-NA-SC 4.0\n",
        "\n",
        "***File naming convention***\n",
        "\n",
        "Each of the 7356 RAVDESS files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 02-01-06-01-02-01-12.mp4). These identifiers define the stimulus characteristics:\n",
        "\n",
        "***Filename identifiers***\n",
        "\n",
        "- Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
        "- Vocal channel (01 = speech, 02 = song).\n",
        "- Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
        "- Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the ‘neutral’ emotion.\n",
        "- Statement (01 = “Kids are talking by the door”, 02 = “Dogs are sitting by the door”).\n",
        "- Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
        "- Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
        "\n",
        "Filename example: 02-01-06-01-02-01-12.mp4 \n",
        "\n",
        "- Video-only (02)\n",
        "- Speech (01)\n",
        "- Fearful (06)\n",
        "- Normal intensity (01)\n",
        "- Statement “dogs” (02)\n",
        "- 1st Repetition (01)\n",
        "- 12th Actor (12)\n",
        "- Female, as the actor ID number is even."
      ]
    },
    {
      "metadata": {
        "id": "JDNbxj45rkvB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Analysis\n",
        "\n",
        "We are using Colab, a Google Cloud environment for jupyter, so we need to import our files from Google Drive and then install LibROSA, a python package for music and audio analysis.\n",
        "\n",
        "After the import, we will plot the signal of the first file."
      ]
    },
    {
      "metadata": {
        "id": "N-o2JI49WBAe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EgFwaDhMbJVm",
        "colab_type": "code",
        "outputId": "b06e31fc-0652-481d-e2b0-88db30011947",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install librosa"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (2.1.6)\n",
            "Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.14.6)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.20.1)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.13.0)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (4.3.0)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.11.0)\n",
            "Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.2.1)\n",
            "Requirement already satisfied: numba>=0.38.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.40.1)\n",
            "Requirement already satisfied: llvmlite>=0.25.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa) (0.26.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rxI4xzngdS-e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "from librosa import display\n",
        "\n",
        "data, sampling_rate = librosa.load('/content/drive/My Drive/Ravdess/03-01-01-01-01-01-01.wav')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WgaSHtCIdtX2",
        "colab_type": "code",
        "outputId": "38818861-a194-418d-a991-6b91cc6f6bf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "cell_type": "code",
      "source": [
        "% pylab inline\n",
        "import os\n",
        "import pandas as pd\n",
        "import glob \n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "librosa.display.waveplot(data, sr=sampling_rate)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['display']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PolyCollection at 0x7fed026d1208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAswAAAEGCAYAAABxSsNVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fed055a41d0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "vCtNuVWlr5jL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load all files\n",
        "\n",
        "We will create our numpy array extracting Mel-frequency cepstral coefficients (MFCCs), while the classes to predict will be extracted from the name of the file (see the introductory section of this notebook to see the naming convention of the files of this dataset)."
      ]
    },
    {
      "metadata": {
        "id": "AKvuF--gd6F-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path = '/content/drive/My Drive/Ravdess'\n",
        "lst = []\n",
        "\n",
        "for subdir, dirs, files in os.walk(path):\n",
        "  for file in files:\n",
        "      try:\n",
        "        #Load librosa array, obtain mfcss, store the file and the mcss information in a new array\n",
        "        X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')\n",
        "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
        "        file = file[6:8]\n",
        "        arr = mfccs, file\n",
        "        lst.append(arr)\n",
        "      # If the file is not valid, skip it\n",
        "      except ValueError:\n",
        "        continue       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kLSggnF7kKY1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Creating X and y: zip makes a list of all the first elements, and a list of all the second elements.\n",
        "X, y = zip(*lst)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VzvBRTJIlIE9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "15d99965-91cd-4bb3-f165-e65cb8068ec4"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X = np.asarray(X)\n",
        "y = np.asarray(y)\n",
        "\n",
        "\n",
        "X.shape, y.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4948, 40), (4948,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "Agw-3KN1sDhh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Decision Tree Classifier\n",
        "\n",
        "To make a first attempt in accomplishing this classification task I chose a decision tree:"
      ]
    },
    {
      "metadata": {
        "id": "Q-Xgb5NslTBO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UshLOC1ClWL3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_BnCR52nlXw0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dtree = DecisionTreeClassifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qWyTownblZM0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "3ce24d8b-4059-44d8-a973-17d79eae38ba"
      },
      "cell_type": "code",
      "source": [
        "dtree.fit(X_train, y_train)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
              "            max_features=None, max_leaf_nodes=None,\n",
              "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "            min_samples_leaf=1, min_samples_split=2,\n",
              "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
              "            splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "HEuw6TUQlr7C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = dtree.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_1v0i0V7sMw7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's go with our classification report.\n",
        "\n",
        "Before we start, a quick reminder of the classes we are trying to predict:\n",
        "\n",
        "emotions = {\n",
        "    \"neutral\": \"01\",\n",
        "    \"calm\": \"02\",\n",
        "    \"happy\": \"03\",\n",
        "    \"sad\": \"04\",\n",
        "    \"angry\": \"05\", \n",
        "    \"fearful\": \"06\", \n",
        "    \"disgust\": \"07\", \n",
        "    \"surprised\": \"08\"\n",
        "}"
      ]
    },
    {
      "metadata": {
        "id": "c4kNSYkAleIv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "3735ea8d-f308-4dc4-a769-cce964f42f54"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "print(classification_report(y_test,predictions))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          01       0.77      0.79      0.78       134\n",
            "          02       0.88      0.82      0.85       251\n",
            "          03       0.78      0.74      0.76       242\n",
            "          04       0.77      0.75      0.76       271\n",
            "          05       0.81      0.86      0.84       253\n",
            "          06       0.76      0.82      0.79       239\n",
            "          07       0.74      0.72      0.73       127\n",
            "          08       0.72      0.75      0.74       116\n",
            "\n",
            "   micro avg       0.79      0.79      0.79      1633\n",
            "   macro avg       0.78      0.78      0.78      1633\n",
            "weighted avg       0.79      0.79      0.79      1633\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x2YH_Ttaw2Am",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Being a multiclass problem (8 classes to predict) and with a little dataset, the result is not that bad.\n",
        "\n",
        "In particular, we have a starting point for precision/recall tradeoff for the classes Calm (02) and Angry (05)"
      ]
    },
    {
      "metadata": {
        "id": "lCVgjLj-gwE2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ]
    },
    {
      "metadata": {
        "id": "jfaTxzZ1w__y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this second approach, I switched to a random forest classifier and I made a gridsearch to make some hyperparameters tuning.\n",
        "\n",
        "The gridsearch is not shown in the code below otherwise the notebook will require too much time to run."
      ]
    },
    {
      "metadata": {
        "id": "wcov_DCXgs7v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3eo0ljqzg-KM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rforest = RandomForestClassifier(criterion=\"gini\", max_depth=10, max_features=\"log2\", \n",
        "                                 max_leaf_nodes = 100, min_samples_leaf = 3, min_samples_split = 20, \n",
        "                                 n_estimators= 22000, random_state= 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tg45qSOfg-26",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "ddcdee11-7e72-4a2b-9ed2-2edc821e59a2"
      },
      "cell_type": "code",
      "source": [
        "rforest.fit(X_train, y_train)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
              "            max_depth=10, max_features='log2', max_leaf_nodes=100,\n",
              "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "            min_samples_leaf=3, min_samples_split=20,\n",
              "            min_weight_fraction_leaf=0.0, n_estimators=22000, n_jobs=None,\n",
              "            oob_score=False, random_state=5, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "aM8KU3qxhGBM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = rforest.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "296FW5sBdanI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "0bbcfb77-92fe-4c1d-99d0-3e47e35117ff"
      },
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test,predictions))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          01       1.00      0.54      0.70       134\n",
            "          02       0.66      0.96      0.78       251\n",
            "          03       0.86      0.71      0.78       242\n",
            "          04       0.81      0.64      0.71       271\n",
            "          05       0.89      0.88      0.88       253\n",
            "          06       0.70      0.80      0.75       239\n",
            "          07       0.73      0.61      0.66       127\n",
            "          08       0.60      0.78      0.68       116\n",
            "\n",
            "   micro avg       0.76      0.76      0.76      1633\n",
            "   macro avg       0.78      0.74      0.74      1633\n",
            "weighted avg       0.79      0.76      0.76      1633\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ksfovJ2RxLTS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We have an improvement: what we see is that the classes \"Happy\" (03), Angry\" (05) and \"Neutral\" (01) are the easiest to predict for this model."
      ]
    },
    {
      "metadata": {
        "id": "t9eqMHV3S8i6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Neural network"
      ]
    },
    {
      "metadata": {
        "id": "G-QscoyMxQtn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We are going to do 2 actions: \n",
        "\n",
        "1) Expand the dimensions of our array, adding a third one (this is necessary for the Neural Network);\n",
        "\n",
        "2) Build our network."
      ]
    },
    {
      "metadata": {
        "id": "W4i187-Pe-w5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_traincnn = np.expand_dims(X_train, axis=2)\n",
        "x_testcnn = np.expand_dims(X_test, axis=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vnvoCRX1gQCh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "48bd23ed-e1f7-4fe1-9af1-1fb29c69a95b"
      },
      "cell_type": "code",
      "source": [
        "x_traincnn.shape, x_testcnn.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3315, 40, 1), (1633, 40, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "HZOGIpuefCd3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7db8f598-3997-4874-cb9c-3987e2d81782"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from matplotlib.pyplot import specgram\n",
        "import keras\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Input, Flatten, Dropout, Activation\n",
        "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "'''\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(256, 5,padding='same',\n",
        "                 input_shape=(40,1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv1D(128, 5,padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(MaxPooling1D(pool_size=(8)))\n",
        "model.add(Conv1D(128, 5,padding='same',))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv1D(128, 5,padding='same',))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\n",
        "'''\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(128, 5,padding='same',\n",
        "                 input_shape=(40,1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(MaxPooling1D(pool_size=(8)))\n",
        "model.add(Conv1D(128, 5,padding='same',))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "opt = keras.optimizers.rmsprop(lr=0.00005, rho=0.9, epsilon=None, decay=0.0)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "LphftMIZzUvz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "With *model.summary* we can see a recap of what we have build:"
      ]
    },
    {
      "metadata": {
        "id": "pIWPB4Zgfic7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "outputId": "5d56f868-466a-4b14-9707-0d6b0af77132"
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_1 (Conv1D)            (None, 40, 128)           768       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 5, 128)            82048     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 640)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                6410      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 89,226\n",
            "Trainable params: 89,226\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5qQSBeBhzcLu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can compile and fit our model:"
      ]
    },
    {
      "metadata": {
        "id": "iNI1znbsfpTx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ktdF-nJKfq6F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 25768
        },
        "outputId": "54b09a58-5e59-44ac-923c-7618ba386039"
      },
      "cell_type": "code",
      "source": [
        "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=750, validation_data=(x_testcnn, y_test))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3315 samples, validate on 1633 samples\n",
            "Epoch 1/750\n",
            "3315/3315 [==============================] - 2s 689us/step - loss: 6.6879 - acc: 0.1481 - val_loss: 2.5389 - val_acc: 0.1886\n",
            "Epoch 2/750\n",
            "3315/3315 [==============================] - 1s 450us/step - loss: 5.0764 - acc: 0.1508 - val_loss: 2.7122 - val_acc: 0.1702\n",
            "Epoch 3/750\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 4.0037 - acc: 0.1662 - val_loss: 2.0457 - val_acc: 0.2015\n",
            "Epoch 4/750\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 3.1572 - acc: 0.1934 - val_loss: 1.9745 - val_acc: 0.2725\n",
            "Epoch 5/750\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 2.5928 - acc: 0.2009 - val_loss: 1.8108 - val_acc: 0.3129\n",
            "Epoch 6/750\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 2.2282 - acc: 0.2235 - val_loss: 1.9562 - val_acc: 0.1745\n",
            "Epoch 7/750\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 2.0376 - acc: 0.2350 - val_loss: 1.7910 - val_acc: 0.3178\n",
            "Epoch 8/750\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 1.9537 - acc: 0.2585 - val_loss: 1.7755 - val_acc: 0.3613\n",
            "Epoch 9/750\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 1.8838 - acc: 0.2766 - val_loss: 1.7710 - val_acc: 0.3264\n",
            "Epoch 10/750\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 1.8578 - acc: 0.2751 - val_loss: 1.7411 - val_acc: 0.3521\n",
            "Epoch 11/750\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 1.8123 - acc: 0.2947 - val_loss: 1.7299 - val_acc: 0.3509\n",
            "Epoch 12/750\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 1.7876 - acc: 0.2989 - val_loss: 1.6988 - val_acc: 0.3809\n",
            "Epoch 13/750\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 1.7518 - acc: 0.3267 - val_loss: 1.6864 - val_acc: 0.3711\n",
            "Epoch 14/750\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 1.7339 - acc: 0.3285 - val_loss: 1.6742 - val_acc: 0.3711\n",
            "Epoch 15/750\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 1.7171 - acc: 0.3445 - val_loss: 1.6539 - val_acc: 0.3846\n",
            "Epoch 16/750\n",
            "3315/3315 [==============================] - 1s 449us/step - loss: 1.6881 - acc: 0.3581 - val_loss: 1.6243 - val_acc: 0.4182\n",
            "Epoch 17/750\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 1.6765 - acc: 0.3560 - val_loss: 1.6094 - val_acc: 0.4133\n",
            "Epoch 18/750\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 1.6625 - acc: 0.3617 - val_loss: 1.5902 - val_acc: 0.4182\n",
            "Epoch 19/750\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 1.6247 - acc: 0.3888 - val_loss: 1.5647 - val_acc: 0.4317\n",
            "Epoch 20/750\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 1.6132 - acc: 0.3792 - val_loss: 1.5389 - val_acc: 0.4691\n",
            "Epoch 21/750\n",
            "3315/3315 [==============================] - 1s 430us/step - loss: 1.5875 - acc: 0.4012 - val_loss: 1.5337 - val_acc: 0.4550\n",
            "Epoch 22/750\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 1.5845 - acc: 0.4036 - val_loss: 1.5134 - val_acc: 0.4715\n",
            "Epoch 23/750\n",
            "3315/3315 [==============================] - 1s 426us/step - loss: 1.5579 - acc: 0.4130 - val_loss: 1.4795 - val_acc: 0.4936\n",
            "Epoch 24/750\n",
            "3315/3315 [==============================] - 1s 422us/step - loss: 1.5407 - acc: 0.4284 - val_loss: 1.4995 - val_acc: 0.4623\n",
            "Epoch 25/750\n",
            "3315/3315 [==============================] - 1s 428us/step - loss: 1.5272 - acc: 0.4299 - val_loss: 1.4593 - val_acc: 0.4789\n",
            "Epoch 26/750\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 1.4998 - acc: 0.4468 - val_loss: 1.4465 - val_acc: 0.5064\n",
            "Epoch 27/750\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 1.4921 - acc: 0.4437 - val_loss: 1.4251 - val_acc: 0.4923\n",
            "Epoch 28/750\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 1.4768 - acc: 0.4468 - val_loss: 1.4452 - val_acc: 0.4911\n",
            "Epoch 29/750\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 1.4520 - acc: 0.4742 - val_loss: 1.4268 - val_acc: 0.5028\n",
            "Epoch 30/750\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 1.4480 - acc: 0.4630 - val_loss: 1.3762 - val_acc: 0.5028\n",
            "Epoch 31/750\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 1.4344 - acc: 0.4540 - val_loss: 1.3658 - val_acc: 0.5285\n",
            "Epoch 32/750\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 1.4159 - acc: 0.4745 - val_loss: 1.3937 - val_acc: 0.4660\n",
            "Epoch 33/750\n",
            "3315/3315 [==============================] - 2s 503us/step - loss: 1.4130 - acc: 0.4742 - val_loss: 1.3587 - val_acc: 0.5113\n",
            "Epoch 34/750\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 1.3911 - acc: 0.4890 - val_loss: 1.3408 - val_acc: 0.5346\n",
            "Epoch 35/750\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 1.3848 - acc: 0.4950 - val_loss: 1.3465 - val_acc: 0.5334\n",
            "Epoch 36/750\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 1.3856 - acc: 0.4869 - val_loss: 1.3371 - val_acc: 0.5187\n",
            "Epoch 37/750\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 1.3675 - acc: 0.5035 - val_loss: 1.3283 - val_acc: 0.5413\n",
            "Epoch 38/750\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 1.3653 - acc: 0.4974 - val_loss: 1.3199 - val_acc: 0.5364\n",
            "Epoch 39/750\n",
            "3315/3315 [==============================] - 1s 446us/step - loss: 1.3576 - acc: 0.5113 - val_loss: 1.3007 - val_acc: 0.5377\n",
            "Epoch 40/750\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 1.3386 - acc: 0.5152 - val_loss: 1.2904 - val_acc: 0.5524\n",
            "Epoch 41/750\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 1.3235 - acc: 0.5110 - val_loss: 1.2807 - val_acc: 0.5487\n",
            "Epoch 42/750\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 1.3145 - acc: 0.5234 - val_loss: 1.2783 - val_acc: 0.5597\n",
            "Epoch 43/750\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 1.3117 - acc: 0.5186 - val_loss: 1.2870 - val_acc: 0.5432\n",
            "Epoch 44/750\n",
            "3315/3315 [==============================] - 1s 446us/step - loss: 1.2946 - acc: 0.5297 - val_loss: 1.2596 - val_acc: 0.5536\n",
            "Epoch 45/750\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 1.2888 - acc: 0.5288 - val_loss: 1.2580 - val_acc: 0.5511\n",
            "Epoch 46/750\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 1.2719 - acc: 0.5282 - val_loss: 1.2634 - val_acc: 0.5468\n",
            "Epoch 47/750\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 1.2808 - acc: 0.5348 - val_loss: 1.2334 - val_acc: 0.5615\n",
            "Epoch 48/750\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 1.2689 - acc: 0.5436 - val_loss: 1.2306 - val_acc: 0.5787\n",
            "Epoch 49/750\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 1.2592 - acc: 0.5427 - val_loss: 1.2119 - val_acc: 0.5726\n",
            "Epoch 50/750\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 1.2609 - acc: 0.5400 - val_loss: 1.2117 - val_acc: 0.5769\n",
            "Epoch 51/750\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 1.2416 - acc: 0.5532 - val_loss: 1.2426 - val_acc: 0.5481\n",
            "Epoch 52/750\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 1.2422 - acc: 0.5523 - val_loss: 1.2143 - val_acc: 0.5707\n",
            "Epoch 53/750\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 1.2293 - acc: 0.5587 - val_loss: 1.1971 - val_acc: 0.5713\n",
            "Epoch 54/750\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 1.2197 - acc: 0.5566 - val_loss: 1.1808 - val_acc: 0.5903\n",
            "Epoch 55/750\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 1.2051 - acc: 0.5659 - val_loss: 1.1728 - val_acc: 0.5897\n",
            "Epoch 56/750\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 1.2080 - acc: 0.5508 - val_loss: 1.1867 - val_acc: 0.5671\n",
            "Epoch 57/750\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 1.2060 - acc: 0.5635 - val_loss: 1.1566 - val_acc: 0.5842\n",
            "Epoch 58/750\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 1.2007 - acc: 0.5641 - val_loss: 1.1991 - val_acc: 0.5664\n",
            "Epoch 59/750\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 1.1867 - acc: 0.5653 - val_loss: 1.1626 - val_acc: 0.5891\n",
            "Epoch 60/750\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 1.1898 - acc: 0.5741 - val_loss: 1.1787 - val_acc: 0.5867\n",
            "Epoch 61/750\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 1.1815 - acc: 0.5741 - val_loss: 1.1449 - val_acc: 0.6007\n",
            "Epoch 62/750\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 1.1725 - acc: 0.5738 - val_loss: 1.1455 - val_acc: 0.6069\n",
            "Epoch 63/750\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 1.1715 - acc: 0.5798 - val_loss: 1.1313 - val_acc: 0.6020\n",
            "Epoch 64/750\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 1.1508 - acc: 0.5789 - val_loss: 1.1300 - val_acc: 0.6032\n",
            "Epoch 65/750\n",
            "3315/3315 [==============================] - 1s 448us/step - loss: 1.1472 - acc: 0.5786 - val_loss: 1.1292 - val_acc: 0.5922\n",
            "Epoch 66/750\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 1.1459 - acc: 0.5801 - val_loss: 1.1185 - val_acc: 0.5946\n",
            "Epoch 67/750\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 1.1432 - acc: 0.5762 - val_loss: 1.1296 - val_acc: 0.6013\n",
            "Epoch 68/750\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 1.1291 - acc: 0.5946 - val_loss: 1.1066 - val_acc: 0.6154\n",
            "Epoch 69/750\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 1.1254 - acc: 0.5967 - val_loss: 1.1086 - val_acc: 0.6111\n",
            "Epoch 70/750\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 1.1276 - acc: 0.5897 - val_loss: 1.1156 - val_acc: 0.6026\n",
            "Epoch 71/750\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 1.1175 - acc: 0.5961 - val_loss: 1.0882 - val_acc: 0.6197\n",
            "Epoch 72/750\n",
            "3315/3315 [==============================] - 1s 429us/step - loss: 1.0997 - acc: 0.5955 - val_loss: 1.0967 - val_acc: 0.6191\n",
            "Epoch 73/750\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 1.0930 - acc: 0.6072 - val_loss: 1.0886 - val_acc: 0.6111\n",
            "Epoch 74/750\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 1.1093 - acc: 0.5879 - val_loss: 1.0703 - val_acc: 0.6283\n",
            "Epoch 75/750\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 1.0947 - acc: 0.5997 - val_loss: 1.0945 - val_acc: 0.6050\n",
            "Epoch 76/750\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 1.0956 - acc: 0.6033 - val_loss: 1.0631 - val_acc: 0.6240\n",
            "Epoch 77/750\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 1.0781 - acc: 0.6069 - val_loss: 1.0787 - val_acc: 0.6203\n",
            "Epoch 78/750\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 1.0766 - acc: 0.6136 - val_loss: 1.1172 - val_acc: 0.5732\n",
            "Epoch 79/750\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 1.0807 - acc: 0.6042 - val_loss: 1.0685 - val_acc: 0.6271\n",
            "Epoch 80/750\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 1.0532 - acc: 0.6275 - val_loss: 1.0492 - val_acc: 0.6314\n",
            "Epoch 81/750\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 1.0606 - acc: 0.6211 - val_loss: 1.0427 - val_acc: 0.6234\n",
            "Epoch 82/750\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 1.0581 - acc: 0.6223 - val_loss: 1.0331 - val_acc: 0.6448\n",
            "Epoch 83/750\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 1.0438 - acc: 0.6259 - val_loss: 1.0522 - val_acc: 0.6160\n",
            "Epoch 84/750\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 1.0410 - acc: 0.6166 - val_loss: 1.0467 - val_acc: 0.6271\n",
            "Epoch 85/750\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 1.0410 - acc: 0.6302 - val_loss: 1.0294 - val_acc: 0.6430\n",
            "Epoch 86/750\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 1.0396 - acc: 0.6302 - val_loss: 1.0213 - val_acc: 0.6381\n",
            "Epoch 87/750\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 1.0341 - acc: 0.6223 - val_loss: 1.0276 - val_acc: 0.6252\n",
            "Epoch 88/750\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 1.0118 - acc: 0.6305 - val_loss: 1.0183 - val_acc: 0.6393\n",
            "Epoch 89/750\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 1.0168 - acc: 0.6368 - val_loss: 1.0079 - val_acc: 0.6381\n",
            "Epoch 90/750\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 1.0099 - acc: 0.6365 - val_loss: 1.0308 - val_acc: 0.6234\n",
            "Epoch 91/750\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 1.0114 - acc: 0.6362 - val_loss: 0.9990 - val_acc: 0.6424\n",
            "Epoch 92/750\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 1.0113 - acc: 0.6407 - val_loss: 0.9996 - val_acc: 0.6485\n",
            "Epoch 93/750\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.9933 - acc: 0.6510 - val_loss: 1.0086 - val_acc: 0.6418\n",
            "Epoch 94/750\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.9880 - acc: 0.6452 - val_loss: 1.0022 - val_acc: 0.6412\n",
            "Epoch 95/750\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 0.9840 - acc: 0.6486 - val_loss: 0.9695 - val_acc: 0.6479\n",
            "Epoch 96/750\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.9810 - acc: 0.6419 - val_loss: 0.9760 - val_acc: 0.6595\n",
            "Epoch 97/750\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.9903 - acc: 0.6440 - val_loss: 0.9951 - val_acc: 0.6424\n",
            "Epoch 98/750\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.9686 - acc: 0.6371 - val_loss: 0.9850 - val_acc: 0.6418\n",
            "Epoch 99/750\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.9751 - acc: 0.6428 - val_loss: 0.9671 - val_acc: 0.6565\n",
            "Epoch 100/750\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.9612 - acc: 0.6495 - val_loss: 0.9627 - val_acc: 0.6589\n",
            "Epoch 101/750\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.9705 - acc: 0.6501 - val_loss: 1.0404 - val_acc: 0.6148\n",
            "Epoch 102/750\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.9615 - acc: 0.6564 - val_loss: 0.9635 - val_acc: 0.6534\n",
            "Epoch 103/750\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.9524 - acc: 0.6549 - val_loss: 0.9668 - val_acc: 0.6516\n",
            "Epoch 104/750\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.9603 - acc: 0.6498 - val_loss: 0.9496 - val_acc: 0.6669\n",
            "Epoch 105/750\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.9551 - acc: 0.6519 - val_loss: 0.9682 - val_acc: 0.6607\n",
            "Epoch 106/750\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.9342 - acc: 0.6685 - val_loss: 0.9239 - val_acc: 0.6797\n",
            "Epoch 107/750\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.9358 - acc: 0.6670 - val_loss: 0.9478 - val_acc: 0.6601\n",
            "Epoch 108/750\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.9232 - acc: 0.6630 - val_loss: 0.9387 - val_acc: 0.6669\n",
            "Epoch 109/750\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 0.9323 - acc: 0.6609 - val_loss: 0.9280 - val_acc: 0.6724\n",
            "Epoch 110/750\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.9356 - acc: 0.6664 - val_loss: 0.9251 - val_acc: 0.6663\n",
            "Epoch 111/750\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.9362 - acc: 0.6567 - val_loss: 0.9540 - val_acc: 0.6601\n",
            "Epoch 112/750\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.9274 - acc: 0.6691 - val_loss: 0.9228 - val_acc: 0.6761\n",
            "Epoch 113/750\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.9205 - acc: 0.6603 - val_loss: 0.9172 - val_acc: 0.6724\n",
            "Epoch 114/750\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.9065 - acc: 0.6712 - val_loss: 0.9337 - val_acc: 0.6571\n",
            "Epoch 115/750\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.9046 - acc: 0.6745 - val_loss: 0.9048 - val_acc: 0.6785\n",
            "Epoch 116/750\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.9044 - acc: 0.6697 - val_loss: 0.9236 - val_acc: 0.6589\n",
            "Epoch 117/750\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.9046 - acc: 0.6697 - val_loss: 0.9227 - val_acc: 0.6687\n",
            "Epoch 118/750\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.8930 - acc: 0.6799 - val_loss: 0.9249 - val_acc: 0.6693\n",
            "Epoch 119/750\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 0.9069 - acc: 0.6709 - val_loss: 0.8906 - val_acc: 0.6846\n",
            "Epoch 120/750\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.8885 - acc: 0.6805 - val_loss: 0.9209 - val_acc: 0.6583\n",
            "Epoch 121/750\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.8966 - acc: 0.6793 - val_loss: 0.8986 - val_acc: 0.6877\n",
            "Epoch 122/750\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.8891 - acc: 0.6733 - val_loss: 0.8920 - val_acc: 0.6785\n",
            "Epoch 123/750\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.8876 - acc: 0.6827 - val_loss: 0.8884 - val_acc: 0.6828\n",
            "Epoch 124/750\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.8772 - acc: 0.6899 - val_loss: 0.8894 - val_acc: 0.6859\n",
            "Epoch 125/750\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.8756 - acc: 0.6811 - val_loss: 0.8849 - val_acc: 0.6840\n",
            "Epoch 126/750\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.8864 - acc: 0.6778 - val_loss: 0.8717 - val_acc: 0.6889\n",
            "Epoch 127/750\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.8692 - acc: 0.6920 - val_loss: 0.8919 - val_acc: 0.6699\n",
            "Epoch 128/750\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.8598 - acc: 0.6896 - val_loss: 0.8734 - val_acc: 0.6999\n",
            "Epoch 129/750\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 0.8518 - acc: 0.7002 - val_loss: 0.8987 - val_acc: 0.6736\n",
            "Epoch 130/750\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.8667 - acc: 0.6896 - val_loss: 0.8652 - val_acc: 0.6932\n",
            "Epoch 131/750\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.8462 - acc: 0.6911 - val_loss: 0.8823 - val_acc: 0.6975\n",
            "Epoch 132/750\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.8482 - acc: 0.6932 - val_loss: 0.8831 - val_acc: 0.6816\n",
            "Epoch 133/750\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.8461 - acc: 0.6851 - val_loss: 0.8595 - val_acc: 0.7091\n",
            "Epoch 134/750\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.8450 - acc: 0.6884 - val_loss: 0.8678 - val_acc: 0.6908\n",
            "Epoch 135/750\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.8376 - acc: 0.6920 - val_loss: 0.8657 - val_acc: 0.6840\n",
            "Epoch 136/750\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 0.8237 - acc: 0.7041 - val_loss: 0.8625 - val_acc: 0.6932\n",
            "Epoch 137/750\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.8356 - acc: 0.6947 - val_loss: 0.8536 - val_acc: 0.6950\n",
            "Epoch 138/750\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.8280 - acc: 0.6980 - val_loss: 0.8581 - val_acc: 0.7006\n",
            "Epoch 139/750\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.8254 - acc: 0.7041 - val_loss: 0.8538 - val_acc: 0.6975\n",
            "Epoch 140/750\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.8233 - acc: 0.6956 - val_loss: 0.8655 - val_acc: 0.6883\n",
            "Epoch 141/750\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.8293 - acc: 0.7005 - val_loss: 0.8497 - val_acc: 0.6871\n",
            "Epoch 142/750\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.8215 - acc: 0.7050 - val_loss: 0.8480 - val_acc: 0.6950\n",
            "Epoch 143/750\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.8130 - acc: 0.7116 - val_loss: 0.8659 - val_acc: 0.6871\n",
            "Epoch 144/750\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.8051 - acc: 0.7125 - val_loss: 0.8366 - val_acc: 0.6932\n",
            "Epoch 145/750\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.8133 - acc: 0.7041 - val_loss: 0.8330 - val_acc: 0.6987\n",
            "Epoch 146/750\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.7951 - acc: 0.7164 - val_loss: 0.8340 - val_acc: 0.6969\n",
            "Epoch 147/750\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 0.7945 - acc: 0.7074 - val_loss: 0.8397 - val_acc: 0.7055\n",
            "Epoch 148/750\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.8009 - acc: 0.7107 - val_loss: 0.8325 - val_acc: 0.7018\n",
            "Epoch 149/750\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.7976 - acc: 0.7146 - val_loss: 0.8249 - val_acc: 0.7116\n",
            "Epoch 150/750\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.7906 - acc: 0.7270 - val_loss: 0.8311 - val_acc: 0.7012\n",
            "Epoch 151/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.7961 - acc: 0.7131 - val_loss: 0.8385 - val_acc: 0.7012\n",
            "Epoch 152/750\n",
            "3315/3315 [==============================] - 2s 455us/step - loss: 0.7908 - acc: 0.7062 - val_loss: 0.8422 - val_acc: 0.6877\n",
            "Epoch 153/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.7821 - acc: 0.7195 - val_loss: 0.8433 - val_acc: 0.6950\n",
            "Epoch 154/750\n",
            "3315/3315 [==============================] - 2s 457us/step - loss: 0.7916 - acc: 0.7161 - val_loss: 0.8398 - val_acc: 0.6840\n",
            "Epoch 155/750\n",
            "3315/3315 [==============================] - 2s 456us/step - loss: 0.7846 - acc: 0.7134 - val_loss: 0.8234 - val_acc: 0.7006\n",
            "Epoch 156/750\n",
            "3315/3315 [==============================] - 2s 456us/step - loss: 0.7749 - acc: 0.7204 - val_loss: 0.8167 - val_acc: 0.7110\n",
            "Epoch 157/750\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.7746 - acc: 0.7237 - val_loss: 0.8859 - val_acc: 0.6693\n",
            "Epoch 158/750\n",
            "3315/3315 [==============================] - 2s 453us/step - loss: 0.7730 - acc: 0.7216 - val_loss: 0.8166 - val_acc: 0.7042\n",
            "Epoch 159/750\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.7727 - acc: 0.7246 - val_loss: 0.8244 - val_acc: 0.6981\n",
            "Epoch 160/750\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.7714 - acc: 0.7110 - val_loss: 0.8221 - val_acc: 0.7085\n",
            "Epoch 161/750\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.7724 - acc: 0.7222 - val_loss: 0.7994 - val_acc: 0.7103\n",
            "Epoch 162/750\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.7597 - acc: 0.7282 - val_loss: 0.7948 - val_acc: 0.7214\n",
            "Epoch 163/750\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.7488 - acc: 0.7360 - val_loss: 0.8025 - val_acc: 0.7159\n",
            "Epoch 164/750\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.7532 - acc: 0.7294 - val_loss: 0.8026 - val_acc: 0.7146\n",
            "Epoch 165/750\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.7585 - acc: 0.7273 - val_loss: 0.7997 - val_acc: 0.7110\n",
            "Epoch 166/750\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.7545 - acc: 0.7240 - val_loss: 0.7814 - val_acc: 0.7183\n",
            "Epoch 167/750\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.7403 - acc: 0.7291 - val_loss: 0.8209 - val_acc: 0.7036\n",
            "Epoch 168/750\n",
            "3315/3315 [==============================] - 1s 448us/step - loss: 0.7541 - acc: 0.7255 - val_loss: 0.7843 - val_acc: 0.7189\n",
            "Epoch 169/750\n",
            "3315/3315 [==============================] - 1s 448us/step - loss: 0.7384 - acc: 0.7288 - val_loss: 0.7884 - val_acc: 0.7201\n",
            "Epoch 170/750\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.7421 - acc: 0.7315 - val_loss: 0.7751 - val_acc: 0.7238\n",
            "Epoch 171/750\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.7375 - acc: 0.7324 - val_loss: 0.8136 - val_acc: 0.6944\n",
            "Epoch 172/750\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.7427 - acc: 0.7288 - val_loss: 0.7892 - val_acc: 0.7159\n",
            "Epoch 173/750\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 0.7351 - acc: 0.7388 - val_loss: 0.7885 - val_acc: 0.7110\n",
            "Epoch 174/750\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.7213 - acc: 0.7370 - val_loss: 0.7925 - val_acc: 0.7165\n",
            "Epoch 175/750\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.7258 - acc: 0.7397 - val_loss: 0.8032 - val_acc: 0.6987\n",
            "Epoch 176/750\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.7274 - acc: 0.7412 - val_loss: 0.7841 - val_acc: 0.7128\n",
            "Epoch 177/750\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.7240 - acc: 0.7324 - val_loss: 0.7894 - val_acc: 0.7103\n",
            "Epoch 178/750\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.7291 - acc: 0.7403 - val_loss: 0.7887 - val_acc: 0.7122\n",
            "Epoch 179/750\n",
            "3315/3315 [==============================] - 1s 435us/step - loss: 0.7285 - acc: 0.7345 - val_loss: 0.7550 - val_acc: 0.7275\n",
            "Epoch 180/750\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.7146 - acc: 0.7421 - val_loss: 0.7570 - val_acc: 0.7324\n",
            "Epoch 181/750\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.7139 - acc: 0.7418 - val_loss: 0.7752 - val_acc: 0.7257\n",
            "Epoch 182/750\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.7052 - acc: 0.7475 - val_loss: 0.7877 - val_acc: 0.7073\n",
            "Epoch 183/750\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.7009 - acc: 0.7502 - val_loss: 0.7675 - val_acc: 0.7318\n",
            "Epoch 184/750\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.7045 - acc: 0.7442 - val_loss: 0.7773 - val_acc: 0.7189\n",
            "Epoch 185/750\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.6857 - acc: 0.7490 - val_loss: 0.7717 - val_acc: 0.7128\n",
            "Epoch 186/750\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.6959 - acc: 0.7469 - val_loss: 0.7710 - val_acc: 0.7275\n",
            "Epoch 187/750\n",
            "3315/3315 [==============================] - 1s 446us/step - loss: 0.6927 - acc: 0.7505 - val_loss: 0.7760 - val_acc: 0.7220\n",
            "Epoch 188/750\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.6997 - acc: 0.7599 - val_loss: 0.7827 - val_acc: 0.7140\n",
            "Epoch 189/750\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.6870 - acc: 0.7511 - val_loss: 0.7577 - val_acc: 0.7208\n",
            "Epoch 190/750\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.6963 - acc: 0.7403 - val_loss: 0.7675 - val_acc: 0.7299\n",
            "Epoch 191/750\n",
            "3315/3315 [==============================] - 1s 446us/step - loss: 0.6990 - acc: 0.7457 - val_loss: 0.7415 - val_acc: 0.7373\n",
            "Epoch 192/750\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.6748 - acc: 0.7602 - val_loss: 0.7495 - val_acc: 0.7269\n",
            "Epoch 193/750\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.6815 - acc: 0.7647 - val_loss: 0.7509 - val_acc: 0.7348\n",
            "Epoch 194/750\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.6813 - acc: 0.7511 - val_loss: 0.7414 - val_acc: 0.7318\n",
            "Epoch 195/750\n",
            "3315/3315 [==============================] - 1s 446us/step - loss: 0.6766 - acc: 0.7566 - val_loss: 0.7384 - val_acc: 0.7330\n",
            "Epoch 196/750\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.6907 - acc: 0.7514 - val_loss: 0.7367 - val_acc: 0.7355\n",
            "Epoch 197/750\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.6856 - acc: 0.7544 - val_loss: 0.7450 - val_acc: 0.7257\n",
            "Epoch 198/750\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.6726 - acc: 0.7535 - val_loss: 0.7285 - val_acc: 0.7440\n",
            "Epoch 199/750\n",
            "3315/3315 [==============================] - 1s 449us/step - loss: 0.6725 - acc: 0.7638 - val_loss: 0.7206 - val_acc: 0.7453\n",
            "Epoch 200/750\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.6742 - acc: 0.7593 - val_loss: 0.7335 - val_acc: 0.7330\n",
            "Epoch 201/750\n",
            "3315/3315 [==============================] - 1s 448us/step - loss: 0.6729 - acc: 0.7520 - val_loss: 0.7311 - val_acc: 0.7342\n",
            "Epoch 202/750\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.6698 - acc: 0.7650 - val_loss: 0.7118 - val_acc: 0.7489\n",
            "Epoch 203/750\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.6539 - acc: 0.7710 - val_loss: 0.7559 - val_acc: 0.7220\n",
            "Epoch 204/750\n",
            "3315/3315 [==============================] - 1s 449us/step - loss: 0.6576 - acc: 0.7520 - val_loss: 0.7325 - val_acc: 0.7348\n",
            "Epoch 205/750\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.6616 - acc: 0.7662 - val_loss: 0.7194 - val_acc: 0.7397\n",
            "Epoch 206/750\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.6637 - acc: 0.7602 - val_loss: 0.7149 - val_acc: 0.7514\n",
            "Epoch 207/750\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.6474 - acc: 0.7698 - val_loss: 0.7220 - val_acc: 0.7422\n",
            "Epoch 208/750\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.6522 - acc: 0.7641 - val_loss: 0.7228 - val_acc: 0.7373\n",
            "Epoch 209/750\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.6478 - acc: 0.7608 - val_loss: 0.7144 - val_acc: 0.7428\n",
            "Epoch 210/750\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.6476 - acc: 0.7774 - val_loss: 0.7133 - val_acc: 0.7404\n",
            "Epoch 211/750\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.6299 - acc: 0.7762 - val_loss: 0.7328 - val_acc: 0.7330\n",
            "Epoch 212/750\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.6491 - acc: 0.7653 - val_loss: 0.7216 - val_acc: 0.7532\n",
            "Epoch 213/750\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.6377 - acc: 0.7692 - val_loss: 0.7099 - val_acc: 0.7538\n",
            "Epoch 214/750\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.6441 - acc: 0.7695 - val_loss: 0.7126 - val_acc: 0.7391\n",
            "Epoch 215/750\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.6346 - acc: 0.7701 - val_loss: 0.7109 - val_acc: 0.7495\n",
            "Epoch 216/750\n",
            "3315/3315 [==============================] - 1s 446us/step - loss: 0.6350 - acc: 0.7735 - val_loss: 0.7140 - val_acc: 0.7391\n",
            "Epoch 217/750\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.6396 - acc: 0.7744 - val_loss: 0.6936 - val_acc: 0.7538\n",
            "Epoch 218/750\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.6328 - acc: 0.7707 - val_loss: 0.7066 - val_acc: 0.7483\n",
            "Epoch 219/750\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.6153 - acc: 0.7843 - val_loss: 0.6894 - val_acc: 0.7642\n",
            "Epoch 220/750\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.6169 - acc: 0.7759 - val_loss: 0.7011 - val_acc: 0.7489\n",
            "Epoch 221/750\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.6426 - acc: 0.7662 - val_loss: 0.6867 - val_acc: 0.7636\n",
            "Epoch 222/750\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.6263 - acc: 0.7738 - val_loss: 0.7070 - val_acc: 0.7446\n",
            "Epoch 223/750\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.6292 - acc: 0.7807 - val_loss: 0.6935 - val_acc: 0.7434\n",
            "Epoch 224/750\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.6203 - acc: 0.7753 - val_loss: 0.7060 - val_acc: 0.7538\n",
            "Epoch 225/750\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.6137 - acc: 0.7840 - val_loss: 0.6884 - val_acc: 0.7593\n",
            "Epoch 226/750\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.6189 - acc: 0.7804 - val_loss: 0.6992 - val_acc: 0.7544\n",
            "Epoch 227/750\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.6121 - acc: 0.7816 - val_loss: 0.7101 - val_acc: 0.7434\n",
            "Epoch 228/750\n",
            "3315/3315 [==============================] - 1s 448us/step - loss: 0.6068 - acc: 0.7846 - val_loss: 0.6923 - val_acc: 0.7459\n",
            "Epoch 229/750\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.5984 - acc: 0.7961 - val_loss: 0.6818 - val_acc: 0.7587\n",
            "Epoch 230/750\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.6080 - acc: 0.7882 - val_loss: 0.6735 - val_acc: 0.7612\n",
            "Epoch 231/750\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.6139 - acc: 0.7867 - val_loss: 0.6825 - val_acc: 0.7538\n",
            "Epoch 232/750\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.6026 - acc: 0.7940 - val_loss: 0.6938 - val_acc: 0.7483\n",
            "Epoch 233/750\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.6144 - acc: 0.7807 - val_loss: 0.6708 - val_acc: 0.7587\n",
            "Epoch 234/750\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.6061 - acc: 0.7913 - val_loss: 0.6939 - val_acc: 0.7477\n",
            "Epoch 235/750\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.5963 - acc: 0.7819 - val_loss: 0.6948 - val_acc: 0.7569\n",
            "Epoch 236/750\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.5991 - acc: 0.7864 - val_loss: 0.6801 - val_acc: 0.7740\n",
            "Epoch 237/750\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.5908 - acc: 0.7937 - val_loss: 0.6877 - val_acc: 0.7483\n",
            "Epoch 238/750\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.6000 - acc: 0.7765 - val_loss: 0.6854 - val_acc: 0.7557\n",
            "Epoch 239/750\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.5779 - acc: 0.8006 - val_loss: 0.6781 - val_acc: 0.7526\n",
            "Epoch 240/750\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.5809 - acc: 0.7897 - val_loss: 0.6558 - val_acc: 0.7655\n",
            "Epoch 241/750\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.5933 - acc: 0.7900 - val_loss: 0.6737 - val_acc: 0.7618\n",
            "Epoch 242/750\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.5838 - acc: 0.7900 - val_loss: 0.6614 - val_acc: 0.7624\n",
            "Epoch 243/750\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.5692 - acc: 0.7922 - val_loss: 0.6593 - val_acc: 0.7728\n",
            "Epoch 244/750\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.5858 - acc: 0.7897 - val_loss: 0.6482 - val_acc: 0.7716\n",
            "Epoch 245/750\n",
            "3315/3315 [==============================] - 1s 431us/step - loss: 0.5827 - acc: 0.7900 - val_loss: 0.6589 - val_acc: 0.7716\n",
            "Epoch 246/750\n",
            "3315/3315 [==============================] - 1s 432us/step - loss: 0.5717 - acc: 0.7991 - val_loss: 0.6531 - val_acc: 0.7820\n",
            "Epoch 247/750\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.5792 - acc: 0.8003 - val_loss: 0.6753 - val_acc: 0.7551\n",
            "Epoch 248/750\n",
            "3315/3315 [==============================] - 1s 439us/step - loss: 0.5746 - acc: 0.8024 - val_loss: 0.6594 - val_acc: 0.7728\n",
            "Epoch 249/750\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.5651 - acc: 0.8003 - val_loss: 0.6572 - val_acc: 0.7636\n",
            "Epoch 250/750\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.5653 - acc: 0.7997 - val_loss: 0.6814 - val_acc: 0.7465\n",
            "Epoch 251/750\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.5743 - acc: 0.7973 - val_loss: 0.6858 - val_acc: 0.7514\n",
            "Epoch 252/750\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.5682 - acc: 0.7946 - val_loss: 0.6509 - val_acc: 0.7765\n",
            "Epoch 253/750\n",
            "3315/3315 [==============================] - 1s 446us/step - loss: 0.5827 - acc: 0.7988 - val_loss: 0.6983 - val_acc: 0.7440\n",
            "Epoch 254/750\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.5827 - acc: 0.7970 - val_loss: 0.6482 - val_acc: 0.7789\n",
            "Epoch 255/750\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.5671 - acc: 0.7976 - val_loss: 0.6403 - val_acc: 0.7661\n",
            "Epoch 256/750\n",
            "3315/3315 [==============================] - 1s 446us/step - loss: 0.5551 - acc: 0.8015 - val_loss: 0.6443 - val_acc: 0.7630\n",
            "Epoch 257/750\n",
            "3315/3315 [==============================] - 1s 444us/step - loss: 0.5506 - acc: 0.8048 - val_loss: 0.6705 - val_acc: 0.7630\n",
            "Epoch 258/750\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.5681 - acc: 0.8015 - val_loss: 0.6502 - val_acc: 0.7679\n",
            "Epoch 259/750\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.5655 - acc: 0.7967 - val_loss: 0.6477 - val_acc: 0.7746\n",
            "Epoch 260/750\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.5545 - acc: 0.7997 - val_loss: 0.6508 - val_acc: 0.7685\n",
            "Epoch 261/750\n",
            "3315/3315 [==============================] - 1s 436us/step - loss: 0.5578 - acc: 0.8018 - val_loss: 0.6471 - val_acc: 0.7642\n",
            "Epoch 262/750\n",
            "3315/3315 [==============================] - 1s 434us/step - loss: 0.5435 - acc: 0.8057 - val_loss: 0.6346 - val_acc: 0.7753\n",
            "Epoch 263/750\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.5521 - acc: 0.8042 - val_loss: 0.6378 - val_acc: 0.7826\n",
            "Epoch 264/750\n",
            "3315/3315 [==============================] - 1s 446us/step - loss: 0.5433 - acc: 0.8100 - val_loss: 0.6319 - val_acc: 0.7716\n",
            "Epoch 265/750\n",
            "3315/3315 [==============================] - 1s 443us/step - loss: 0.5464 - acc: 0.8090 - val_loss: 0.6140 - val_acc: 0.7826\n",
            "Epoch 266/750\n",
            "3315/3315 [==============================] - 2s 481us/step - loss: 0.5391 - acc: 0.8106 - val_loss: 0.6744 - val_acc: 0.7544\n",
            "Epoch 267/750\n",
            "3315/3315 [==============================] - 3s 861us/step - loss: 0.5359 - acc: 0.8060 - val_loss: 0.6264 - val_acc: 0.7851\n",
            "Epoch 268/750\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.5456 - acc: 0.8087 - val_loss: 0.6257 - val_acc: 0.7722\n",
            "Epoch 269/750\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.5383 - acc: 0.8006 - val_loss: 0.6464 - val_acc: 0.7740\n",
            "Epoch 270/750\n",
            "3315/3315 [==============================] - 1s 452us/step - loss: 0.5369 - acc: 0.8078 - val_loss: 0.6334 - val_acc: 0.7808\n",
            "Epoch 271/750\n",
            "3315/3315 [==============================] - 2s 454us/step - loss: 0.5274 - acc: 0.8124 - val_loss: 0.6295 - val_acc: 0.7777\n",
            "Epoch 272/750\n",
            "3315/3315 [==============================] - 2s 455us/step - loss: 0.5260 - acc: 0.8139 - val_loss: 0.6416 - val_acc: 0.7673\n",
            "Epoch 273/750\n",
            "3315/3315 [==============================] - 2s 453us/step - loss: 0.5178 - acc: 0.8214 - val_loss: 0.6286 - val_acc: 0.7753\n",
            "Epoch 274/750\n",
            "3315/3315 [==============================] - 2s 455us/step - loss: 0.5339 - acc: 0.8142 - val_loss: 0.6109 - val_acc: 0.7906\n",
            "Epoch 275/750\n",
            "3315/3315 [==============================] - 1s 450us/step - loss: 0.5064 - acc: 0.8238 - val_loss: 0.6191 - val_acc: 0.7844\n",
            "Epoch 276/750\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.5257 - acc: 0.8136 - val_loss: 0.6425 - val_acc: 0.7538\n",
            "Epoch 277/750\n",
            "3315/3315 [==============================] - 2s 453us/step - loss: 0.5187 - acc: 0.8181 - val_loss: 0.6245 - val_acc: 0.7875\n",
            "Epoch 278/750\n",
            "3315/3315 [==============================] - 2s 453us/step - loss: 0.5342 - acc: 0.8100 - val_loss: 0.6371 - val_acc: 0.7710\n",
            "Epoch 279/750\n",
            "3315/3315 [==============================] - 2s 454us/step - loss: 0.5190 - acc: 0.8169 - val_loss: 0.6228 - val_acc: 0.7838\n",
            "Epoch 280/750\n",
            "3315/3315 [==============================] - 1s 452us/step - loss: 0.5165 - acc: 0.8157 - val_loss: 0.6253 - val_acc: 0.7783\n",
            "Epoch 281/750\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.5165 - acc: 0.8229 - val_loss: 0.6260 - val_acc: 0.7777\n",
            "Epoch 282/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.5244 - acc: 0.8081 - val_loss: 0.6011 - val_acc: 0.7985\n",
            "Epoch 283/750\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.5023 - acc: 0.8217 - val_loss: 0.6297 - val_acc: 0.7777\n",
            "Epoch 284/750\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.5209 - acc: 0.8196 - val_loss: 0.6142 - val_acc: 0.7777\n",
            "Epoch 285/750\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.5089 - acc: 0.8163 - val_loss: 0.6098 - val_acc: 0.7887\n",
            "Epoch 286/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.5031 - acc: 0.8214 - val_loss: 0.5998 - val_acc: 0.7826\n",
            "Epoch 287/750\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.4998 - acc: 0.8220 - val_loss: 0.5890 - val_acc: 0.7918\n",
            "Epoch 288/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.5062 - acc: 0.8184 - val_loss: 0.6013 - val_acc: 0.7967\n",
            "Epoch 289/750\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.5176 - acc: 0.8175 - val_loss: 0.6021 - val_acc: 0.7887\n",
            "Epoch 290/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.4931 - acc: 0.8241 - val_loss: 0.5984 - val_acc: 0.7967\n",
            "Epoch 291/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.5065 - acc: 0.8103 - val_loss: 0.6026 - val_acc: 0.7887\n",
            "Epoch 292/750\n",
            "3315/3315 [==============================] - 2s 455us/step - loss: 0.5084 - acc: 0.8187 - val_loss: 0.6346 - val_acc: 0.7851\n",
            "Epoch 293/750\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.4955 - acc: 0.8323 - val_loss: 0.6064 - val_acc: 0.7924\n",
            "Epoch 294/750\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.5050 - acc: 0.8205 - val_loss: 0.5957 - val_acc: 0.7918\n",
            "Epoch 295/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.4909 - acc: 0.8235 - val_loss: 0.6098 - val_acc: 0.7973\n",
            "Epoch 296/750\n",
            "3315/3315 [==============================] - 2s 456us/step - loss: 0.5038 - acc: 0.8250 - val_loss: 0.6249 - val_acc: 0.7704\n",
            "Epoch 297/750\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.4987 - acc: 0.8229 - val_loss: 0.5989 - val_acc: 0.8016\n",
            "Epoch 298/750\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.4905 - acc: 0.8281 - val_loss: 0.5805 - val_acc: 0.8016\n",
            "Epoch 299/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.4908 - acc: 0.8235 - val_loss: 0.5873 - val_acc: 0.7955\n",
            "Epoch 300/750\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.4748 - acc: 0.8359 - val_loss: 0.5959 - val_acc: 0.7949\n",
            "Epoch 301/750\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.4935 - acc: 0.8290 - val_loss: 0.5756 - val_acc: 0.8089\n",
            "Epoch 302/750\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.4798 - acc: 0.8365 - val_loss: 0.5712 - val_acc: 0.8022\n",
            "Epoch 303/750\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.4629 - acc: 0.8323 - val_loss: 0.5844 - val_acc: 0.7906\n",
            "Epoch 304/750\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.5001 - acc: 0.8178 - val_loss: 0.5708 - val_acc: 0.7998\n",
            "Epoch 305/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.4713 - acc: 0.8413 - val_loss: 0.6136 - val_acc: 0.7924\n",
            "Epoch 306/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.4697 - acc: 0.8377 - val_loss: 0.5661 - val_acc: 0.8034\n",
            "Epoch 307/750\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.4837 - acc: 0.8284 - val_loss: 0.6037 - val_acc: 0.7759\n",
            "Epoch 308/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.4772 - acc: 0.8317 - val_loss: 0.5770 - val_acc: 0.7998\n",
            "Epoch 309/750\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.4763 - acc: 0.8250 - val_loss: 0.5926 - val_acc: 0.7998\n",
            "Epoch 310/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.4788 - acc: 0.8320 - val_loss: 0.5761 - val_acc: 0.8040\n",
            "Epoch 311/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.4627 - acc: 0.8374 - val_loss: 0.5779 - val_acc: 0.8034\n",
            "Epoch 312/750\n",
            "3315/3315 [==============================] - 1s 451us/step - loss: 0.4704 - acc: 0.8425 - val_loss: 0.5992 - val_acc: 0.7949\n",
            "Epoch 313/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.4605 - acc: 0.8419 - val_loss: 0.5829 - val_acc: 0.7955\n",
            "Epoch 314/750\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.4715 - acc: 0.8365 - val_loss: 0.5824 - val_acc: 0.7942\n",
            "Epoch 315/750\n",
            "3315/3315 [==============================] - 1s 452us/step - loss: 0.4844 - acc: 0.8389 - val_loss: 0.6080 - val_acc: 0.7936\n",
            "Epoch 316/750\n",
            "3315/3315 [==============================] - 2s 453us/step - loss: 0.4625 - acc: 0.8401 - val_loss: 0.5926 - val_acc: 0.7955\n",
            "Epoch 317/750\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.4732 - acc: 0.8326 - val_loss: 0.5829 - val_acc: 0.7979\n",
            "Epoch 318/750\n",
            "3315/3315 [==============================] - 2s 456us/step - loss: 0.4586 - acc: 0.8395 - val_loss: 0.5705 - val_acc: 0.7973\n",
            "Epoch 319/750\n",
            "3315/3315 [==============================] - 1s 452us/step - loss: 0.4651 - acc: 0.8389 - val_loss: 0.5630 - val_acc: 0.8065\n",
            "Epoch 320/750\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.4656 - acc: 0.8443 - val_loss: 0.5712 - val_acc: 0.7985\n",
            "Epoch 321/750\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.4653 - acc: 0.8437 - val_loss: 0.5748 - val_acc: 0.8047\n",
            "Epoch 322/750\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.4438 - acc: 0.8446 - val_loss: 0.5564 - val_acc: 0.8169\n",
            "Epoch 323/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.4549 - acc: 0.8389 - val_loss: 0.5673 - val_acc: 0.8120\n",
            "Epoch 324/750\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.4578 - acc: 0.8335 - val_loss: 0.5589 - val_acc: 0.8071\n",
            "Epoch 325/750\n",
            "3315/3315 [==============================] - 2s 456us/step - loss: 0.4538 - acc: 0.8404 - val_loss: 0.5689 - val_acc: 0.8096\n",
            "Epoch 326/750\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.4540 - acc: 0.8383 - val_loss: 0.5642 - val_acc: 0.8108\n",
            "Epoch 327/750\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.4389 - acc: 0.8492 - val_loss: 0.5592 - val_acc: 0.8047\n",
            "Epoch 328/750\n",
            "3315/3315 [==============================] - 2s 455us/step - loss: 0.4496 - acc: 0.8452 - val_loss: 0.5835 - val_acc: 0.7991\n",
            "Epoch 329/750\n",
            "3315/3315 [==============================] - 2s 457us/step - loss: 0.4402 - acc: 0.8474 - val_loss: 0.5440 - val_acc: 0.8224\n",
            "Epoch 330/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.4481 - acc: 0.8474 - val_loss: 0.5861 - val_acc: 0.7998\n",
            "Epoch 331/750\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.4435 - acc: 0.8471 - val_loss: 0.5651 - val_acc: 0.8089\n",
            "Epoch 332/750\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.4414 - acc: 0.8386 - val_loss: 0.5529 - val_acc: 0.8163\n",
            "Epoch 333/750\n",
            "3315/3315 [==============================] - 1s 451us/step - loss: 0.4313 - acc: 0.8525 - val_loss: 0.5564 - val_acc: 0.8102\n",
            "Epoch 334/750\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.4423 - acc: 0.8422 - val_loss: 0.5729 - val_acc: 0.8108\n",
            "Epoch 335/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.4316 - acc: 0.8440 - val_loss: 0.5511 - val_acc: 0.8108\n",
            "Epoch 336/750\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.4345 - acc: 0.8459 - val_loss: 0.5811 - val_acc: 0.7955\n",
            "Epoch 337/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.4428 - acc: 0.8486 - val_loss: 0.5605 - val_acc: 0.8157\n",
            "Epoch 338/750\n",
            "3315/3315 [==============================] - 2s 453us/step - loss: 0.4442 - acc: 0.8480 - val_loss: 0.5467 - val_acc: 0.8132\n",
            "Epoch 339/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.4373 - acc: 0.8410 - val_loss: 0.5695 - val_acc: 0.8016\n",
            "Epoch 340/750\n",
            "3315/3315 [==============================] - 2s 457us/step - loss: 0.4398 - acc: 0.8468 - val_loss: 0.5441 - val_acc: 0.8132\n",
            "Epoch 341/750\n",
            "3315/3315 [==============================] - 2s 453us/step - loss: 0.4269 - acc: 0.8555 - val_loss: 0.5635 - val_acc: 0.8059\n",
            "Epoch 342/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.4356 - acc: 0.8510 - val_loss: 0.5477 - val_acc: 0.8206\n",
            "Epoch 343/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.4279 - acc: 0.8531 - val_loss: 0.5501 - val_acc: 0.8169\n",
            "Epoch 344/750\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.4286 - acc: 0.8501 - val_loss: 0.5600 - val_acc: 0.8120\n",
            "Epoch 345/750\n",
            "3315/3315 [==============================] - 2s 454us/step - loss: 0.4360 - acc: 0.8389 - val_loss: 0.5548 - val_acc: 0.8096\n",
            "Epoch 346/750\n",
            "3315/3315 [==============================] - 2s 454us/step - loss: 0.4319 - acc: 0.8531 - val_loss: 0.5411 - val_acc: 0.8218\n",
            "Epoch 347/750\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.4170 - acc: 0.8561 - val_loss: 0.5353 - val_acc: 0.8261\n",
            "Epoch 348/750\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.4266 - acc: 0.8489 - val_loss: 0.5331 - val_acc: 0.8224\n",
            "Epoch 349/750\n",
            "3315/3315 [==============================] - 1s 451us/step - loss: 0.4277 - acc: 0.8504 - val_loss: 0.5433 - val_acc: 0.8230\n",
            "Epoch 350/750\n",
            "3315/3315 [==============================] - 2s 455us/step - loss: 0.4169 - acc: 0.8507 - val_loss: 0.5353 - val_acc: 0.8145\n",
            "Epoch 351/750\n",
            "3315/3315 [==============================] - 2s 457us/step - loss: 0.4184 - acc: 0.8525 - val_loss: 0.5636 - val_acc: 0.8145\n",
            "Epoch 352/750\n",
            "3315/3315 [==============================] - 2s 454us/step - loss: 0.4190 - acc: 0.8585 - val_loss: 0.5415 - val_acc: 0.8230\n",
            "Epoch 353/750\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.4220 - acc: 0.8477 - val_loss: 0.5340 - val_acc: 0.8102\n",
            "Epoch 354/750\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.4227 - acc: 0.8576 - val_loss: 0.5342 - val_acc: 0.8273\n",
            "Epoch 355/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.4146 - acc: 0.8561 - val_loss: 0.5343 - val_acc: 0.8273\n",
            "Epoch 356/750\n",
            "3315/3315 [==============================] - 1s 451us/step - loss: 0.4180 - acc: 0.8549 - val_loss: 0.5637 - val_acc: 0.8040\n",
            "Epoch 357/750\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.4330 - acc: 0.8495 - val_loss: 0.5402 - val_acc: 0.8181\n",
            "Epoch 358/750\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.4278 - acc: 0.8543 - val_loss: 0.5190 - val_acc: 0.8261\n",
            "Epoch 359/750\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.4106 - acc: 0.8567 - val_loss: 0.5320 - val_acc: 0.8255\n",
            "Epoch 360/750\n",
            "3315/3315 [==============================] - 2s 490us/step - loss: 0.4120 - acc: 0.8597 - val_loss: 0.5418 - val_acc: 0.8102\n",
            "Epoch 361/750\n",
            "3315/3315 [==============================] - 2s 501us/step - loss: 0.4097 - acc: 0.8585 - val_loss: 0.5240 - val_acc: 0.8218\n",
            "Epoch 362/750\n",
            "3315/3315 [==============================] - 2s 489us/step - loss: 0.4112 - acc: 0.8558 - val_loss: 0.5391 - val_acc: 0.8242\n",
            "Epoch 363/750\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.4091 - acc: 0.8528 - val_loss: 0.5221 - val_acc: 0.8261\n",
            "Epoch 364/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.4229 - acc: 0.8516 - val_loss: 0.5406 - val_acc: 0.8096\n",
            "Epoch 365/750\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.4147 - acc: 0.8570 - val_loss: 0.5249 - val_acc: 0.8181\n",
            "Epoch 366/750\n",
            "3315/3315 [==============================] - 2s 457us/step - loss: 0.3984 - acc: 0.8612 - val_loss: 0.5447 - val_acc: 0.8089\n",
            "Epoch 367/750\n",
            "3315/3315 [==============================] - 2s 456us/step - loss: 0.3963 - acc: 0.8640 - val_loss: 0.5184 - val_acc: 0.8310\n",
            "Epoch 368/750\n",
            "3315/3315 [==============================] - 2s 456us/step - loss: 0.4133 - acc: 0.8606 - val_loss: 0.5271 - val_acc: 0.8230\n",
            "Epoch 369/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.4053 - acc: 0.8646 - val_loss: 0.5612 - val_acc: 0.7961\n",
            "Epoch 370/750\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.3945 - acc: 0.8546 - val_loss: 0.5326 - val_acc: 0.8187\n",
            "Epoch 371/750\n",
            "3315/3315 [==============================] - 2s 457us/step - loss: 0.3982 - acc: 0.8643 - val_loss: 0.5021 - val_acc: 0.8322\n",
            "Epoch 372/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.4047 - acc: 0.8627 - val_loss: 0.5005 - val_acc: 0.8340\n",
            "Epoch 373/750\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.4012 - acc: 0.8594 - val_loss: 0.5079 - val_acc: 0.8340\n",
            "Epoch 374/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.4000 - acc: 0.8567 - val_loss: 0.5105 - val_acc: 0.8291\n",
            "Epoch 375/750\n",
            "3315/3315 [==============================] - 2s 457us/step - loss: 0.3966 - acc: 0.8615 - val_loss: 0.5224 - val_acc: 0.8212\n",
            "Epoch 376/750\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.3961 - acc: 0.8615 - val_loss: 0.5187 - val_acc: 0.8334\n",
            "Epoch 377/750\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.3831 - acc: 0.8667 - val_loss: 0.5088 - val_acc: 0.8304\n",
            "Epoch 378/750\n",
            "3315/3315 [==============================] - 2s 456us/step - loss: 0.4049 - acc: 0.8510 - val_loss: 0.5361 - val_acc: 0.8151\n",
            "Epoch 379/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.3848 - acc: 0.8633 - val_loss: 0.5095 - val_acc: 0.8359\n",
            "Epoch 380/750\n",
            "3315/3315 [==============================] - 2s 456us/step - loss: 0.3981 - acc: 0.8646 - val_loss: 0.5336 - val_acc: 0.8200\n",
            "Epoch 381/750\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.4015 - acc: 0.8591 - val_loss: 0.5195 - val_acc: 0.8194\n",
            "Epoch 382/750\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.4018 - acc: 0.8600 - val_loss: 0.5088 - val_acc: 0.8261\n",
            "Epoch 383/750\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.3899 - acc: 0.8633 - val_loss: 0.5101 - val_acc: 0.8255\n",
            "Epoch 384/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.3764 - acc: 0.8694 - val_loss: 0.4968 - val_acc: 0.8371\n",
            "Epoch 385/750\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.3896 - acc: 0.8646 - val_loss: 0.5296 - val_acc: 0.8151\n",
            "Epoch 386/750\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.3973 - acc: 0.8667 - val_loss: 0.5087 - val_acc: 0.8347\n",
            "Epoch 387/750\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.3978 - acc: 0.8640 - val_loss: 0.5016 - val_acc: 0.8322\n",
            "Epoch 388/750\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.3948 - acc: 0.8640 - val_loss: 0.5050 - val_acc: 0.8347\n",
            "Epoch 389/750\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.3850 - acc: 0.8609 - val_loss: 0.4893 - val_acc: 0.8383\n",
            "Epoch 390/750\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.3978 - acc: 0.8640 - val_loss: 0.4985 - val_acc: 0.8377\n",
            "Epoch 391/750\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.3870 - acc: 0.8688 - val_loss: 0.4974 - val_acc: 0.8402\n",
            "Epoch 392/750\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.3810 - acc: 0.8676 - val_loss: 0.4944 - val_acc: 0.8328\n",
            "Epoch 393/750\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.3729 - acc: 0.8670 - val_loss: 0.4933 - val_acc: 0.8322\n",
            "Epoch 394/750\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.3744 - acc: 0.8676 - val_loss: 0.5021 - val_acc: 0.8316\n",
            "Epoch 395/750\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.3754 - acc: 0.8694 - val_loss: 0.5058 - val_acc: 0.8310\n",
            "Epoch 396/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.3673 - acc: 0.8769 - val_loss: 0.4894 - val_acc: 0.8334\n",
            "Epoch 397/750\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.3654 - acc: 0.8793 - val_loss: 0.4972 - val_acc: 0.8298\n",
            "Epoch 398/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.3825 - acc: 0.8712 - val_loss: 0.5118 - val_acc: 0.8304\n",
            "Epoch 399/750\n",
            "3315/3315 [==============================] - 2s 457us/step - loss: 0.3669 - acc: 0.8769 - val_loss: 0.4974 - val_acc: 0.8304\n",
            "Epoch 400/750\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.3698 - acc: 0.8691 - val_loss: 0.5042 - val_acc: 0.8291\n",
            "Epoch 401/750\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.3843 - acc: 0.8637 - val_loss: 0.5123 - val_acc: 0.8273\n",
            "Epoch 402/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.3717 - acc: 0.8751 - val_loss: 0.4771 - val_acc: 0.8543\n",
            "Epoch 403/750\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.3719 - acc: 0.8715 - val_loss: 0.5470 - val_acc: 0.8120\n",
            "Epoch 404/750\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.3601 - acc: 0.8784 - val_loss: 0.4967 - val_acc: 0.8347\n",
            "Epoch 405/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.3691 - acc: 0.8697 - val_loss: 0.4980 - val_acc: 0.8371\n",
            "Epoch 406/750\n",
            "3315/3315 [==============================] - 2s 457us/step - loss: 0.3645 - acc: 0.8775 - val_loss: 0.4760 - val_acc: 0.8432\n",
            "Epoch 407/750\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.3568 - acc: 0.8793 - val_loss: 0.4919 - val_acc: 0.8304\n",
            "Epoch 408/750\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.3607 - acc: 0.8748 - val_loss: 0.4883 - val_acc: 0.8414\n",
            "Epoch 409/750\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.3533 - acc: 0.8775 - val_loss: 0.4795 - val_acc: 0.8451\n",
            "Epoch 410/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.3669 - acc: 0.8724 - val_loss: 0.4887 - val_acc: 0.8383\n",
            "Epoch 411/750\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.3668 - acc: 0.8724 - val_loss: 0.4868 - val_acc: 0.8451\n",
            "Epoch 412/750\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.3728 - acc: 0.8679 - val_loss: 0.5121 - val_acc: 0.8267\n",
            "Epoch 413/750\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.3570 - acc: 0.8805 - val_loss: 0.4982 - val_acc: 0.8347\n",
            "Epoch 414/750\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.3691 - acc: 0.8742 - val_loss: 0.4992 - val_acc: 0.8377\n",
            "Epoch 415/750\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.3580 - acc: 0.8778 - val_loss: 0.5234 - val_acc: 0.8114\n",
            "Epoch 416/750\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.3473 - acc: 0.8875 - val_loss: 0.5008 - val_acc: 0.8322\n",
            "Epoch 417/750\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.3561 - acc: 0.8781 - val_loss: 0.4803 - val_acc: 0.8438\n",
            "Epoch 418/750\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.3712 - acc: 0.8694 - val_loss: 0.4826 - val_acc: 0.8420\n",
            "Epoch 419/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.3533 - acc: 0.8763 - val_loss: 0.4655 - val_acc: 0.8451\n",
            "Epoch 420/750\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.3514 - acc: 0.8860 - val_loss: 0.5150 - val_acc: 0.8255\n",
            "Epoch 421/750\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.3441 - acc: 0.8766 - val_loss: 0.4858 - val_acc: 0.8396\n",
            "Epoch 422/750\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.3498 - acc: 0.8757 - val_loss: 0.4801 - val_acc: 0.8402\n",
            "Epoch 423/750\n",
            "3315/3315 [==============================] - 2s 457us/step - loss: 0.3547 - acc: 0.8757 - val_loss: 0.4657 - val_acc: 0.8469\n",
            "Epoch 424/750\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.3451 - acc: 0.8824 - val_loss: 0.4876 - val_acc: 0.8371\n",
            "Epoch 425/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.3604 - acc: 0.8733 - val_loss: 0.4789 - val_acc: 0.8451\n",
            "Epoch 426/750\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.3374 - acc: 0.8851 - val_loss: 0.4771 - val_acc: 0.8371\n",
            "Epoch 427/750\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.3416 - acc: 0.8796 - val_loss: 0.5082 - val_acc: 0.8328\n",
            "Epoch 428/750\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.3420 - acc: 0.8830 - val_loss: 0.4858 - val_acc: 0.8426\n",
            "Epoch 429/750\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.3391 - acc: 0.8827 - val_loss: 0.4688 - val_acc: 0.8530\n",
            "Epoch 430/750\n",
            "3315/3315 [==============================] - 2s 455us/step - loss: 0.3277 - acc: 0.8875 - val_loss: 0.4897 - val_acc: 0.8408\n",
            "Epoch 431/750\n",
            "3315/3315 [==============================] - 1s 452us/step - loss: 0.3347 - acc: 0.8884 - val_loss: 0.4748 - val_acc: 0.8371\n",
            "Epoch 432/750\n",
            "3315/3315 [==============================] - 2s 454us/step - loss: 0.3327 - acc: 0.8866 - val_loss: 0.4806 - val_acc: 0.8402\n",
            "Epoch 433/750\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.3314 - acc: 0.8905 - val_loss: 0.4612 - val_acc: 0.8518\n",
            "Epoch 434/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.3295 - acc: 0.8814 - val_loss: 0.4915 - val_acc: 0.8353\n",
            "Epoch 435/750\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.3434 - acc: 0.8817 - val_loss: 0.4568 - val_acc: 0.8494\n",
            "Epoch 436/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.3421 - acc: 0.8836 - val_loss: 0.4673 - val_acc: 0.8457\n",
            "Epoch 437/750\n",
            "3315/3315 [==============================] - 2s 454us/step - loss: 0.3360 - acc: 0.8830 - val_loss: 0.4865 - val_acc: 0.8359\n",
            "Epoch 438/750\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.3389 - acc: 0.8821 - val_loss: 0.4904 - val_acc: 0.8389\n",
            "Epoch 439/750\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.3337 - acc: 0.8827 - val_loss: 0.4499 - val_acc: 0.8543\n",
            "Epoch 440/750\n",
            "3315/3315 [==============================] - 2s 453us/step - loss: 0.3443 - acc: 0.8817 - val_loss: 0.4554 - val_acc: 0.8561\n",
            "Epoch 441/750\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.3370 - acc: 0.8842 - val_loss: 0.4700 - val_acc: 0.8438\n",
            "Epoch 442/750\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.3311 - acc: 0.8905 - val_loss: 0.4607 - val_acc: 0.8432\n",
            "Epoch 443/750\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.3347 - acc: 0.8778 - val_loss: 0.4843 - val_acc: 0.8438\n",
            "Epoch 444/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.3285 - acc: 0.8890 - val_loss: 0.4729 - val_acc: 0.8432\n",
            "Epoch 445/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.3240 - acc: 0.8941 - val_loss: 0.4810 - val_acc: 0.8469\n",
            "Epoch 446/750\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.3309 - acc: 0.8875 - val_loss: 0.4636 - val_acc: 0.8432\n",
            "Epoch 447/750\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.3300 - acc: 0.8869 - val_loss: 0.4728 - val_acc: 0.8524\n",
            "Epoch 448/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.3241 - acc: 0.8926 - val_loss: 0.4746 - val_acc: 0.8463\n",
            "Epoch 449/750\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.3246 - acc: 0.8878 - val_loss: 0.4626 - val_acc: 0.8530\n",
            "Epoch 450/750\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.3236 - acc: 0.8929 - val_loss: 0.4725 - val_acc: 0.8518\n",
            "Epoch 451/750\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.3166 - acc: 0.8917 - val_loss: 0.4656 - val_acc: 0.8396\n",
            "Epoch 452/750\n",
            "3315/3315 [==============================] - 2s 453us/step - loss: 0.3187 - acc: 0.8944 - val_loss: 0.4573 - val_acc: 0.8438\n",
            "Epoch 453/750\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.3202 - acc: 0.8890 - val_loss: 0.4771 - val_acc: 0.8365\n",
            "Epoch 454/750\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.3292 - acc: 0.8869 - val_loss: 0.4564 - val_acc: 0.8573\n",
            "Epoch 455/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.3189 - acc: 0.8893 - val_loss: 0.4534 - val_acc: 0.8475\n",
            "Epoch 456/750\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.3054 - acc: 0.8959 - val_loss: 0.4420 - val_acc: 0.8512\n",
            "Epoch 457/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.3288 - acc: 0.8845 - val_loss: 0.4683 - val_acc: 0.8347\n",
            "Epoch 458/750\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.3259 - acc: 0.8899 - val_loss: 0.4379 - val_acc: 0.8549\n",
            "Epoch 459/750\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.3072 - acc: 0.8914 - val_loss: 0.4652 - val_acc: 0.8555\n",
            "Epoch 460/750\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.3151 - acc: 0.8941 - val_loss: 0.4459 - val_acc: 0.8487\n",
            "Epoch 461/750\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.3159 - acc: 0.8923 - val_loss: 0.4530 - val_acc: 0.8481\n",
            "Epoch 462/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.3116 - acc: 0.8947 - val_loss: 0.4741 - val_acc: 0.8371\n",
            "Epoch 463/750\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.3090 - acc: 0.8995 - val_loss: 0.4850 - val_acc: 0.8396\n",
            "Epoch 464/750\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.3120 - acc: 0.8938 - val_loss: 0.4566 - val_acc: 0.8536\n",
            "Epoch 465/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.3114 - acc: 0.8938 - val_loss: 0.4652 - val_acc: 0.8438\n",
            "Epoch 466/750\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.3248 - acc: 0.8866 - val_loss: 0.4681 - val_acc: 0.8487\n",
            "Epoch 467/750\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.3152 - acc: 0.8929 - val_loss: 0.4523 - val_acc: 0.8506\n",
            "Epoch 468/750\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.3038 - acc: 0.8995 - val_loss: 0.4411 - val_acc: 0.8573\n",
            "Epoch 469/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.3122 - acc: 0.8968 - val_loss: 0.4431 - val_acc: 0.8610\n",
            "Epoch 470/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.3075 - acc: 0.8974 - val_loss: 0.4614 - val_acc: 0.8634\n",
            "Epoch 471/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.3096 - acc: 0.8932 - val_loss: 0.4402 - val_acc: 0.8604\n",
            "Epoch 472/750\n",
            "3315/3315 [==============================] - 2s 454us/step - loss: 0.2962 - acc: 0.8965 - val_loss: 0.4662 - val_acc: 0.8420\n",
            "Epoch 473/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.3123 - acc: 0.8902 - val_loss: 0.4730 - val_acc: 0.8445\n",
            "Epoch 474/750\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.3230 - acc: 0.8941 - val_loss: 0.4548 - val_acc: 0.8506\n",
            "Epoch 475/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.3062 - acc: 0.8980 - val_loss: 0.4389 - val_acc: 0.8573\n",
            "Epoch 476/750\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.2990 - acc: 0.8989 - val_loss: 0.4613 - val_acc: 0.8500\n",
            "Epoch 477/750\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.2929 - acc: 0.8992 - val_loss: 0.4509 - val_acc: 0.8487\n",
            "Epoch 478/750\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.3036 - acc: 0.9005 - val_loss: 0.4237 - val_acc: 0.8641\n",
            "Epoch 479/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.3082 - acc: 0.8887 - val_loss: 0.4561 - val_acc: 0.8518\n",
            "Epoch 480/750\n",
            "3315/3315 [==============================] - 2s 457us/step - loss: 0.2923 - acc: 0.9038 - val_loss: 0.4473 - val_acc: 0.8610\n",
            "Epoch 481/750\n",
            "3315/3315 [==============================] - 1s 452us/step - loss: 0.3050 - acc: 0.8992 - val_loss: 0.4577 - val_acc: 0.8469\n",
            "Epoch 482/750\n",
            "3315/3315 [==============================] - 1s 451us/step - loss: 0.3023 - acc: 0.9008 - val_loss: 0.4373 - val_acc: 0.8506\n",
            "Epoch 483/750\n",
            "3315/3315 [==============================] - 2s 456us/step - loss: 0.2960 - acc: 0.9017 - val_loss: 0.4556 - val_acc: 0.8530\n",
            "Epoch 484/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.3068 - acc: 0.8980 - val_loss: 0.4359 - val_acc: 0.8616\n",
            "Epoch 485/750\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.2998 - acc: 0.8977 - val_loss: 0.4476 - val_acc: 0.8530\n",
            "Epoch 486/750\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.3218 - acc: 0.8848 - val_loss: 0.4405 - val_acc: 0.8653\n",
            "Epoch 487/750\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.2961 - acc: 0.9005 - val_loss: 0.4594 - val_acc: 0.8524\n",
            "Epoch 488/750\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.3020 - acc: 0.8935 - val_loss: 0.4537 - val_acc: 0.8506\n",
            "Epoch 489/750\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.2899 - acc: 0.9011 - val_loss: 0.4306 - val_acc: 0.8604\n",
            "Epoch 490/750\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.2919 - acc: 0.9017 - val_loss: 0.4512 - val_acc: 0.8438\n",
            "Epoch 491/750\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.2941 - acc: 0.9044 - val_loss: 0.4394 - val_acc: 0.8555\n",
            "Epoch 492/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.2977 - acc: 0.8929 - val_loss: 0.4314 - val_acc: 0.8634\n",
            "Epoch 493/750\n",
            "3315/3315 [==============================] - 2s 456us/step - loss: 0.2877 - acc: 0.8992 - val_loss: 0.4365 - val_acc: 0.8598\n",
            "Epoch 494/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.2905 - acc: 0.9011 - val_loss: 0.4336 - val_acc: 0.8634\n",
            "Epoch 495/750\n",
            "3315/3315 [==============================] - 2s 457us/step - loss: 0.2935 - acc: 0.9020 - val_loss: 0.4446 - val_acc: 0.8518\n",
            "Epoch 496/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.2902 - acc: 0.9011 - val_loss: 0.4300 - val_acc: 0.8573\n",
            "Epoch 497/750\n",
            "3315/3315 [==============================] - 2s 455us/step - loss: 0.2836 - acc: 0.9038 - val_loss: 0.4362 - val_acc: 0.8616\n",
            "Epoch 498/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.2947 - acc: 0.8971 - val_loss: 0.4505 - val_acc: 0.8561\n",
            "Epoch 499/750\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.2950 - acc: 0.8974 - val_loss: 0.4372 - val_acc: 0.8604\n",
            "Epoch 500/750\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.2794 - acc: 0.9008 - val_loss: 0.4290 - val_acc: 0.8683\n",
            "Epoch 501/750\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.2987 - acc: 0.8968 - val_loss: 0.4359 - val_acc: 0.8585\n",
            "Epoch 502/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.2891 - acc: 0.9056 - val_loss: 0.4337 - val_acc: 0.8665\n",
            "Epoch 503/750\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.2688 - acc: 0.9068 - val_loss: 0.4501 - val_acc: 0.8585\n",
            "Epoch 504/750\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.2808 - acc: 0.9068 - val_loss: 0.4293 - val_acc: 0.8579\n",
            "Epoch 505/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.2668 - acc: 0.9038 - val_loss: 0.4428 - val_acc: 0.8506\n",
            "Epoch 506/750\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.2827 - acc: 0.8989 - val_loss: 0.4300 - val_acc: 0.8641\n",
            "Epoch 507/750\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.2700 - acc: 0.9101 - val_loss: 0.4379 - val_acc: 0.8579\n",
            "Epoch 508/750\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.2809 - acc: 0.9032 - val_loss: 0.4243 - val_acc: 0.8671\n",
            "Epoch 509/750\n",
            "3315/3315 [==============================] - 2s 457us/step - loss: 0.2900 - acc: 0.8989 - val_loss: 0.4247 - val_acc: 0.8683\n",
            "Epoch 510/750\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.2936 - acc: 0.9068 - val_loss: 0.4585 - val_acc: 0.8451\n",
            "Epoch 511/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.2766 - acc: 0.8995 - val_loss: 0.4418 - val_acc: 0.8641\n",
            "Epoch 512/750\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.2711 - acc: 0.9140 - val_loss: 0.4170 - val_acc: 0.8757\n",
            "Epoch 513/750\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.2671 - acc: 0.9107 - val_loss: 0.4492 - val_acc: 0.8530\n",
            "Epoch 514/750\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.2872 - acc: 0.9011 - val_loss: 0.4293 - val_acc: 0.8585\n",
            "Epoch 515/750\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.2785 - acc: 0.9044 - val_loss: 0.4244 - val_acc: 0.8726\n",
            "Epoch 516/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.2771 - acc: 0.9050 - val_loss: 0.4126 - val_acc: 0.8732\n",
            "Epoch 517/750\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.2683 - acc: 0.9080 - val_loss: 0.4164 - val_acc: 0.8653\n",
            "Epoch 518/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.2787 - acc: 0.9065 - val_loss: 0.4234 - val_acc: 0.8604\n",
            "Epoch 519/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.2750 - acc: 0.9098 - val_loss: 0.4109 - val_acc: 0.8647\n",
            "Epoch 520/750\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.2694 - acc: 0.9113 - val_loss: 0.4404 - val_acc: 0.8530\n",
            "Epoch 521/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.2789 - acc: 0.9032 - val_loss: 0.4086 - val_acc: 0.8732\n",
            "Epoch 522/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.2737 - acc: 0.9068 - val_loss: 0.4560 - val_acc: 0.8530\n",
            "Epoch 523/750\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.2730 - acc: 0.9059 - val_loss: 0.4231 - val_acc: 0.8677\n",
            "Epoch 524/750\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.2607 - acc: 0.9149 - val_loss: 0.4353 - val_acc: 0.8604\n",
            "Epoch 525/750\n",
            "3315/3315 [==============================] - 2s 457us/step - loss: 0.2741 - acc: 0.9080 - val_loss: 0.4089 - val_acc: 0.8714\n",
            "Epoch 526/750\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.2723 - acc: 0.9089 - val_loss: 0.4203 - val_acc: 0.8714\n",
            "Epoch 527/750\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.2692 - acc: 0.9101 - val_loss: 0.4448 - val_acc: 0.8543\n",
            "Epoch 528/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.2787 - acc: 0.9050 - val_loss: 0.4240 - val_acc: 0.8763\n",
            "Epoch 529/750\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.2670 - acc: 0.9101 - val_loss: 0.4319 - val_acc: 0.8634\n",
            "Epoch 530/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.2643 - acc: 0.9113 - val_loss: 0.4542 - val_acc: 0.8579\n",
            "Epoch 531/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.2715 - acc: 0.9059 - val_loss: 0.4128 - val_acc: 0.8702\n",
            "Epoch 532/750\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.2626 - acc: 0.9125 - val_loss: 0.4333 - val_acc: 0.8616\n",
            "Epoch 533/750\n",
            "3315/3315 [==============================] - 2s 454us/step - loss: 0.2567 - acc: 0.9158 - val_loss: 0.4298 - val_acc: 0.8579\n",
            "Epoch 534/750\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.2581 - acc: 0.9119 - val_loss: 0.4170 - val_acc: 0.8745\n",
            "Epoch 535/750\n",
            "3315/3315 [==============================] - 2s 453us/step - loss: 0.2577 - acc: 0.9173 - val_loss: 0.4399 - val_acc: 0.8653\n",
            "Epoch 536/750\n",
            "3315/3315 [==============================] - 2s 457us/step - loss: 0.2691 - acc: 0.9107 - val_loss: 0.4242 - val_acc: 0.8616\n",
            "Epoch 537/750\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.2623 - acc: 0.9125 - val_loss: 0.4111 - val_acc: 0.8720\n",
            "Epoch 538/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.2614 - acc: 0.9161 - val_loss: 0.4074 - val_acc: 0.8763\n",
            "Epoch 539/750\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.2625 - acc: 0.9131 - val_loss: 0.4343 - val_acc: 0.8690\n",
            "Epoch 540/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.2590 - acc: 0.9134 - val_loss: 0.4183 - val_acc: 0.8745\n",
            "Epoch 541/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.2666 - acc: 0.9065 - val_loss: 0.4169 - val_acc: 0.8683\n",
            "Epoch 542/750\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.2668 - acc: 0.9107 - val_loss: 0.4269 - val_acc: 0.8665\n",
            "Epoch 543/750\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.2581 - acc: 0.9107 - val_loss: 0.4158 - val_acc: 0.8641\n",
            "Epoch 544/750\n",
            "3315/3315 [==============================] - 2s 455us/step - loss: 0.2620 - acc: 0.9134 - val_loss: 0.4042 - val_acc: 0.8726\n",
            "Epoch 545/750\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.2472 - acc: 0.9146 - val_loss: 0.4164 - val_acc: 0.8714\n",
            "Epoch 546/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.2631 - acc: 0.9047 - val_loss: 0.4264 - val_acc: 0.8683\n",
            "Epoch 547/750\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.2601 - acc: 0.9062 - val_loss: 0.4066 - val_acc: 0.8714\n",
            "Epoch 548/750\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.2531 - acc: 0.9134 - val_loss: 0.4020 - val_acc: 0.8781\n",
            "Epoch 549/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.2628 - acc: 0.9131 - val_loss: 0.4092 - val_acc: 0.8745\n",
            "Epoch 550/750\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.2571 - acc: 0.9113 - val_loss: 0.4097 - val_acc: 0.8751\n",
            "Epoch 551/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.2611 - acc: 0.9119 - val_loss: 0.4571 - val_acc: 0.8536\n",
            "Epoch 552/750\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.2610 - acc: 0.9143 - val_loss: 0.4014 - val_acc: 0.8690\n",
            "Epoch 553/750\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.2736 - acc: 0.9065 - val_loss: 0.3819 - val_acc: 0.8843\n",
            "Epoch 554/750\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.2594 - acc: 0.9089 - val_loss: 0.4086 - val_acc: 0.8769\n",
            "Epoch 555/750\n",
            "3315/3315 [==============================] - 2s 454us/step - loss: 0.2506 - acc: 0.9134 - val_loss: 0.4283 - val_acc: 0.8647\n",
            "Epoch 556/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.2630 - acc: 0.9155 - val_loss: 0.4126 - val_acc: 0.8812\n",
            "Epoch 557/750\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.2529 - acc: 0.9101 - val_loss: 0.4021 - val_acc: 0.8714\n",
            "Epoch 558/750\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 0.2511 - acc: 0.9164 - val_loss: 0.4085 - val_acc: 0.8794\n",
            "Epoch 559/750\n",
            "3315/3315 [==============================] - 2s 486us/step - loss: 0.2459 - acc: 0.9255 - val_loss: 0.3947 - val_acc: 0.8763\n",
            "Epoch 560/750\n",
            "3315/3315 [==============================] - 2s 492us/step - loss: 0.2607 - acc: 0.9131 - val_loss: 0.4443 - val_acc: 0.8665\n",
            "Epoch 561/750\n",
            "3315/3315 [==============================] - 2s 481us/step - loss: 0.2579 - acc: 0.9143 - val_loss: 0.3999 - val_acc: 0.8690\n",
            "Epoch 562/750\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.2473 - acc: 0.9198 - val_loss: 0.4261 - val_acc: 0.8671\n",
            "Epoch 563/750\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.2508 - acc: 0.9152 - val_loss: 0.4163 - val_acc: 0.8671\n",
            "Epoch 564/750\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.2351 - acc: 0.9210 - val_loss: 0.4183 - val_acc: 0.8659\n",
            "Epoch 565/750\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.2494 - acc: 0.9158 - val_loss: 0.4203 - val_acc: 0.8696\n",
            "Epoch 566/750\n",
            "3315/3315 [==============================] - 2s 453us/step - loss: 0.2436 - acc: 0.9125 - val_loss: 0.4158 - val_acc: 0.8690\n",
            "Epoch 567/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.2554 - acc: 0.9119 - val_loss: 0.4071 - val_acc: 0.8806\n",
            "Epoch 568/750\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.2669 - acc: 0.9116 - val_loss: 0.4020 - val_acc: 0.8726\n",
            "Epoch 569/750\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.2434 - acc: 0.9189 - val_loss: 0.3973 - val_acc: 0.8800\n",
            "Epoch 570/750\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.2388 - acc: 0.9152 - val_loss: 0.4137 - val_acc: 0.8708\n",
            "Epoch 571/750\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.2567 - acc: 0.9164 - val_loss: 0.3999 - val_acc: 0.8788\n",
            "Epoch 572/750\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.2427 - acc: 0.9186 - val_loss: 0.3958 - val_acc: 0.8757\n",
            "Epoch 573/750\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.2494 - acc: 0.9119 - val_loss: 0.4036 - val_acc: 0.8769\n",
            "Epoch 574/750\n",
            "3315/3315 [==============================] - 2s 484us/step - loss: 0.2423 - acc: 0.9173 - val_loss: 0.4154 - val_acc: 0.8690\n",
            "Epoch 575/750\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.2475 - acc: 0.9170 - val_loss: 0.4081 - val_acc: 0.8775\n",
            "Epoch 576/750\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.2374 - acc: 0.9149 - val_loss: 0.4013 - val_acc: 0.8769\n",
            "Epoch 577/750\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.2425 - acc: 0.9192 - val_loss: 0.4046 - val_acc: 0.8702\n",
            "Epoch 578/750\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.2259 - acc: 0.9273 - val_loss: 0.4013 - val_acc: 0.8732\n",
            "Epoch 579/750\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.2391 - acc: 0.9210 - val_loss: 0.3943 - val_acc: 0.8794\n",
            "Epoch 580/750\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.2382 - acc: 0.9146 - val_loss: 0.3799 - val_acc: 0.8849\n",
            "Epoch 581/750\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.2434 - acc: 0.9158 - val_loss: 0.3816 - val_acc: 0.8824\n",
            "Epoch 582/750\n",
            "3315/3315 [==============================] - 2s 481us/step - loss: 0.2390 - acc: 0.9219 - val_loss: 0.4011 - val_acc: 0.8806\n",
            "Epoch 583/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.2480 - acc: 0.9143 - val_loss: 0.4088 - val_acc: 0.8788\n",
            "Epoch 584/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.2334 - acc: 0.9219 - val_loss: 0.4129 - val_acc: 0.8739\n",
            "Epoch 585/750\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 0.2417 - acc: 0.9167 - val_loss: 0.3848 - val_acc: 0.8861\n",
            "Epoch 586/750\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.2339 - acc: 0.9225 - val_loss: 0.4154 - val_acc: 0.8702\n",
            "Epoch 587/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.2385 - acc: 0.9195 - val_loss: 0.4139 - val_acc: 0.8665\n",
            "Epoch 588/750\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.2286 - acc: 0.9222 - val_loss: 0.4074 - val_acc: 0.8751\n",
            "Epoch 589/750\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.2364 - acc: 0.9195 - val_loss: 0.3955 - val_acc: 0.8788\n",
            "Epoch 590/750\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.2324 - acc: 0.9231 - val_loss: 0.3966 - val_acc: 0.8812\n",
            "Epoch 591/750\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.2403 - acc: 0.9216 - val_loss: 0.4083 - val_acc: 0.8788\n",
            "Epoch 592/750\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.2313 - acc: 0.9225 - val_loss: 0.4058 - val_acc: 0.8781\n",
            "Epoch 593/750\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.2327 - acc: 0.9222 - val_loss: 0.4017 - val_acc: 0.8745\n",
            "Epoch 594/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.2388 - acc: 0.9161 - val_loss: 0.3854 - val_acc: 0.8818\n",
            "Epoch 595/750\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.2358 - acc: 0.9195 - val_loss: 0.3994 - val_acc: 0.8769\n",
            "Epoch 596/750\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.2305 - acc: 0.9219 - val_loss: 0.4014 - val_acc: 0.8690\n",
            "Epoch 597/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.2325 - acc: 0.9216 - val_loss: 0.3711 - val_acc: 0.8830\n",
            "Epoch 598/750\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.2350 - acc: 0.9152 - val_loss: 0.3971 - val_acc: 0.8781\n",
            "Epoch 599/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.2258 - acc: 0.9167 - val_loss: 0.4116 - val_acc: 0.8757\n",
            "Epoch 600/750\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.2211 - acc: 0.9249 - val_loss: 0.3890 - val_acc: 0.8849\n",
            "Epoch 601/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.2319 - acc: 0.9183 - val_loss: 0.3760 - val_acc: 0.8843\n",
            "Epoch 602/750\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.2350 - acc: 0.9189 - val_loss: 0.4121 - val_acc: 0.8757\n",
            "Epoch 603/750\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.2324 - acc: 0.9110 - val_loss: 0.3982 - val_acc: 0.8732\n",
            "Epoch 604/750\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.2395 - acc: 0.9207 - val_loss: 0.3968 - val_acc: 0.8751\n",
            "Epoch 605/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.2225 - acc: 0.9207 - val_loss: 0.3883 - val_acc: 0.8855\n",
            "Epoch 606/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.2437 - acc: 0.9155 - val_loss: 0.3800 - val_acc: 0.8806\n",
            "Epoch 607/750\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.2346 - acc: 0.9219 - val_loss: 0.3918 - val_acc: 0.8843\n",
            "Epoch 608/750\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.2337 - acc: 0.9204 - val_loss: 0.3771 - val_acc: 0.8873\n",
            "Epoch 609/750\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.2379 - acc: 0.9170 - val_loss: 0.3836 - val_acc: 0.8885\n",
            "Epoch 610/750\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.2219 - acc: 0.9237 - val_loss: 0.3980 - val_acc: 0.8794\n",
            "Epoch 611/750\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.2229 - acc: 0.9255 - val_loss: 0.3892 - val_acc: 0.8867\n",
            "Epoch 612/750\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.2347 - acc: 0.9195 - val_loss: 0.4069 - val_acc: 0.8751\n",
            "Epoch 613/750\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.2240 - acc: 0.9249 - val_loss: 0.3825 - val_acc: 0.8885\n",
            "Epoch 614/750\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.2281 - acc: 0.9219 - val_loss: 0.3756 - val_acc: 0.8849\n",
            "Epoch 615/750\n",
            "3315/3315 [==============================] - 2s 455us/step - loss: 0.2220 - acc: 0.9252 - val_loss: 0.3954 - val_acc: 0.8824\n",
            "Epoch 616/750\n",
            "3315/3315 [==============================] - 2s 454us/step - loss: 0.2233 - acc: 0.9249 - val_loss: 0.3799 - val_acc: 0.8947\n",
            "Epoch 617/750\n",
            "3315/3315 [==============================] - 1s 448us/step - loss: 0.2286 - acc: 0.9213 - val_loss: 0.4278 - val_acc: 0.8634\n",
            "Epoch 618/750\n",
            "3315/3315 [==============================] - 1s 442us/step - loss: 0.2354 - acc: 0.9173 - val_loss: 0.3938 - val_acc: 0.8757\n",
            "Epoch 619/750\n",
            "3315/3315 [==============================] - 1s 447us/step - loss: 0.2253 - acc: 0.9225 - val_loss: 0.3870 - val_acc: 0.8775\n",
            "Epoch 620/750\n",
            "3315/3315 [==============================] - 1s 441us/step - loss: 0.2241 - acc: 0.9204 - val_loss: 0.4058 - val_acc: 0.8757\n",
            "Epoch 621/750\n",
            "3315/3315 [==============================] - 1s 445us/step - loss: 0.2273 - acc: 0.9204 - val_loss: 0.3802 - val_acc: 0.8879\n",
            "Epoch 622/750\n",
            "3315/3315 [==============================] - 2s 453us/step - loss: 0.2246 - acc: 0.9261 - val_loss: 0.4036 - val_acc: 0.8732\n",
            "Epoch 623/750\n",
            "3315/3315 [==============================] - 1s 440us/step - loss: 0.2284 - acc: 0.9192 - val_loss: 0.3950 - val_acc: 0.8800\n",
            "Epoch 624/750\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.2246 - acc: 0.9216 - val_loss: 0.4065 - val_acc: 0.8800\n",
            "Epoch 625/750\n",
            "3315/3315 [==============================] - 1s 433us/step - loss: 0.2256 - acc: 0.9255 - val_loss: 0.3646 - val_acc: 0.8928\n",
            "Epoch 626/750\n",
            "3315/3315 [==============================] - 1s 438us/step - loss: 0.2126 - acc: 0.9255 - val_loss: 0.3870 - val_acc: 0.8806\n",
            "Epoch 627/750\n",
            "3315/3315 [==============================] - 1s 437us/step - loss: 0.2144 - acc: 0.9249 - val_loss: 0.3937 - val_acc: 0.8794\n",
            "Epoch 628/750\n",
            "3315/3315 [==============================] - 2s 457us/step - loss: 0.2185 - acc: 0.9255 - val_loss: 0.3937 - val_acc: 0.8800\n",
            "Epoch 629/750\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.2035 - acc: 0.9363 - val_loss: 0.3882 - val_acc: 0.8824\n",
            "Epoch 630/750\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.2268 - acc: 0.9213 - val_loss: 0.3705 - val_acc: 0.8928\n",
            "Epoch 631/750\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.2184 - acc: 0.9267 - val_loss: 0.3931 - val_acc: 0.8757\n",
            "Epoch 632/750\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.2163 - acc: 0.9246 - val_loss: 0.3741 - val_acc: 0.8836\n",
            "Epoch 633/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.2104 - acc: 0.9276 - val_loss: 0.3738 - val_acc: 0.8843\n",
            "Epoch 634/750\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.2136 - acc: 0.9291 - val_loss: 0.3872 - val_acc: 0.8867\n",
            "Epoch 635/750\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.2058 - acc: 0.9318 - val_loss: 0.3798 - val_acc: 0.8849\n",
            "Epoch 636/750\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.2155 - acc: 0.9306 - val_loss: 0.3737 - val_acc: 0.8916\n",
            "Epoch 637/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.2011 - acc: 0.9312 - val_loss: 0.3773 - val_acc: 0.8879\n",
            "Epoch 638/750\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.2330 - acc: 0.9246 - val_loss: 0.3871 - val_acc: 0.8885\n",
            "Epoch 639/750\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.2094 - acc: 0.9279 - val_loss: 0.3783 - val_acc: 0.8910\n",
            "Epoch 640/750\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.2096 - acc: 0.9315 - val_loss: 0.3898 - val_acc: 0.8830\n",
            "Epoch 641/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.2007 - acc: 0.9330 - val_loss: 0.3806 - val_acc: 0.8806\n",
            "Epoch 642/750\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.2098 - acc: 0.9309 - val_loss: 0.3667 - val_acc: 0.8861\n",
            "Epoch 643/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.2097 - acc: 0.9276 - val_loss: 0.3757 - val_acc: 0.8959\n",
            "Epoch 644/750\n",
            "3315/3315 [==============================] - 2s 457us/step - loss: 0.2097 - acc: 0.9276 - val_loss: 0.3891 - val_acc: 0.8879\n",
            "Epoch 645/750\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.2078 - acc: 0.9306 - val_loss: 0.3885 - val_acc: 0.8800\n",
            "Epoch 646/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.2216 - acc: 0.9303 - val_loss: 0.3788 - val_acc: 0.8806\n",
            "Epoch 647/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.2049 - acc: 0.9270 - val_loss: 0.3806 - val_acc: 0.8843\n",
            "Epoch 648/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.2026 - acc: 0.9291 - val_loss: 0.3760 - val_acc: 0.8879\n",
            "Epoch 649/750\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.1957 - acc: 0.9357 - val_loss: 0.3900 - val_acc: 0.8849\n",
            "Epoch 650/750\n",
            "3315/3315 [==============================] - 2s 454us/step - loss: 0.2125 - acc: 0.9264 - val_loss: 0.3792 - val_acc: 0.8879\n",
            "Epoch 651/750\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.2204 - acc: 0.9279 - val_loss: 0.3827 - val_acc: 0.8836\n",
            "Epoch 652/750\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.2312 - acc: 0.9210 - val_loss: 0.3667 - val_acc: 0.8904\n",
            "Epoch 653/750\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.1995 - acc: 0.9315 - val_loss: 0.3711 - val_acc: 0.8898\n",
            "Epoch 654/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.2059 - acc: 0.9276 - val_loss: 0.3884 - val_acc: 0.8769\n",
            "Epoch 655/750\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.2018 - acc: 0.9315 - val_loss: 0.3902 - val_acc: 0.8830\n",
            "Epoch 656/750\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.2063 - acc: 0.9294 - val_loss: 0.3847 - val_acc: 0.8781\n",
            "Epoch 657/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.2010 - acc: 0.9363 - val_loss: 0.3716 - val_acc: 0.8830\n",
            "Epoch 658/750\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.1953 - acc: 0.9336 - val_loss: 0.3693 - val_acc: 0.8928\n",
            "Epoch 659/750\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.2045 - acc: 0.9309 - val_loss: 0.3895 - val_acc: 0.8861\n",
            "Epoch 660/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.2001 - acc: 0.9318 - val_loss: 0.3879 - val_acc: 0.8769\n",
            "Epoch 661/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.2108 - acc: 0.9309 - val_loss: 0.3697 - val_acc: 0.8898\n",
            "Epoch 662/750\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.1998 - acc: 0.9306 - val_loss: 0.3853 - val_acc: 0.8800\n",
            "Epoch 663/750\n",
            "3315/3315 [==============================] - 2s 456us/step - loss: 0.1943 - acc: 0.9342 - val_loss: 0.3637 - val_acc: 0.8916\n",
            "Epoch 664/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.1936 - acc: 0.9354 - val_loss: 0.3812 - val_acc: 0.8934\n",
            "Epoch 665/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.2026 - acc: 0.9336 - val_loss: 0.3948 - val_acc: 0.8879\n",
            "Epoch 666/750\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.2000 - acc: 0.9300 - val_loss: 0.3686 - val_acc: 0.8879\n",
            "Epoch 667/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.2012 - acc: 0.9360 - val_loss: 0.3734 - val_acc: 0.8916\n",
            "Epoch 668/750\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.1939 - acc: 0.9388 - val_loss: 0.3677 - val_acc: 0.8892\n",
            "Epoch 669/750\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.1915 - acc: 0.9345 - val_loss: 0.3721 - val_acc: 0.8879\n",
            "Epoch 670/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.1965 - acc: 0.9342 - val_loss: 0.4077 - val_acc: 0.8812\n",
            "Epoch 671/750\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.2008 - acc: 0.9342 - val_loss: 0.3849 - val_acc: 0.8904\n",
            "Epoch 672/750\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.1925 - acc: 0.9297 - val_loss: 0.3740 - val_acc: 0.8898\n",
            "Epoch 673/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.1983 - acc: 0.9373 - val_loss: 0.3843 - val_acc: 0.8800\n",
            "Epoch 674/750\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.1881 - acc: 0.9382 - val_loss: 0.3887 - val_acc: 0.8849\n",
            "Epoch 675/750\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.1966 - acc: 0.9321 - val_loss: 0.3690 - val_acc: 0.8934\n",
            "Epoch 676/750\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.1988 - acc: 0.9306 - val_loss: 0.3696 - val_acc: 0.8898\n",
            "Epoch 677/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.1925 - acc: 0.9348 - val_loss: 0.3520 - val_acc: 0.8922\n",
            "Epoch 678/750\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.1881 - acc: 0.9363 - val_loss: 0.3666 - val_acc: 0.8898\n",
            "Epoch 679/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.1760 - acc: 0.9424 - val_loss: 0.3592 - val_acc: 0.8922\n",
            "Epoch 680/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.1857 - acc: 0.9339 - val_loss: 0.3854 - val_acc: 0.8824\n",
            "Epoch 681/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.2217 - acc: 0.9255 - val_loss: 0.3576 - val_acc: 0.8904\n",
            "Epoch 682/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.1960 - acc: 0.9330 - val_loss: 0.3805 - val_acc: 0.8904\n",
            "Epoch 683/750\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.1894 - acc: 0.9360 - val_loss: 0.3646 - val_acc: 0.8867\n",
            "Epoch 684/750\n",
            "3315/3315 [==============================] - 2s 457us/step - loss: 0.1875 - acc: 0.9379 - val_loss: 0.3694 - val_acc: 0.8855\n",
            "Epoch 685/750\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.2112 - acc: 0.9315 - val_loss: 0.3994 - val_acc: 0.8769\n",
            "Epoch 686/750\n",
            "3315/3315 [==============================] - 2s 453us/step - loss: 0.1975 - acc: 0.9318 - val_loss: 0.3885 - val_acc: 0.8873\n",
            "Epoch 687/750\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.1949 - acc: 0.9354 - val_loss: 0.3770 - val_acc: 0.8928\n",
            "Epoch 688/750\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.1957 - acc: 0.9336 - val_loss: 0.3701 - val_acc: 0.8892\n",
            "Epoch 689/750\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.2027 - acc: 0.9324 - val_loss: 0.3698 - val_acc: 0.8898\n",
            "Epoch 690/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.1870 - acc: 0.9397 - val_loss: 0.3679 - val_acc: 0.8910\n",
            "Epoch 691/750\n",
            "3315/3315 [==============================] - 2s 454us/step - loss: 0.2021 - acc: 0.9291 - val_loss: 0.3513 - val_acc: 0.8922\n",
            "Epoch 692/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.1956 - acc: 0.9351 - val_loss: 0.3713 - val_acc: 0.8873\n",
            "Epoch 693/750\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.1965 - acc: 0.9321 - val_loss: 0.3595 - val_acc: 0.8904\n",
            "Epoch 694/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.1988 - acc: 0.9327 - val_loss: 0.3791 - val_acc: 0.8885\n",
            "Epoch 695/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.1926 - acc: 0.9318 - val_loss: 0.3734 - val_acc: 0.8916\n",
            "Epoch 696/750\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.1854 - acc: 0.9385 - val_loss: 0.3819 - val_acc: 0.8953\n",
            "Epoch 697/750\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.1893 - acc: 0.9324 - val_loss: 0.3844 - val_acc: 0.8910\n",
            "Epoch 698/750\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.1885 - acc: 0.9360 - val_loss: 0.3719 - val_acc: 0.8830\n",
            "Epoch 699/750\n",
            "3315/3315 [==============================] - 2s 456us/step - loss: 0.1862 - acc: 0.9391 - val_loss: 0.3634 - val_acc: 0.9039\n",
            "Epoch 700/750\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.1835 - acc: 0.9379 - val_loss: 0.3799 - val_acc: 0.8898\n",
            "Epoch 701/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.1879 - acc: 0.9367 - val_loss: 0.3664 - val_acc: 0.8928\n",
            "Epoch 702/750\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.1807 - acc: 0.9397 - val_loss: 0.4038 - val_acc: 0.8781\n",
            "Epoch 703/750\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.1824 - acc: 0.9388 - val_loss: 0.3663 - val_acc: 0.8947\n",
            "Epoch 704/750\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.1763 - acc: 0.9391 - val_loss: 0.3711 - val_acc: 0.8996\n",
            "Epoch 705/750\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.1981 - acc: 0.9309 - val_loss: 0.3565 - val_acc: 0.8977\n",
            "Epoch 706/750\n",
            "3315/3315 [==============================] - 2s 455us/step - loss: 0.1962 - acc: 0.9333 - val_loss: 0.3653 - val_acc: 0.8965\n",
            "Epoch 707/750\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.1824 - acc: 0.9415 - val_loss: 0.3531 - val_acc: 0.9039\n",
            "Epoch 708/750\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.1751 - acc: 0.9409 - val_loss: 0.3842 - val_acc: 0.8867\n",
            "Epoch 709/750\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.1815 - acc: 0.9448 - val_loss: 0.3546 - val_acc: 0.8996\n",
            "Epoch 710/750\n",
            "3315/3315 [==============================] - 2s 456us/step - loss: 0.1885 - acc: 0.9370 - val_loss: 0.3528 - val_acc: 0.9026\n",
            "Epoch 711/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.1954 - acc: 0.9333 - val_loss: 0.3831 - val_acc: 0.8849\n",
            "Epoch 712/750\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.1818 - acc: 0.9367 - val_loss: 0.3657 - val_acc: 0.8892\n",
            "Epoch 713/750\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.1761 - acc: 0.9433 - val_loss: 0.3620 - val_acc: 0.8885\n",
            "Epoch 714/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.1928 - acc: 0.9330 - val_loss: 0.3471 - val_acc: 0.8971\n",
            "Epoch 715/750\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.1848 - acc: 0.9367 - val_loss: 0.3663 - val_acc: 0.8904\n",
            "Epoch 716/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.1843 - acc: 0.9373 - val_loss: 0.3634 - val_acc: 0.8947\n",
            "Epoch 717/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.1902 - acc: 0.9303 - val_loss: 0.3690 - val_acc: 0.8879\n",
            "Epoch 718/750\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.1868 - acc: 0.9370 - val_loss: 0.3842 - val_acc: 0.8849\n",
            "Epoch 719/750\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.1855 - acc: 0.9336 - val_loss: 0.3511 - val_acc: 0.8990\n",
            "Epoch 720/750\n",
            "3315/3315 [==============================] - 2s 455us/step - loss: 0.1768 - acc: 0.9406 - val_loss: 0.3547 - val_acc: 0.8941\n",
            "Epoch 721/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.1779 - acc: 0.9445 - val_loss: 0.4317 - val_acc: 0.8708\n",
            "Epoch 722/750\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.1785 - acc: 0.9354 - val_loss: 0.3572 - val_acc: 0.9008\n",
            "Epoch 723/750\n",
            "3315/3315 [==============================] - 2s 486us/step - loss: 0.1775 - acc: 0.9373 - val_loss: 0.3541 - val_acc: 0.8965\n",
            "Epoch 724/750\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.1879 - acc: 0.9367 - val_loss: 0.3483 - val_acc: 0.8977\n",
            "Epoch 725/750\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.1822 - acc: 0.9400 - val_loss: 0.3790 - val_acc: 0.8953\n",
            "Epoch 726/750\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.1718 - acc: 0.9363 - val_loss: 0.3580 - val_acc: 0.9075\n",
            "Epoch 727/750\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.1751 - acc: 0.9400 - val_loss: 0.3468 - val_acc: 0.8959\n",
            "Epoch 728/750\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.1755 - acc: 0.9376 - val_loss: 0.3581 - val_acc: 0.8965\n",
            "Epoch 729/750\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.1734 - acc: 0.9367 - val_loss: 0.3451 - val_acc: 0.9051\n",
            "Epoch 730/750\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.1870 - acc: 0.9382 - val_loss: 0.3563 - val_acc: 0.8941\n",
            "Epoch 731/750\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.1879 - acc: 0.9330 - val_loss: 0.3772 - val_acc: 0.8898\n",
            "Epoch 732/750\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.1719 - acc: 0.9463 - val_loss: 0.3676 - val_acc: 0.8959\n",
            "Epoch 733/750\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.1746 - acc: 0.9388 - val_loss: 0.3599 - val_acc: 0.8965\n",
            "Epoch 734/750\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.1727 - acc: 0.9430 - val_loss: 0.3807 - val_acc: 0.8867\n",
            "Epoch 735/750\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.1777 - acc: 0.9415 - val_loss: 0.3479 - val_acc: 0.8947\n",
            "Epoch 736/750\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.1665 - acc: 0.9400 - val_loss: 0.4063 - val_acc: 0.8824\n",
            "Epoch 737/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.1830 - acc: 0.9376 - val_loss: 0.3598 - val_acc: 0.8953\n",
            "Epoch 738/750\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.1908 - acc: 0.9370 - val_loss: 0.3556 - val_acc: 0.8947\n",
            "Epoch 739/750\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.1737 - acc: 0.9397 - val_loss: 0.3727 - val_acc: 0.8934\n",
            "Epoch 740/750\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.1715 - acc: 0.9406 - val_loss: 0.3736 - val_acc: 0.8934\n",
            "Epoch 741/750\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.1770 - acc: 0.9388 - val_loss: 0.3772 - val_acc: 0.8892\n",
            "Epoch 742/750\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.1825 - acc: 0.9397 - val_loss: 0.3742 - val_acc: 0.8977\n",
            "Epoch 743/750\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.1671 - acc: 0.9430 - val_loss: 0.3630 - val_acc: 0.8983\n",
            "Epoch 744/750\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.1772 - acc: 0.9415 - val_loss: 0.3564 - val_acc: 0.8990\n",
            "Epoch 745/750\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.1681 - acc: 0.9463 - val_loss: 0.3394 - val_acc: 0.8990\n",
            "Epoch 746/750\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.1697 - acc: 0.9400 - val_loss: 0.3409 - val_acc: 0.9032\n",
            "Epoch 747/750\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.1823 - acc: 0.9403 - val_loss: 0.3522 - val_acc: 0.8941\n",
            "Epoch 748/750\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.1754 - acc: 0.9391 - val_loss: 0.3625 - val_acc: 0.8916\n",
            "Epoch 749/750\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.1802 - acc: 0.9376 - val_loss: 0.3645 - val_acc: 0.8885\n",
            "Epoch 750/750\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.1737 - acc: 0.9385 - val_loss: 0.3581 - val_acc: 0.8898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mFytY6LDzgJ0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's plot the loss:"
      ]
    },
    {
      "metadata": {
        "id": "TFz4ClZov9gZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "outputId": "e0ae93d2-3085-4793-cdab-38330dbf2924"
      },
      "cell_type": "code",
      "source": [
        "plt.plot(cnnhistory.history['loss'])\n",
        "plt.plot(cnnhistory.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEVCAYAAAAb/KWvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7feced60ddd8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Vf1W7LgP2DA5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "And now let's plot the accuracy:"
      ]
    },
    {
      "metadata": {
        "id": "8yyFBt7ASPUe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "outputId": "49b3a66e-76cc-4b37-be20-78b4984ab6ac"
      },
      "cell_type": "code",
      "source": [
        "plt.plot(cnnhistory.history['acc'])\n",
        "plt.plot(cnnhistory.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAGZmbcoFwMysTbkAmJm1KRcAM7M29S/ntfNLo//bWQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fececd76f98>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "x_ySPOyHxkZ3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Save the model"
      ]
    },
    {
      "metadata": {
        "id": "f5kRmoD-sdHj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d2a863e6-8054-4e01-884d-80507811ffc4"
      },
      "cell_type": "code",
      "source": [
        "model_name = 'Emotion_Voice_Detection_Model.h5'\n",
        "save_dir = '/content/drive/My Drive/Ravdess_model'\n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_model"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved trained model at /content/drive/My Drive/Ravdess_model/Emotion_Voice_Detection_Model.h5 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MNUiznKNwUtJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Thanks for reading: to be continued..."
      ]
    }
  ]
}